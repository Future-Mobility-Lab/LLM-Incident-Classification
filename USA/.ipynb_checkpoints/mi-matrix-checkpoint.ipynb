{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4bb591d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_features(model_name):\n",
    "    return np.load(f'{model_name}_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f1256f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = load_features('albert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b30f50ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48714, 768)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "267453ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.024480124935507774\n",
      "Epoch 2, Loss: 0.02436743676662445\n",
      "Epoch 3, Loss: 0.024535322561860085\n",
      "Epoch 4, Loss: 0.02442612498998642\n",
      "Epoch 5, Loss: 0.024631595239043236\n",
      "Epoch 6, Loss: 0.02449626848101616\n",
      "Epoch 7, Loss: 0.024525536224246025\n",
      "Epoch 8, Loss: 0.024733291938900948\n",
      "Epoch 9, Loss: 0.024591457098722458\n",
      "Epoch 10, Loss: 0.02454942651093006\n",
      "Epoch 11, Loss: 0.024509530514478683\n",
      "Epoch 12, Loss: 0.024539312347769737\n",
      "Epoch 13, Loss: 0.02451564557850361\n",
      "Epoch 14, Loss: 0.02467780001461506\n",
      "Epoch 15, Loss: 0.02446221187710762\n",
      "Epoch 16, Loss: 0.024628471583127975\n",
      "Epoch 17, Loss: 0.024357981979846954\n",
      "Epoch 18, Loss: 0.024424651637673378\n",
      "Epoch 19, Loss: 0.02459125593304634\n",
      "Epoch 20, Loss: 0.02471555396914482\n",
      "Epoch 21, Loss: 0.024535659700632095\n",
      "Epoch 22, Loss: 0.024541249498724937\n",
      "Epoch 23, Loss: 0.024585653096437454\n",
      "Epoch 24, Loss: 0.024570763111114502\n",
      "Epoch 25, Loss: 0.024509068578481674\n",
      "Epoch 26, Loss: 0.024706760421395302\n",
      "Epoch 27, Loss: 0.024555418640375137\n",
      "Epoch 28, Loss: 0.02448127418756485\n",
      "Epoch 29, Loss: 0.02460223250091076\n",
      "Epoch 30, Loss: 0.02455853298306465\n",
      "Epoch 31, Loss: 0.024644240736961365\n",
      "Epoch 32, Loss: 0.024695269763469696\n",
      "Epoch 33, Loss: 0.02463977038860321\n",
      "Epoch 34, Loss: 0.02461804263293743\n",
      "Epoch 35, Loss: 0.024661071598529816\n",
      "Epoch 36, Loss: 0.024684153497219086\n",
      "Epoch 37, Loss: 0.02441704273223877\n",
      "Epoch 38, Loss: 0.024527350440621376\n",
      "Epoch 39, Loss: 0.024527227506041527\n",
      "Epoch 40, Loss: 0.024480242282152176\n",
      "Epoch 41, Loss: 0.02455611526966095\n",
      "Epoch 42, Loss: 0.024371424689888954\n",
      "Epoch 43, Loss: 0.02458711341023445\n",
      "Epoch 44, Loss: 0.024456480517983437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ce34d5182998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a dummy numpy array for demonstration\n",
    "# Uncomment this line if A is not defined\n",
    "# A = np.random.rand(48714, 4096)\n",
    "\n",
    "# Convert the numpy array to PyTorch tensor\n",
    "A_tensor = torch.tensor(A, dtype=torch.float32)\n",
    "\n",
    "# Specify the CUDA device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the AutoEncoder model\n",
    "class ComplexAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexAutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = ComplexAutoEncoder().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_data, in train_data:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_data)\n",
    "        loss = loss_function(output, batch_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "349c9fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1335.4993896484375\n",
      "Epoch 2, Loss: 1058.6412353515625\n",
      "Epoch 3, Loss: 890.7281494140625\n",
      "Epoch 4, Loss: 751.6736450195312\n",
      "Epoch 5, Loss: 698.8134765625\n",
      "Epoch 6, Loss: 680.2098999023438\n",
      "Epoch 7, Loss: 609.8023681640625\n",
      "Epoch 8, Loss: 551.2883911132812\n",
      "Epoch 9, Loss: 556.24951171875\n",
      "Epoch 10, Loss: 553.1036987304688\n",
      "Epoch 11, Loss: 520.4054565429688\n",
      "Epoch 12, Loss: 490.642578125\n",
      "Epoch 13, Loss: 467.3554382324219\n",
      "Epoch 14, Loss: 442.7686767578125\n",
      "Epoch 15, Loss: 457.72906494140625\n",
      "Epoch 16, Loss: 423.1995544433594\n",
      "Epoch 17, Loss: 404.68890380859375\n",
      "Epoch 18, Loss: 428.3824768066406\n",
      "Epoch 19, Loss: 381.81256103515625\n",
      "Epoch 20, Loss: 387.40362548828125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For reparameterization trick\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps*std\n",
    "\n",
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(1024, 64)\n",
    "        self.fc_logvar = nn.Linear(1024, 64)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        return mu\n",
    "\n",
    "# Loss function\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.MSELoss()(recon_x, x)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + 1e-5 * kl_loss\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.MSELoss(reduction='sum')(recon_x, x)  # or use nn.BCEWithLogitsLoss() for binary data\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Normalizing by the same number for a fair comparison between reconstruction and KL-divergence loss\n",
    "    batch_size = x.size(0)\n",
    "    kl_div /= batch_size * 4096  # 4096 is the feature dimension, adjust as needed\n",
    "    \n",
    "    return recon_loss + kl_div\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.001)\n",
    "\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Initialize VAE model\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Data preparation\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "A_scaled = scaler.fit_transform(A)\n",
    "\n",
    "# Convert to tensor\n",
    "A_tensor_scaled = torch.tensor(A_scaled, dtype=torch.float32)\n",
    "train_data = DataLoader(TensorDataset(A_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "# Training\n",
    "num_epochs = 20\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_data, in train_data:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch_data)\n",
    "        loss = vae_loss(recon_batch, batch_data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Extract reduced-dimension features\n",
    "model.eval()\n",
    "all_latents = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, in train_data:\n",
    "        batch_data = batch_data.to(device)\n",
    "        latent_mu = model.encode(batch_data)\n",
    "        all_latents.append(latent_mu.cpu().numpy())\n",
    "\n",
    "# Concatenate all the latent features into a single numpy array\n",
    "all_latents_np = np.concatenate(all_latents, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05025ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48714, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_latents_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eef02a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('TART_features.npy', all_latents_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276e85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01d19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86de425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/deap/creator.py:188: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/deap/creator.py:188: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated individual with MSE: 1273159096860.8179\n",
      "Evaluated individual with MSE: 1485244983226.3015\n",
      "Evaluated individual with MSE: 1261011239759.8057\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1154058876050.2278\n",
      "Evaluated individual with MSE: 1253208780725.2744\n",
      "Evaluated individual with MSE: 1255138046655.85\n",
      "Evaluated individual with MSE: 1246633388559.9958\n",
      "Evaluated individual with MSE: 1154948689169.7205\n",
      "Evaluated individual with MSE: 1210825509547.1968\n",
      "Evaluated individual with MSE: 1224336568960.082\n",
      "Evaluated individual with MSE: 1237972452887.659\n",
      "Evaluated individual with MSE: 1237733924374.3284\n",
      "Evaluated individual with MSE: 1249910045244.051\n",
      "Evaluated individual with MSE: 1145707283460.2961\n",
      "Evaluated individual with MSE: 1245988821975.8538\n",
      "Evaluated individual with MSE: 1218188489281.1978\n",
      "Evaluated individual with MSE: 1083864051363.5217\n",
      "Evaluated individual with MSE: 1260131409481.2144\n",
      "Evaluated individual with MSE: 1085024810400.0134\n",
      "Evaluated individual with MSE: 1142006971319.3167\n",
      "Evaluated individual with MSE: 1362846852570.1946\n",
      "Evaluated individual with MSE: 1174331192330.8145\n",
      "Evaluated individual with MSE: 1338794973490.4727\n",
      "Evaluated individual with MSE: 1135409689307.2578\n",
      "Evaluated individual with MSE: 1304274227844.8655\n",
      "Evaluated individual with MSE: 1223422395741.4521\n",
      "Evaluated individual with MSE: 1223282709518.007\n",
      "Evaluated individual with MSE: 1181009915939.4307\n",
      "Evaluated individual with MSE: 1084512606482.6299\n",
      "gen\tnevals\tmin       \n",
      "0  \t30    \t1.0597e+12\n",
      "Evaluated individual with MSE: 1108060776423.1135\n",
      "Evaluated individual with MSE: 1091221231006.4595\n",
      "Evaluated individual with MSE: 1153298489773.5198\n",
      "Evaluated individual with MSE: 1109104909981.5647\n",
      "Evaluated individual with MSE: 2021500898712.9854\n",
      "Evaluated individual with MSE: 1910666441198.606\n",
      "Evaluated individual with MSE: 1081686683544.9736\n",
      "Evaluated individual with MSE: 1175957514393.0251\n",
      "Evaluated individual with MSE: 1202382135007.3276\n",
      "Evaluated individual with MSE: 1097421306898.7599\n",
      "Evaluated individual with MSE: 1181009915939.4307\n",
      "Evaluated individual with MSE: 1181009915939.4307\n",
      "Evaluated individual with MSE: 1084756299822.2808\n",
      "1  \t13    \t1.0597e+12\n",
      "Evaluated individual with MSE: 1995580269906.9463\n",
      "Evaluated individual with MSE: 1864495801029.4055\n",
      "Evaluated individual with MSE: 1698493156545.9297\n",
      "Evaluated individual with MSE: 1913985551635.6687\n",
      "Evaluated individual with MSE: 1068209246614.8485\n",
      "Evaluated individual with MSE: 1076089554671.8364\n",
      "Evaluated individual with MSE: 2112613906251.74\n",
      "Evaluated individual with MSE: 1959763228023.9705\n",
      "Evaluated individual with MSE: 1837622943505.0435\n",
      "Evaluated individual with MSE: 1916655025711.594\n",
      "Evaluated individual with MSE: 2325261914590.4727\n",
      "Evaluated individual with MSE: 1838884362560.2612\n",
      "Evaluated individual with MSE: 1951121816835.0195\n",
      "Evaluated individual with MSE: 1726712059884.99\n",
      "Evaluated individual with MSE: 1067694739517.9778\n",
      "Evaluated individual with MSE: 1076996129366.1924\n",
      "2  \t16    \t1.0597e+12\n",
      "Evaluated individual with MSE: 1786836718196.9155\n",
      "Evaluated individual with MSE: 2028131857480.4678\n",
      "Evaluated individual with MSE: 1862292136848.8027\n",
      "Evaluated individual with MSE: 1072456769754.0278\n",
      "Evaluated individual with MSE: 1072565093795.8301\n",
      "Evaluated individual with MSE: 1101019369666.3162\n",
      "Evaluated individual with MSE: 1870797101477.0232\n",
      "Evaluated individual with MSE: 2007377362791.1113\n",
      "Evaluated individual with MSE: 1074570827851.2838\n",
      "Evaluated individual with MSE: 1078832879416.114\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1069481201267.7347\n",
      "Evaluated individual with MSE: 1074673627966.9572\n",
      "Evaluated individual with MSE: 1069247906824.3472\n",
      "Evaluated individual with MSE: 1092840937556.6968\n",
      "Evaluated individual with MSE: 1076411383000.1189\n",
      "Evaluated individual with MSE: 1084053590220.3007\n",
      "3  \t18    \t1.0597e+12\n",
      "Evaluated individual with MSE: 1070207243928.8398\n",
      "Evaluated individual with MSE: 1086704828204.646\n",
      "Evaluated individual with MSE: 1073472295032.1309\n",
      "Evaluated individual with MSE: 1090834819546.1509\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1089766012135.8125\n",
      "Evaluated individual with MSE: 1103547858350.6033\n",
      "Evaluated individual with MSE: 1098309840779.8925\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1059702279442.579\n",
      "Evaluated individual with MSE: 1059702279442.579\n",
      "Evaluated individual with MSE: 1103077314290.3757\n",
      "Evaluated individual with MSE: 1087417180783.9901\n",
      "Evaluated individual with MSE: 1069682331492.4343\n",
      "Evaluated individual with MSE: 1067402650028.9146\n",
      "Evaluated individual with MSE: 1057991438866.8861\n",
      "Evaluated individual with MSE: 1072009057780.9026\n",
      "Evaluated individual with MSE: 1093473259117.7344\n",
      "4  \t20    \t1.05799e+12\n",
      "Evaluated individual with MSE: 1078204380597.2767\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1059702279442.579\n",
      "Evaluated individual with MSE: 1059702279442.579\n",
      "Evaluated individual with MSE: 1056482535170.6257\n",
      "Evaluated individual with MSE: 1063663475749.8833\n",
      "Evaluated individual with MSE: 1070322711560.1659\n",
      "Evaluated individual with MSE: 1063471334376.1061\n",
      "Evaluated individual with MSE: 1069115204505.6755\n",
      "Evaluated individual with MSE: 1082138803613.297\n",
      "Evaluated individual with MSE: 1086191593478.6504\n",
      "Evaluated individual with MSE: 1063575807562.0784\n",
      "Evaluated individual with MSE: 1077208607363.6566\n",
      "5  \t14    \t1.05648e+12\n",
      "Evaluated individual with MSE: 1057873937136.1573\n",
      "Evaluated individual with MSE: 1059596222234.8815\n",
      "Evaluated individual with MSE: 1085105582017.3049\n",
      "Evaluated individual with MSE: 1061793347338.1667\n",
      "Evaluated individual with MSE: 1056075215273.2388\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1059702279442.579\n",
      "Evaluated individual with MSE: 1074072818471.8878\n",
      "Evaluated individual with MSE: 1059702279442.579\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1097736115508.4581\n",
      "Evaluated individual with MSE: 1059702279442.5791\n",
      "Evaluated individual with MSE: 1067027502384.545\n",
      "Evaluated individual with MSE: 1063054793475.2856\n",
      "Evaluated individual with MSE: 1057579655945.1543\n",
      "Evaluated individual with MSE: 1082286537353.2278\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f8d7884e104e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Run the evolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Run the evolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meaSimple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcxpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/deap/algorithms.py\u001b[0m in \u001b[0;36meaSimple\u001b[0;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moffspring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f8d7884e104e>\u001b[0m in \u001b[0;36mautoencoder\u001b[0;34m(individual, data)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluated individual with MSE: {mse}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate some sample data for demonstration\n",
    "n_samples, n_features = A.shape[0], A.shape[1]\n",
    "X = A\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the autoencoder\n",
    "def autoencoder(individual, data):\n",
    "    n_inputs = n_features\n",
    "    n_hidden = 64  # Number of components (latent features)\n",
    "    n_outputs = n_features\n",
    "\n",
    "    # Split individual into encoder and decoder parts\n",
    "    encoder_weights = np.array(individual[:n_inputs * n_hidden]).reshape((n_inputs, n_hidden))\n",
    "    decoder_weights = np.array(individual[n_inputs * n_hidden:]).reshape((n_hidden, n_outputs))\n",
    "\n",
    "    # Encoding\n",
    "    encoded = np.dot(data, encoder_weights)\n",
    "    \n",
    "    # Decoding\n",
    "    decoded = np.dot(encoded, decoder_weights)\n",
    "    \n",
    "    mse = np.mean((data - decoded)**2)\n",
    "    print(f\"Evaluated individual with MSE: {mse}\")\n",
    "    # Reconstruction loss (MSE)\n",
    "#     mse = np.mean((data - decoded)**2)\n",
    "    \n",
    "    return mse,\n",
    "def autoencoder(individual, data):\n",
    "    n_inputs = n_features\n",
    "    n_hidden1 = 128  # Intermediate layer\n",
    "    n_hidden2 = 64  # Number of components (latent features)\n",
    "    n_outputs = n_features\n",
    "    \n",
    "    split1 = n_inputs * n_hidden1\n",
    "    split2 = split1 + n_hidden1 * n_hidden2\n",
    "    split3 = split2 + n_hidden2 * n_outputs\n",
    "    \n",
    "    # Split individual into encoder and decoder parts\n",
    "    encoder_weights1 = np.array(individual[:split1]).reshape((n_inputs, n_hidden1))\n",
    "    encoder_weights2 = np.array(individual[split1:split2]).reshape((n_hidden1, n_hidden2))\n",
    "    decoder_weights = np.array(individual[split2:split3]).reshape((n_hidden2, n_outputs))\n",
    "    \n",
    "    # Encoding\n",
    "    encoded1 = np.dot(data, encoder_weights1)\n",
    "    encoded2 = np.dot(encoded1, encoder_weights2)\n",
    "    \n",
    "    # Decoding\n",
    "    decoded = np.dot(encoded2, decoder_weights)\n",
    "    \n",
    "    mse = np.mean((data - decoded)**2)\n",
    "    print(f\"Evaluated individual with MSE: {mse}\")\n",
    "\n",
    "    return mse,\n",
    "\n",
    "# Define the fitness and individual\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Define gene and individual creation operations\n",
    "toolbox.register(\"attr_float\", np.random.uniform, -10, 10)\n",
    "# toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=n_features * 64 * 2)\n",
    "\n",
    "individual_length = n_features * 128 + 128 * 64 + 64 * n_features\n",
    "\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=individual_length)\n",
    "\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define genetic operators\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "# toolbox.register(\"evaluate\", autoencoder, data=X_scaled)\n",
    "toolbox.register(\"evaluate\", autoencoder, data=X_scaled)\n",
    "# Create population and run the evolution\n",
    "population = toolbox.population(n=30)\n",
    "ngen = 50  # Number of generations\n",
    "cxpb, mutpb = 0.5, 0.2  # Probabilities of crossing and mutating\n",
    "\n",
    "# Statistics to be collected\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"min\", np.min)\n",
    "\n",
    "# Run the evolution\n",
    "# Run the evolution\n",
    "algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen, stats=stats, verbose=True)\n",
    "\n",
    "\n",
    "# Extract the best individual\n",
    "best_ind = tools.selBest(population, 1)[0]\n",
    "best_encoder_weights = np.array(best_ind[:n_features * 64]).reshape((n_features, 64))\n",
    "\n",
    "# Encode the data to 64 dimensions\n",
    "encoded_data = np.dot(X_scaled, best_encoder_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46a42bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6da09155d893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Compute the mutual information matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmi_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mutual_info_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Compute eigenvalues and eigenvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-6da09155d893>\u001b[0m in \u001b[0;36mcompute_mutual_info_matrix\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutual_info_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mmi_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmi_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmi\u001b[0m  \u001b[0;31m# The matrix is symmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36mmutual_info_regression\u001b[0;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \"\"\"\n\u001b[1;32m    367\u001b[0m     return _estimate_mi(X, y, discrete_features, False, n_neighbors,\n\u001b[0;32m--> 368\u001b[0;31m                         copy, random_state)\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[0;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[0;32m--> 286\u001b[0;31m           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[0;32m--> 286\u001b[0;31m           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36m_compute_mi\u001b[0;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_compute_mi_cd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_compute_mi_cc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36m_compute_mi_cc\u001b[0;34m(x, y, n_neighbors)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# neighbors within a specified radius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mkd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'chebyshev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_radius\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def compute_mutual_info_matrix(X):\n",
    "    n_features = X.shape[1]\n",
    "    mi_matrix = np.zeros((n_features, n_features))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        print(i, n_features)\n",
    "        for j in range(i, n_features):\n",
    "            mi = mutual_info_regression(X[:, [i]], X[:, j])[0]\n",
    "            mi_matrix[i, j] = mi\n",
    "            mi_matrix[j, i] = mi  # The matrix is symmetric\n",
    "            \n",
    "    return mi_matrix\n",
    "\n",
    "# Generate some example data\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = A.shape[0], A.shape[1]\n",
    "X = A#np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Apply the Quantile Transformer\n",
    "quantile_transformer = QuantileTransformer(n_quantiles=20, random_state=0)\n",
    "X_transformed = quantile_transformer.fit_transform(X)\n",
    "\n",
    "# Compute the mutual information matrix\n",
    "mi_matrix = compute_mutual_info_matrix(X_transformed)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = eigh(mi_matrix)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_index]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
    "\n",
    "# Choose the top k eigenvalues (and their corresponding eigenvectors)\n",
    "k = 32  # Number of components to keep\n",
    "top_eigenvectors = sorted_eigenvectors[:, :k]\n",
    "\n",
    "# Project the data into lower-dimensional space\n",
    "X_mi_based = np.dot(X_transformed, top_eigenvectors)\n",
    "\n",
    "# Print first 5 rows of transformed data\n",
    "print(\"Transformed Data (First 5 Rows):\")\n",
    "print(X_mi_based[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b52bbebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 768\n",
      "1 768\n",
      "2 768\n",
      "3 768\n",
      "4 768\n",
      "5 768\n",
      "6 768\n",
      "7 768\n",
      "8 768\n",
      "9 768\n",
      "10 768\n",
      "11 768\n",
      "12 768\n",
      "13 768\n",
      "14 768\n",
      "15 768\n",
      "16 768\n",
      "17 768\n",
      "18 768\n",
      "19 768\n",
      "20 768\n",
      "21 768\n",
      "22 768\n",
      "23 768\n",
      "24 768\n",
      "25 768\n",
      "26 768\n",
      "27 768\n",
      "28 768\n",
      "29 768\n",
      "30 768\n",
      "31 768\n",
      "32 768\n",
      "33 768\n",
      "34 768\n",
      "35 768\n",
      "36 768\n",
      "37 768\n",
      "38 768\n",
      "39 768\n",
      "40 768\n",
      "41 768\n",
      "42 768\n",
      "43 768\n",
      "44 768\n",
      "45 768\n",
      "46 768\n",
      "47 768\n",
      "48 768\n",
      "49 768\n",
      "50 768\n",
      "51 768\n",
      "52 768\n",
      "53 768\n",
      "54 768\n",
      "55 768\n",
      "56 768\n",
      "57 768\n",
      "58 768\n",
      "59 768\n",
      "60 768\n",
      "61 768\n",
      "62 768\n",
      "63 768\n",
      "64 768\n",
      "65 768\n",
      "66 768\n",
      "67 768\n",
      "68 768\n",
      "69 768\n",
      "70 768\n",
      "71 768\n",
      "72 768\n",
      "73 768\n",
      "74 768\n",
      "75 768\n",
      "76 768\n",
      "77 768\n",
      "78 768\n",
      "79 768\n",
      "80 768\n",
      "81 768\n",
      "82 768\n",
      "83 768\n",
      "84 768\n",
      "85 768\n",
      "86 768\n",
      "87 768\n",
      "88 768\n",
      "89 768\n",
      "90 768\n",
      "91 768\n",
      "92 768\n",
      "93 768\n",
      "94 768\n",
      "95 768\n",
      "96 768\n",
      "97 768\n",
      "98 768\n",
      "99 768\n",
      "100 768\n",
      "101 768\n",
      "102 768\n",
      "103 768\n",
      "104 768\n",
      "105 768\n",
      "106 768\n",
      "107 768\n",
      "108 768\n",
      "109 768\n",
      "110 768\n",
      "111 768\n",
      "112 768\n",
      "113 768\n",
      "114 768\n",
      "115 768\n",
      "116 768\n",
      "117 768\n",
      "118 768\n",
      "119 768\n",
      "120 768\n",
      "121 768\n",
      "122 768\n",
      "123 768\n",
      "124 768\n",
      "125 768\n",
      "126 768\n",
      "127 768\n",
      "128 768\n",
      "129 768\n",
      "130 768\n",
      "131 768\n",
      "132 768\n",
      "133 768\n",
      "134 768\n",
      "135 768\n",
      "136 768\n",
      "137 768\n",
      "138 768\n",
      "139 768\n",
      "140 768\n",
      "141 768\n",
      "142 768\n",
      "143 768\n",
      "144 768\n",
      "145 768\n",
      "146 768\n",
      "147 768\n",
      "148 768\n",
      "149 768\n",
      "150 768\n",
      "151 768\n",
      "152 768\n",
      "153 768\n",
      "154 768\n",
      "155 768\n",
      "156 768\n",
      "157 768\n",
      "158 768\n",
      "159 768\n",
      "160 768\n",
      "161 768\n",
      "162 768\n",
      "163 768\n",
      "164 768\n",
      "165 768\n",
      "166 768\n",
      "167 768\n",
      "168 768\n",
      "169 768\n",
      "170 768\n",
      "171 768\n",
      "172 768\n",
      "173 768\n",
      "174 768\n",
      "175 768\n",
      "176 768\n",
      "177 768\n",
      "178 768\n",
      "179 768\n",
      "180 768\n",
      "181 768\n",
      "182 768\n",
      "183 768\n",
      "184 768\n",
      "185 768\n",
      "186 768\n",
      "187 768\n",
      "188 768\n",
      "189 768\n",
      "190 768\n",
      "191 768\n",
      "192 768\n",
      "193 768\n",
      "194 768\n",
      "195 768\n",
      "196 768\n",
      "197 768\n",
      "198 768\n",
      "199 768\n",
      "200 768\n",
      "201 768\n",
      "202 768\n",
      "203 768\n",
      "204 768\n",
      "205 768\n",
      "206 768\n",
      "207 768\n",
      "208 768\n",
      "209 768\n",
      "210 768\n",
      "211 768\n",
      "212 768\n",
      "213 768\n",
      "214 768\n",
      "215 768\n",
      "216 768\n",
      "217 768\n",
      "218 768\n",
      "219 768\n",
      "220 768\n",
      "221 768\n",
      "222 768\n",
      "223 768\n",
      "224 768\n",
      "225 768\n",
      "226 768\n",
      "227 768\n",
      "228 768\n",
      "229 768\n",
      "230 768\n",
      "231 768\n",
      "232 768\n",
      "233 768\n",
      "234 768\n",
      "235 768\n",
      "236 768\n",
      "237 768\n",
      "238 768\n",
      "239 768\n",
      "240 768\n",
      "241 768\n",
      "242 768\n",
      "243 768\n",
      "244 768\n",
      "245 768\n",
      "246 768\n",
      "247 768\n",
      "248 768\n",
      "249 768\n",
      "250 768\n",
      "251 768\n",
      "252 768\n",
      "253 768\n",
      "254 768\n",
      "255 768\n",
      "256 768\n",
      "257 768\n",
      "258 768\n",
      "259 768\n",
      "260 768\n",
      "261 768\n",
      "262 768\n",
      "263 768\n",
      "264 768\n",
      "265 768\n",
      "266 768\n",
      "267 768\n",
      "268 768\n",
      "269 768\n",
      "270 768\n",
      "271 768\n",
      "272 768\n",
      "273 768\n",
      "274 768\n",
      "275 768\n",
      "276 768\n",
      "277 768\n",
      "278 768\n",
      "279 768\n",
      "280 768\n",
      "281 768\n",
      "282 768\n",
      "283 768\n",
      "284 768\n",
      "285 768\n",
      "286 768\n",
      "287 768\n",
      "288 768\n",
      "289 768\n",
      "290 768\n",
      "291 768\n",
      "292 768\n",
      "293 768\n",
      "294 768\n",
      "295 768\n",
      "296 768\n",
      "297 768\n",
      "298 768\n",
      "299 768\n",
      "300 768\n",
      "301 768\n",
      "302 768\n",
      "303 768\n",
      "304 768\n",
      "305 768\n",
      "306 768\n",
      "307 768\n",
      "308 768\n",
      "309 768\n",
      "310 768\n",
      "311 768\n",
      "312 768\n",
      "313 768\n",
      "314 768\n",
      "315 768\n",
      "316 768\n",
      "317 768\n",
      "318 768\n",
      "319 768\n",
      "320 768\n",
      "321 768\n",
      "322 768\n",
      "323 768\n",
      "324 768\n",
      "325 768\n",
      "326 768\n",
      "327 768\n",
      "328 768\n",
      "329 768\n",
      "330 768\n",
      "331 768\n",
      "332 768\n",
      "333 768\n",
      "334 768\n",
      "335 768\n",
      "336 768\n",
      "337 768\n",
      "338 768\n",
      "339 768\n",
      "340 768\n",
      "341 768\n",
      "342 768\n",
      "343 768\n",
      "344 768\n",
      "345 768\n",
      "346 768\n",
      "347 768\n",
      "348 768\n",
      "349 768\n",
      "350 768\n",
      "351 768\n",
      "352 768\n",
      "353 768\n",
      "354 768\n",
      "355 768\n",
      "356 768\n",
      "357 768\n",
      "358 768\n",
      "359 768\n",
      "360 768\n",
      "361 768\n",
      "362 768\n",
      "363 768\n",
      "364 768\n",
      "365 768\n",
      "366 768\n",
      "367 768\n",
      "368 768\n",
      "369 768\n",
      "370 768\n",
      "371 768\n",
      "372 768\n",
      "373 768\n",
      "374 768\n",
      "375 768\n",
      "376 768\n",
      "377 768\n",
      "378 768\n",
      "379 768\n",
      "380 768\n",
      "381 768\n",
      "382 768\n",
      "383 768\n",
      "384 768\n",
      "385 768\n",
      "386 768\n",
      "387 768\n",
      "388 768\n",
      "389 768\n",
      "390 768\n",
      "391 768\n",
      "392 768\n",
      "393 768\n",
      "394 768\n",
      "395 768\n",
      "396 768\n",
      "397 768\n",
      "398 768\n",
      "399 768\n",
      "400 768\n",
      "401 768\n",
      "402 768\n",
      "403 768\n",
      "404 768\n",
      "405 768\n",
      "406 768\n",
      "407 768\n",
      "408 768\n",
      "409 768\n",
      "410 768\n",
      "411 768\n",
      "412 768\n",
      "413 768\n",
      "414 768\n",
      "415 768\n",
      "416 768\n",
      "417 768\n",
      "418 768\n",
      "419 768\n",
      "420 768\n",
      "421 768\n",
      "422 768\n",
      "423 768\n",
      "424 768\n",
      "425 768\n",
      "426 768\n",
      "427 768\n",
      "428 768\n",
      "429 768\n",
      "430 768\n",
      "431 768\n",
      "432 768\n",
      "433 768\n",
      "434 768\n",
      "435 768\n",
      "436 768\n",
      "437 768\n",
      "438 768\n",
      "439 768\n",
      "440 768\n",
      "441 768\n",
      "442 768\n",
      "443 768\n",
      "444 768\n",
      "445 768\n",
      "446 768\n",
      "447 768\n",
      "448 768\n",
      "449 768\n",
      "450 768\n",
      "451 768\n",
      "452 768\n",
      "453 768\n",
      "454 768\n",
      "455 768\n",
      "456 768\n",
      "457 768\n",
      "458 768\n",
      "459 768\n",
      "460 768\n",
      "461 768\n",
      "462 768\n",
      "463 768\n",
      "464 768\n",
      "465 768\n",
      "466 768\n",
      "467 768\n",
      "468 768\n",
      "469 768\n",
      "470 768\n",
      "471 768\n",
      "472 768\n",
      "473 768\n",
      "474 768\n",
      "475 768\n",
      "476 768\n",
      "477 768\n",
      "478 768\n",
      "479 768\n",
      "480 768\n",
      "481 768\n",
      "482 768\n",
      "483 768\n",
      "484 768\n",
      "485 768\n",
      "486 768\n",
      "487 768\n",
      "488 768\n",
      "489 768\n",
      "490 768\n",
      "491 768\n",
      "492 768\n",
      "493 768\n",
      "494 768\n",
      "495 768\n",
      "496 768\n",
      "497 768\n",
      "498 768\n",
      "499 768\n",
      "500 768\n",
      "501 768\n",
      "502 768\n",
      "503 768\n",
      "504 768\n",
      "505 768\n",
      "506 768\n",
      "507 768\n",
      "508 768\n",
      "509 768\n",
      "510 768\n",
      "511 768\n",
      "512 768\n",
      "513 768\n",
      "514 768\n",
      "515 768\n",
      "516 768\n",
      "517 768\n",
      "518 768\n",
      "519 768\n",
      "520 768\n",
      "521 768\n",
      "522 768\n",
      "523 768\n",
      "524 768\n",
      "525 768\n",
      "526 768\n",
      "527 768\n",
      "528 768\n",
      "529 768\n",
      "530 768\n",
      "531 768\n",
      "532 768\n",
      "533 768\n",
      "534 768\n",
      "535 768\n",
      "536 768\n",
      "537 768\n",
      "538 768\n",
      "539 768\n",
      "540 768\n",
      "541 768\n",
      "542 768\n",
      "543 768\n",
      "544 768\n",
      "545 768\n",
      "546 768\n",
      "547 768\n",
      "548 768\n",
      "549 768\n",
      "550 768\n",
      "551 768\n",
      "552 768\n",
      "553 768\n",
      "554 768\n",
      "555 768\n",
      "556 768\n",
      "557 768\n",
      "558 768\n",
      "559 768\n",
      "560 768\n",
      "561 768\n",
      "562 768\n",
      "563 768\n",
      "564 768\n",
      "565 768\n",
      "566 768\n",
      "567 768\n",
      "568 768\n",
      "569 768\n",
      "570 768\n",
      "571 768\n",
      "572 768\n",
      "573 768\n",
      "574 768\n",
      "575 768\n",
      "576 768\n",
      "577 768\n",
      "578 768\n",
      "579 768\n",
      "580 768\n",
      "581 768\n",
      "582 768\n",
      "583 768\n",
      "584 768\n",
      "585 768\n",
      "586 768\n",
      "587 768\n",
      "588 768\n",
      "589 768\n",
      "590 768\n",
      "591 768\n",
      "592 768\n",
      "593 768\n",
      "594 768\n",
      "595 768\n",
      "596 768\n",
      "597 768\n",
      "598 768\n",
      "599 768\n",
      "600 768\n",
      "601 768\n",
      "602 768\n",
      "603 768\n",
      "604 768\n",
      "605 768\n",
      "606 768\n",
      "607 768\n",
      "608 768\n",
      "609 768\n",
      "610 768\n",
      "611 768\n",
      "612 768\n",
      "613 768\n",
      "614 768\n",
      "615 768\n",
      "616 768\n",
      "617 768\n",
      "618 768\n",
      "619 768\n",
      "620 768\n",
      "621 768\n",
      "622 768\n",
      "623 768\n",
      "624 768\n",
      "625 768\n",
      "626 768\n",
      "627 768\n",
      "628 768\n",
      "629 768\n",
      "630 768\n",
      "631 768\n",
      "632 768\n",
      "633 768\n",
      "634 768\n",
      "635 768\n",
      "636 768\n",
      "637 768\n",
      "638 768\n",
      "639 768\n",
      "640 768\n",
      "641 768\n",
      "642 768\n",
      "643 768\n",
      "644 768\n",
      "645 768\n",
      "646 768\n",
      "647 768\n",
      "648 768\n",
      "649 768\n",
      "650 768\n",
      "651 768\n",
      "652 768\n",
      "653 768\n",
      "654 768\n",
      "655 768\n",
      "656 768\n",
      "657 768\n",
      "658 768\n",
      "659 768\n",
      "660 768\n",
      "661 768\n",
      "662 768\n",
      "663 768\n",
      "664 768\n",
      "665 768\n",
      "666 768\n",
      "667 768\n",
      "668 768\n",
      "669 768\n",
      "670 768\n",
      "671 768\n",
      "672 768\n",
      "673 768\n",
      "674 768\n",
      "675 768\n",
      "676 768\n",
      "677 768\n",
      "678 768\n",
      "679 768\n",
      "680 768\n",
      "681 768\n",
      "682 768\n",
      "683 768\n",
      "684 768\n",
      "685 768\n",
      "686 768\n",
      "687 768\n",
      "688 768\n",
      "689 768\n",
      "690 768\n",
      "691 768\n",
      "692 768\n",
      "693 768\n",
      "694 768\n",
      "695 768\n",
      "696 768\n",
      "697 768\n",
      "698 768\n",
      "699 768\n",
      "700 768\n",
      "701 768\n",
      "702 768\n",
      "703 768\n",
      "704 768\n",
      "705 768\n",
      "706 768\n",
      "707 768\n",
      "708 768\n",
      "709 768\n",
      "710 768\n",
      "711 768\n",
      "712 768\n",
      "713 768\n",
      "714 768\n",
      "715 768\n",
      "716 768\n",
      "717 768\n",
      "718 768\n",
      "719 768\n",
      "720 768\n",
      "721 768\n",
      "722 768\n",
      "723 768\n",
      "724 768\n",
      "725 768\n",
      "726 768\n",
      "727 768\n",
      "728 768\n",
      "729 768\n",
      "730 768\n",
      "731 768\n",
      "732 768\n",
      "733 768\n",
      "734 768\n",
      "735 768\n",
      "736 768\n",
      "737 768\n",
      "738 768\n",
      "739 768\n",
      "740 768\n",
      "741 768\n",
      "742 768\n",
      "743 768\n",
      "744 768\n",
      "745 768\n",
      "746 768\n",
      "747 768\n",
      "748 768\n",
      "749 768\n",
      "750 768\n",
      "751 768\n",
      "752 768\n",
      "753 768\n",
      "754 768\n",
      "755 768\n",
      "756 768\n",
      "757 768\n",
      "758 768\n",
      "759 768\n",
      "760 768\n",
      "761 768\n",
      "762 768\n",
      "763 768\n",
      "764 768\n",
      "765 768\n",
      "766 768\n",
      "767 768\n",
      "[[19.50455  18.219301 21.02205  ... 19.402582 19.045582 20.007177]\n",
      " [18.219301 17.961395 20.708809 ... 19.02761  18.754932 19.515633]\n",
      " [21.02205  20.708809 20.97867  ... 19.237282 19.027079 20.010466]\n",
      " ...\n",
      " [19.402582 19.02761  19.237282 ... 19.203346 18.77469  19.945541]\n",
      " [19.045582 18.754932 19.027079 ... 18.77469  18.360336 19.048325]\n",
      " [20.007177 19.515633 20.010466 ... 19.945541 19.048325 19.612543]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to calculate mutual information\n",
    "def calc_mutual_information(x, y, bins=900, eps=1e-10):\n",
    "    joint_hist = torch.histc(x * 10 + y, bins=bins, min=0, max=400).cuda()\n",
    "    joint_hist += eps  # Add epsilon to avoid zero values\n",
    "    \n",
    "    joint_prob = joint_hist / joint_hist.sum()\n",
    "    \n",
    "    x_hist = torch.histc(x, bins=30, min=0, max=20).cuda()\n",
    "    x_hist += eps  # Add epsilon to avoid zero values\n",
    "    x_prob = x_hist / x_hist.sum()\n",
    "    \n",
    "    y_hist = torch.histc(y, bins=30, min=0, max=20).cuda()\n",
    "    y_hist += eps  # Add epsilon to avoid zero values\n",
    "    y_prob = y_hist / y_hist.sum()\n",
    "    \n",
    "    outer_x_y = torch.ger(x_prob, y_prob).cuda()\n",
    "    outer_x_y = outer_x_y.flatten()\n",
    "\n",
    "    mi = torch.sum(joint_prob * (torch.log2(joint_prob + eps) - torch.log2(outer_x_y + eps)))\n",
    "    \n",
    "    if torch.isnan(mi):\n",
    "        print(\"NaN detected in mutual information\")\n",
    "    \n",
    "    return mi\n",
    "\n",
    "\n",
    "\n",
    "# Function to compute the mutual information matrix\n",
    "def compute_mutual_info_matrix(X):\n",
    "    X_torch = torch.Tensor(X).cuda()\n",
    "    n_features = X.shape[1]\n",
    "    mi_matrix = torch.zeros((n_features, n_features)).cuda()\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        print(i, n_features)\n",
    "        for j in range(i, n_features):\n",
    "            mi = calc_mutual_information(X_torch[:, i], X_torch[:, j])\n",
    "            mi_matrix[i, j] = mi\n",
    "            mi_matrix[j, i] = mi  # The matrix is symmetric\n",
    "            \n",
    "    return mi_matrix.cpu().numpy()  # Convert back to numpy array if necessary\n",
    "\n",
    "# Make sure your data 'A' is a NumPy array\n",
    "A = A #.cpu().numpy()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "A_transformed = scaler.fit_transform(A)\n",
    "\n",
    "\n",
    "# Calculate and print the mutual information matrix\n",
    "mi_matrix = compute_mutual_info_matrix(A_transformed)\n",
    "print(mi_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4344303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data (First 5 Rows):\n",
      "[[-6.09434187e-01  1.02105319e-01  1.15341961e+00  1.19685374e-01\n",
      "   4.08342540e-01 -1.38089991e+00  3.09183407e+00 -1.22194946e+00\n",
      "  -3.84110212e-03  5.57337642e-01 -1.53433070e-01 -3.02158213e+00\n",
      "  -1.21030760e+00  4.74165231e-01  3.18425512e+00 -2.53713250e+00\n",
      "  -4.20108259e-01  3.41934711e-02  2.18286586e+00 -1.67183590e+00\n",
      "   4.64980930e-01 -6.26017809e-01  4.36029792e-01  7.30823755e-01\n",
      "   1.43280625e+00  1.55591750e+00  1.66121721e-01 -1.61234140e+00\n",
      "  -5.64761460e-01  1.95986402e+00 -8.88687015e-01 -3.62872154e-01]\n",
      " [-8.87036562e-01 -2.36873794e+00 -3.77493203e-02 -5.49288630e-01\n",
      "   1.78220093e+00 -2.98791504e+00  4.97397995e+00 -4.69161749e+00\n",
      "   1.12975442e+00 -2.12534726e-01 -1.04841971e+00 -4.67019367e+00\n",
      "  -7.06262589e-01  1.28673565e+00  5.74570656e+00 -4.97799206e+00\n",
      "   9.60308909e-02  3.27860618e+00 -1.33796859e+00 -2.92274237e+00\n",
      "   5.18779576e-01 -1.15604615e+00 -7.65139937e-01 -2.84464121e-01\n",
      "   1.16572464e+00  1.50683165e+00  1.21811438e+00  1.69400334e+00\n",
      "   4.10213709e+00  1.53751469e+00 -7.57114291e-02 -2.90270495e+00]\n",
      " [-1.01469564e+00 -2.15287209e+00  1.16529417e+00 -1.41421640e+00\n",
      "   1.52066052e+00 -2.17076683e+00  5.25859451e+00 -5.75924778e+00\n",
      "   7.79685676e-02 -6.86423600e-01  3.12465239e+00 -2.49364805e+00\n",
      "  -2.36222458e+00  8.14171791e-01  6.79679394e+00 -3.28258705e+00\n",
      "  -1.07877016e-01  2.42620602e-01 -6.05875850e-02 -2.73732257e+00\n",
      "   1.91910833e-01 -4.55005348e-01 -4.03426051e-01  1.25869286e+00\n",
      "  -4.04296875e-01  1.13684356e+00  1.16829538e+00 -1.38267779e+00\n",
      "   1.72078025e+00 -2.98329979e-01 -3.51930708e-02  9.99590755e-02]\n",
      " [-4.19115007e-01 -1.26228547e+00  1.35746884e+00  4.02827442e-01\n",
      "   1.29594004e+00 -1.41482031e+00  5.13415623e+00 -3.64222360e+00\n",
      "   2.19757855e-01  2.66980934e+00 -3.87294006e+00 -3.60135555e+00\n",
      "   2.35273075e+00 -4.58866835e+00 -3.88262486e+00  3.95946264e+00\n",
      "   7.51203060e-01 -4.52287316e-01  2.35966945e+00  1.53215981e+00\n",
      "   7.22980261e-01 -1.51135218e+00  2.30067372e-01  1.36997700e-02\n",
      "   5.67416310e-01  6.73596621e-01  3.73377919e-01 -7.32148767e-01\n",
      "  -1.43675423e+00 -3.97250891e-01 -1.03411961e+00  6.64389372e-01]\n",
      " [-7.15290844e-01 -1.59036088e+00 -1.40395260e+00 -1.09890223e+00\n",
      "   1.71596408e+00 -1.96864164e+00  4.86767197e+00 -6.41598749e+00\n",
      "  -3.02167296e-01  5.75206518e-01  7.97451198e-01 -3.24635363e+00\n",
      "  -1.33408457e-01 -7.25566447e-01  4.69646025e+00 -2.65567136e+00\n",
      "   4.69123542e-01  1.23663497e+00 -1.62411475e+00 -8.91950667e-01\n",
      "  -1.01242065e+00 -4.94689226e-01 -1.41486788e+00  2.96175098e+00\n",
      "  -8.88361692e-01  4.61942166e-01 -8.51414919e-01 -3.44160914e-01\n",
      "   4.09416103e+00 -9.65253174e-01  2.12246642e-01 -1.67986715e+00]]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = eigh(mi_matrix)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_index]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
    "\n",
    "# Choose the top k eigenvalues (and their corresponding eigenvectors)\n",
    "k = 128  # Number of components to keep\n",
    "top_eigenvectors = sorted_eigenvectors[:, :k]\n",
    "\n",
    "\n",
    "# quantile_transformer = QuantileTransformer(n_quantiles=20, random_state=0)\n",
    "# A_transformed = quantile_transformer.fit_transform(A)\n",
    "\n",
    "# Project the data into lower-dimensional space\n",
    "X_mi_based = np.dot(A_transformed, top_eigenvectors)\n",
    "\n",
    "# Print first 5 rows of transformed data\n",
    "print(\"Transformed Data (First 5 Rows):\")\n",
    "print(X_mi_based[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ebcfab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('TART_features.npy', X_mi_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76d537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
