{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d02288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_features(model_name):\n",
    "    return np.load(f'{model_name}_features.npy')\n",
    "dt = pd.read_csv('q-merged-sampled-bin.csv')\n",
    "# Define the list of model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1e236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.fulltext[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d341d847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAd10lEQVR4nO3df7hcVX3v8fdHqBASCEHgRKIYUhUoxHrjwRolcKANxh+9T0Vaa4IGrTdWCgWTAsH6YPDxB9ImQiPVxFsNxETqI1UQFIjXHuBKQJP6I7EQW024N4n5BSGQEH+E+71/rDXNZpzDmZMzyaw5+byeZz8zs/faa9ZeZ7I/2Xuv2aOIwMzMrDQvaHcDzMzMGnFAmZlZkRxQZmZWJAeUmZkVyQFlZmZFOrTdDWiFY489NsaOHdvuZljFrl27GD58eLubMSS4L1vL/dlarejPlStXbouI4+rnD4mAGjt2LCtWrGh3M6yit7eXnp6edjdjSHBftpb7s7Va0Z+SHms036f4zMysSA4oMzMrkgPKzMyK5IAyM7MiOaDMzKxI/QaUpDmSom7aVFmuXGajpN2SeiWdVlfHKEmLJe3I02JJR9eVGS/pvlzHBknXSFLLttTMzDpKs0dQa4AXV6bxlWVXArOAS4EzgC3AMklHVsosBSYAU/I0AVhcWyjpKGAZsDnXcRlwBTBzwFtkZmZDQrPfg9oTEZvqZ+YjnMuB6yLitjxvOimkpgILJJ1KCqUzI2J5LvN+4AFJJ0fEGmAacAQwPSJ2A6slnQLMlDQv/JsgZmYHnWaPoMblU3hrJd0qaVyefxIwGri3VjAHzP3A6/OsicBO4MFKfd8FdtWVeSCvW3MPcAIwtvnNMTOzoaKZI6iHgYuAR4HjgQ8DD+brTKNzmc1162wGxuTno4Gt1aOgiAhJWyrrjwbWN6ijtmxtfaMkzQBmAHR1ddHb29vEpvRt1YYdg1q/VcaPGdnuJrSkL7qGwfwltw+qjhL6ogRbntgx6L5shRL+Hv5s7lXKPuukkYcMev/bl34DKiK+VX0t6SHg58B04KH90qomRMRCYCFAd3d3DPZWGxfNvqsFrRq8ddN62t2ElvTFrPF7mLtqcHfSKqEvSjB/ye2D7stWKOHv4c/mXqXssxZNGb7fbh014GHmEbET+AnwCqB2XaqrrlhXZdkm4LjqiLz8/Pi6Mo3qoFLGzMwOIgMOKEmHA6cAvyCdetsETK5bPom915yWAyNI15lqJgLD68pMyuvWTAY2AusG2kYzM+t8zXwP6u8lnS3pJEl/AHyVFC435+tKNwBXSTpf0unAItKgiKUAEfEIcDdpRN9ESROBBcCdeQQfuewzwCJJp0s6H5gNeASfmdlBqpkTsS8BvgwcC2wlXXd6XUTUbo9+PTAMuAkYRRpUcV5EPF2pYyownzQyD+AO4JLawojYIWlyrmMFsB2YC8zbt80yM7NO18wgiT/vZ3kAc/LUV5ntwIX91LMKOKu/9piZ2cHB9+IzM7MiOaDMzKxIDigzMyuSA8rMzIrkgDIzsyI5oMzMrEgOKDMzK5IDyszMiuSAMjOzIjmgzMysSA4oMzMrkgPKzMyK5IAyM7MiOaDMzKxIDigzMyuSA8rMzIrkgDIzsyI5oMzMrEgOKDMzK5IDyszMiuSAMjOzIjmgzMysSA4oMzMrkgPKzMyK5IAyM7MiOaDMzKxIDigzMyuSA8rMzIrkgDIzsyI5oMzMrEgOKDMzK5IDyszMijTggJJ0taSQ9JnKPEmaI2mjpN2SeiWdVrfeKEmLJe3I02JJR9eVGS/pvlzHBknXSNI+b52ZmXWsAQWUpNcBM4Af1y26EpgFXAqcAWwBlkk6slJmKTABmJKnCcDiSt1HAcuAzbmOy4ArgJkDaaOZmQ0NTQeUpJHAEuC9wPbKfAGXA9dFxG0RsRqYDhwJTM1lTiWF0oyIWB4Ry4H3A2+VdHKuahpwBDA9IlZHxFeBTwEzfRRlZnbwUUQ0V1D6Z2BdRFwlqRdYHRGXSBoH/Ax4bUR8v1L+LmBbREyX9F7gRuCoyG+YQ+dp4NKI+KKkW4AXRcRbKnWcAXwPGBcRa+vaM4N0NEdXV9drbr311n3sgmTVhh2DWr9Vxo8Z2e4mtKQvuobB5t2Dq6OEvijBlid2DLovW6GEv4c/m3uVss86aeQhjBgxYlB1nHPOOSsjort+/qHNrCzpfwAvBy5ssHh0ftxcN38zMKZSZmtU0jAiQtKWyvqjgfUN6qgte05ARcRCYCFAd3d39PT0NLMpfbpo9l2DWr9V1k3raXcTWtIXs8bvYe6qpj5efSqhL0owf8ntg+7LVijh7+HP5l6l7LMWTRnOYPe/fen3r5RPwX0CODMifrNfWmFmZlanmWtQE4FjgZ9I2iNpD3A2cHF+/ngu11W3XhewKT/fBBxXvZaUnx9fV6ZRHVTKmJnZQaKZgPo6MB54dWVaAdyan/+UFCCTaytIOhyYBDyYZy0HRpDCrmYiMLyuzKS8bs1kYCOwrrnNMTOzoaLfU3wR8STwZHWepF3AE3nEHpJuAD4k6VFSYH0Y2EkaWk5EPCLpbmBBHtwAsAC4MyLW5NdLgY8AiyR9DHglMBu4NpodyWFmZkNGq668Xg8MA24CRgEPA+dFxNOVMlOB+cA9+fUdwCW1hRGxQ9LkXMcK0lD2ucC8FrXRzMw6yD4FVET01L0OYE6e+lpnO41HAVbLrALO2pc2mZnZ0OJ78ZmZWZEcUGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRHFBmZlYkB5SZmRXJAWVmZkVyQJmZWZEcUGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRHFBmZlYkB5SZmRXJAWVmZkVyQJmZWZEcUGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVqd+AkvRXkn4s6ak8LZf0lspySZojaaOk3ZJ6JZ1WV8coSYsl7cjTYklH15UZL+m+XMcGSddIUsu21MzMOkozR1DrgauACUA38B3g65JelZdfCcwCLgXOALYAyyQdWaljaV5/Sp4mAItrCyUdBSwDNuc6LgOuAGbu64aZmVlnO7S/AhFxe92sv5X0AWCipFXA5cB1EXEbgKTppJCaCiyQdCoplM6MiOW5zPuBBySdHBFrgGnAEcD0iNgNrJZ0CjBT0ryIiFZsrJmZdY4BXYOSdIikPwdGAA8CJwGjgXtrZXLA3A+8Ps+aCOzM5Wu+C+yqK/NAXrfmHuAEYOxA2mhmZkNDv0dQkK4PAcuBw0lh87aIWCWpFjCb61bZDIzJz0cDW6tHQRERkrbkZbUy6xvUUVu2tkGbZgAzALq6uujt7W1mU/o0a/yeQa3fKoPdjlZoRV90DRt8PSX0RQla0ZetUMLfw5/NvUr4TADs3Llzv/VHUwEFrAFeDYwELgBultSzX1rUpIhYCCwE6O7ujp6ewTXnotl3taBVg7duWk+7m9CSvpg1fg9zVzX78WqshL4owfwltw+6L1uhhL+HP5t7lbLPWjRlOIPd//alqVN8EfHriPjPiFgZEVcDPwQ+CGzKRbrqVumqLNsEHFcdkZefH19XplEdVMqYmdlBZF+/B/UC4DDSqbdNwOTaAkmHA5PYe81pOema1cTK+hOB4XVlJuV1ayYDG4F1+9hGMzPrYM18D+o6SZMkjc3fVfok0AMsydeVbgCuknS+pNOBRaTrVEsBIuIR4G7SiL6JkiYCC4A78wg+ctlngEWSTpd0PjAb8Ag+M7ODVDMnYkcDX8qPO4AfA2+KiHvy8uuBYcBNwCjgYeC8iHi6UsdUYD5pZB7AHcAltYURsUPS5FzHCmA7MBeYt2+bZWZmna6Z70Fd1M/yAObkqa8y24EL+6lnFXBWf+0xM7ODg+/FZ2ZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRHFBmZlYkB5SZmRXJAWVmZkVyQJmZWZEcUGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRHFBmZlYkB5SZmRXJAWVmZkVyQJmZWZEcUGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRHFBmZlakfgNK0tWSvi/pKUlbJX1D0ul1ZSRpjqSNknZL6pV0Wl2ZUZIWS9qRp8WSjq4rM17SfbmODZKukaSWbKmZmXWUZo6geoB/BF4PnAvsAb4t6ZhKmSuBWcClwBnAFmCZpCMrZZYCE4ApeZoALK4tlHQUsAzYnOu4DLgCmLkP22VmZh3u0P4KRMQbq68lvQvYAbwB+EY+wrkcuC4ibstlppNCaiqwQNKppFA6MyKW5zLvBx6QdHJErAGmAUcA0yNiN7Ba0inATEnzIiJassVmZtYR9uUa1JF5ve359UnAaODeWoEcMPeTjroAJgI7gQcr9XwX2FVX5oG8bs09wAnA2H1op5mZdbB+j6AauBH4IbA8vx6dHzfXldsMjKmU2Vo9CoqIkLSlsv5oYH2DOmrL1lYXSJoBzADo6uqit7d3HzZlr1nj9wxq/VYZ7Ha0Qiv6omvY4OspoS9K0Iq+bIUS/h7+bO5VwmcCYOfOnfutPwYUUJLmAWeSTtU9u19a1KSIWAgsBOju7o6enp5B1XfR7Lta0KrBWzetp91NaElfzBq/h7mr9uX/P3uV0BclmL/k9kH3ZSuU8PfwZ3OvUvZZi6YMZ7D73740fYpP0qeBdwLnRsTPK4s25ceuulW6Kss2AcdVR+Tl58fXlWlUR/U9zMzsINFUQEm6kb3h9Gjd4rWkAJlcKX84MIm915yWAyNI15lqJgLD68pMyuvWTAY2AuuaaaeZmQ0dzXwP6ibgPaQRedsljc7TCEjXkoAbgKsknZ+/I7WINChiaS7zCHA3aUTfREkTgQXAnXkEH7nsM8AiSadLOh+YDXgEn5nZQaiZE7EX58f/VTf/WmBOfn49MAy4CRgFPAycFxFPV8pPBeaTRuYB3AFcUlsYETskTc51rCCNEpwLzGtyW8zMbAhp5ntQ/d7JIR/hzGFvYDUqsx24sJ96VgFn9fd+ZmY29PlefGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRHFBmZlYkB5SZmRXJAWVmZkVyQJmZWZEcUGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRHFBmZlYkB5SZmRXJAWVmZkVyQJmZWZEcUGZmViQHlJmZFckBZWZmRXJAmZlZkRxQZmZWJAeUmZkVyQFlZmZFckCZmVmRmgooSWdJukPSBkkh6aK65ZI0R9JGSbsl9Uo6ra7MKEmLJe3I02JJR9eVGS/pvlzHBknXSNJgN9LMzDpPs0dQI4DVwGXA7gbLrwRmAZcCZwBbgGWSjqyUWQpMAKbkaQKwuLZQ0lHAMmBzruMy4ApgZvObY2ZmQ8WhzRSKiG8C3wSQtKi6LB/hXA5cFxG35XnTSSE1FVgg6VRSKJ0ZEctzmfcDD0g6OSLWANOAI4DpEbEbWC3pFGCmpHkREYPdWDMz6xwa6H5f0k7gkohYlF+PA34GvDYivl8pdxewLSKmS3ovcCNwVC1ocrA9DVwaEV+UdAvwooh4S6WOM4DvAeMiYm1dO2YAMwC6urpec+uttw5sy+us2rBjUOu3yvgxI9vdhJb0Rdcw2NzoWHsASuiLEmx5Yseg+7IVSvh7+LO5Vyn7rJNGHsKIESMGVcc555yzMiK66+c3dQTVj9H5cXPd/M3AmEqZrdWjoIgISVsq648G1jeoo7bsOQEVEQuBhQDd3d3R09MziE2Ai2bfNaj1W2XdtJ52N6ElfTFr/B7mrhrcx6uEvijB/CW3D7ovW6GEv4c/m3uVss9aNGU4g93/9sWj+MzMrEitCKhN+bGrbn5XZdkm4LjqiLz8/Pi6Mo3qqL6HmZkdJFoRUGtJATK5NkPS4cAk4ME8azlpJODEynoTgeF1ZSbldWsmAxuBdS1op5mZdZBmvwc1QtKrJb06r3Nifn1ivq50A3CVpPMlnQ4sAnaShpYTEY8Ad5NG9E2UNBFYANyZR/CRyz4DLJJ0uqTzgdmAR/CZmR2Emj2C6gZ+kKdhwLX5+Ufz8uuBTwM3ASuAFwPnRcTTlTqmAj8C7snTj4B31RZGxA7SEdMJuY6bgLnAvH3YLjMz63DNfg+qF+jzjg75CGdOnvoqsx24sJ/3WQWc1UybzMxsaPMoPjMzK5IDyszMiuSAMjOzIjmgzMysSA4oMzMrkgPKzMyK5IAyM7MiOaDMzKxIDigzMyuSA8rMzIrkgDIzsyI5oMzMrEgOKDMzK5IDyszMiuSAMjOzIjmgzMysSA4oMzMrkgPKzMyK5IAyM7MiOaDMzKxIDigzMyuSA8rMzIrkgDIzsyI5oMzMrEgOKDMzK5IDyszMiuSAMjOzIjmgzMysSA4oMzMrkgPKzMyK5IAyM7MiFRdQki6WtFbSLyWtlDSp3W0yM7MDr6iAkvQO4EbgE8B/Ax4EviXpxLY2zMzMDriiAgqYCSyKiM9HxCMRcSnwC+ADbW6XmZkdYMUElKQXAq8B7q1bdC/w+gPfIjMzaydFRLvbAICkE4ANwNkRcX9l/jXAtIg4ua78DGBGfnkysOZAtdWaciywrd2NGCLcl63l/mytVvTnyyLiuPqZhw6y0raJiIXAwna3wxqTtCIiutvdjqHAfdla7s/W2p/9WcwpPlICPwt01c3vAjYd+OaYmVk7FRNQEfFrYCUwuW7RZNJoPjMzO4iUdopvHrBY0veA7wJ/CZwAfK6trbJ94dOvreO+bC33Z2vtt/4sZpBEjaSLgSuBFwOrgQ9WB02YmdnBobiAMjMzg4KuQZmZmVU5oKwlJL1E0nxJyyU9IykkjW13uzqVpAsk3SbpMUm7Ja2R9ElJR7a7bZ1G0hslfUfSJkm/krRe0lck/V672zZUSLo7/5v/WCvrdUBZq7wc+DNgO/BAm9syFPwN6WsXHwKmAJ8l3fJrmST/ux2YY0gjhC8BzgOuBk4DHpL0snY2bCiQ9E7g9/dH3aWN4rPOdX9EdAFIeh9pR2D77o8jYmvl9X2SngBuBnqA77SlVR0oIr4MfLk6L48UfhS4AJjbjnYNBZJGAZ8GPggsbXX9/p+YtURE/L92t2EoqQunmu/nxzEHsi1D1OP5cU9bW9H5PgWszv8JaDkfQZl1jrPz4yNtbUWHknQIcAjwMuA60h1q9suO9WAg6Uzg3eyn03vggDLrCJLGAB8Fvh0RK9rdng71MOkXEwD+Ezg3Ira0sT0dK//6xALg7yNiv92o26f4zAonaQRwO+l01Hva3JxO9i7gdcBU4CnSgJOxbW1R57oSGAZ8fH++iY+gzAomaRjwDWAc6ado1re5SR0rImqnRh+W9C1gHTCbdEs1a1L+hfO/Bd4HHCbpsMriwyQdDTwdEc8O9r18BGVWKEm/A3wV6AbeHBGr2tykISMiniSd5nt5m5vSicYBhwNfIn2tpDZB+nrEdmB8K97IR1BmBcrfdVoCnAu8NSIeanOThhRJXcAppD62gfkhcE6D+f9KCq1/IoX/oDmgrGUkXZCf1i5Ev0nSVmBrRNzXpmZ1qpuAPyWd498l6XWVZet9qq95kr4G/BvwY9K1p1eSvrezB38HasDy0Wdv/XxJAI9FxG8t21e+Way1jKS+Pkz3RUTPgWxLp5O0jjQcupFrI2LOgWtNZ5N0FekuJ78LvBD4v6Qd7CcjYl37Wja05H//H4+ID7esTgeUmZmVyIMkzMysSA4oMzMrkgPKzMyK5IAyM7MiOaDMzKxIDigzMyuSA8qsjSTNyT+VPbbdbamSNDa3a06722IHLweUdRxJ4yQtlPSopGckbZf0iKSbJTW6BUtHkdSTg+vodrfFrJ18qyPrKJK6gfuA3wC3AD8h3fb/FaSfmX+adE+wTvEx0o/n/aoyrwf4CLAIePKAt8isEA4o6zQfAY4AXh0RP6pfKGn0gW/SwEk6MiKejog9+GfHzRryKT7rNK8AHm8UTgARsal+nqQ/knSvpCcl/VLSjyX9ZV2ZhyVtlvRb/2mT9MZ8PebyyjxJ+oCklfk0405J/1p/irF6LUfSO3L53cD8vPw516AkLSKFMMDavKy2/gfz88kN2niYpMclfed5e29v+bdL6s198oykNZL+If9S6vOtd3Huyw2Sfi3pF5K+1OgamqS3SLpP0jZJuyX9H0n/IumVlTIvlfQFSY9J+pWkLZIelDS9me2woc0BZZ3mZ8CLJJ3fTGFJM4B7gRGkO4PPzHV8VtLfVYreDBwPTGlQzbtJRzlLK/MWA58h/azAlaRQGUn6ldb/3qCOPwE+C9wN/DXwrT6avAD4Wn7+QdKvwL4L+BfSKc1fAe9tsN7bgGOA/9lHvf9F0sdJvzN1HPBp4HLg68CbSUenz+dvgG3APwB/BXwlv/eDkl5UeY+zgTuAo4FPApcAnwdeRP4NpvyfgWWku7bfClxMOt35U2BSf9thB4GI8OSpYyZgIvBrIEg7si8AHwBObVD2xcAvgaUNlt0IPAuMy6+PIe38v1JX7khgF3BHZd7b8vvPqCt7KLACWMveGzGPzWV/00cb5+TlY59vXmXZ0rxNx9TNXwY8ARzeT/+9Ntf9nfqygBq0e05dmeEN6vzDXPbKyrx5ed7xz9OWV9Wv58lTdfIRlHWUiFhO+r2pm0lHLO8B/hH4d0n3SxpXKX4BcBjwT5KOrU6kn1F/AfBHud4n8rw/rhs9dwHpqOLmyrwLSYMxvl5X59G5jrGkU5FVd8XenxwfjIV5m6bVZuTTa38ILImIX/azfm29q+vLRvZ8K0fErvyeL5A0Mm/3j4AdwB9Uiu7Ij29vdNq0rsw5ko7vp912EHJAWceJiFURcVFEdJHCYDrwAOm00O2V6yin5sdvA1vrpmV5WVel6ptJP2X9Z5V57yb9hPU3KvNOJR1ZbW5Q75wG9UI62hu0SD8G91PgLyqz30M6+un39B4pOIMUKgMm6VxJvaSjyifZu90jgVGVop8BfkD6z8MTkr4p6a8lHVfZlsdIp13PA36Rr89dL+mMfWmbDT0exWcdLe/kbpG0mBRSbyCdxvrfpJ02pJD5RR9V/Lzy/Fukne27gYWSTgTOBj4XEb+ulFMuN/V5mra67vUz/W9N0z4P/J2k15BC4CJgRfQxcKSByNOA5OC4l3TdbTbpVObuXNetVP7DGxGP5/KTgMnAWaTrXddKenM+EiYiPizpC8Bbctn3AVdIuj4irhpoG21ocUDZkBARIelhUkCNybP/Iz9ui4hvN1HHHklLgcvyqcJ3ksLo5rqi/0H62fCHImJnSzagrin9LF9EOvL4C+B24ETSQIRm/BR4E/D7wPcG2K6pwCHAmyJibW2mpOE89+gJgIh4lvTLtb253KuAlcCHSYFUK/dz0qjG+ZIOB+4BrpQ0NyK2DLCNNoT4FJ91FEmT+xgKPox0qgjg3/PjV0gDH67Ny+vXGSnpsLrZtTB6N2n03JqIeLiuzC2kfzsNQ0FS/em9gaqF3jGNFkbENtKou6mk0XHP8NwRhs+nVu4TjYaUS1L9vIpna8Xq5n+Iun1JvjZV71HSEdcxucxISb9TLZCvi9Wu1f1W6NnBxUdQ1mk+TRpmfgewirRzfilpZ/1K4JaIWAUQEeslfYB0beaRfBrwMdLw6vGkod+/B6yrVR4RP5C0ijTE+yjSzvc5IuKrkr4IXCJpAnAnaej1S0ijDF8OjKtfbwAeyo+fkrSENGpvdURUTxsuJF0reytwc0Q81UzFEfE9SZ8CrgL+TdI/A5uAk0gDQl5L33ev+BqpX74paSFpNOVk0mi8bXVlPy/pJaRTgo+R7vbxDtK1u1tymXNIp1JvA9aQgvk1pNN8D0fEmma2yYawdg8j9ORpIBPpKOkm0kX+baTvJz1Our3Re4EXNFjnDaSd6xbSTnVjLj+LBsOy8/wgHTG89Hna8i7Sda+nSCGyjvR9pXdUyoylwXDtyvI5NBhSTvpu1c9Jw9MbDfcW6VRjAJP2oR/fCXyXNBpxF+no5gbghc/XblKor8zrbCNdezoxb3tvpdz5pO9BrScdxW4l3aLq7ZUyJwGfIx0xPZXrfAT4KDCy3Z81T+2fat95MLMOI+knwCERcUq722K2P/galFkHknQu6fTk59vdFrP9xUdQZh0kB9PvAleTbt/08mjy+pNZp/EgCbPOcg1wJmmk4nSHkw1lPoIyM7Mi+RqUmZkVyQFlZmZFckCZmVmRHFBmZlYkB5SZmRXp/wOky4jjNTY7ZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(dt.Crash_Severity)\n",
    "plt.xticks(np.arange(1,5),fontsize=16)\n",
    "plt.xlabel('Severity class',fontsize=18)\n",
    "plt.grid()\n",
    "plt.yticks(np.arange(0,6000,1000),fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('S-hist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a873eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/agrigore/ENV/lib/python3.6/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: numpy in /home/agrigore/ENV/lib/python3.6/site-packages (from lightgbm) (1.19.5)\r\n",
      "Requirement already satisfied: scipy in /home/agrigore/ENV/lib/python3.6/site-packages (from lightgbm) (1.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cbc51ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for language model: bert...\n",
      "(25000, 768)\n",
      "bert report\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3085\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3078\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3077\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3082\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3077\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3079\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3082\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3085\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3076\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3079\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.5863600000000001,\n",
      "\tAverage F1 Score: 0.562395381046802,\n",
      "\tAverage Precision: 0.5771633822640627,\n",
      "\tAverage Recall: 0.58636\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46580000000000005,\n",
      "\tAverage F1 Score: 0.44809567734579014,\n",
      "\tAverage Precision: 0.4416900856028672,\n",
      "\tAverage Recall: 0.46580000000000005\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.65764,\n",
      "\tAverage F1 Score: 0.6497691915034212,\n",
      "\tAverage Precision: 0.6539529480603995,\n",
      "\tAverage Recall: 0.65764\n",
      "\n",
      "Cross-validating XGBoost...\n",
      "[06:14:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:14:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:14:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:14:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:14:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:15:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:15:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:15:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:15:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:15:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.60848,\n",
      "\tAverage F1 Score: 0.5923663601486424,\n",
      "\tAverage Precision: 0.6018680715079624,\n",
      "\tAverage Recall: 0.60848\n",
      "\n",
      "bert NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046783 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.57756,\n",
      "\tAverage F1 Score: 0.5724735705487674,\n",
      "\tAverage Precision: 0.5698632516519392,\n",
      "\tAverage Recall: 0.57756\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46088000000000007,\n",
      "\tAverage F1 Score: 0.4429851011725505,\n",
      "\tAverage Precision: 0.43776627835220383,\n",
      "\tAverage Recall: 0.46088000000000007\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.5980000000000001,\n",
      "\tAverage F1 Score: 0.5963164697924188,\n",
      "\tAverage Precision: 0.5968466575505501,\n",
      "\tAverage Recall: 0.5980000000000001\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:16:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:17:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:18:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:18:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:19:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:19:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:20:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:21:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:21:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:22:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.59528,\n",
      "\tAverage F1 Score: 0.5932546187521871,\n",
      "\tAverage Precision: 0.5921958104011699,\n",
      "\tAverage Recall: 0.5952799999999999\n",
      "\n",
      "bert report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198918\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6247999999999999,\n",
      "\tAverage F1 Score: 0.6103435788451503,\n",
      "\tAverage Precision: 0.6159405497193944,\n",
      "\tAverage Recall: 0.6248\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46580000000000005,\n",
      "\tAverage F1 Score: 0.44809567734579014,\n",
      "\tAverage Precision: 0.4416900856028672,\n",
      "\tAverage Recall: 0.46580000000000005\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.6386000000000001,\n",
      "\tAverage F1 Score: 0.6323461404438279,\n",
      "\tAverage Precision: 0.6300466010910981,\n",
      "\tAverage Recall: 0.6386000000000001\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:25:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:26:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:26:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:27:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:29:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:29:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6447199999999998,\n",
      "\tAverage F1 Score: 0.6354548368812949,\n",
      "\tAverage Precision: 0.6386250608029425,\n",
      "\tAverage Recall: 0.64472\n",
      "\n",
      "Loading features for language model: bert-large...\n",
      "(25000, 1024)\n",
      "bert-large NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.60124,\n",
      "\tAverage F1 Score: 0.5966260330215676,\n",
      "\tAverage Precision: 0.5949583443167764,\n",
      "\tAverage Recall: 0.60124\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46464,\n",
      "\tAverage F1 Score: 0.44773637386072707,\n",
      "\tAverage Precision: 0.4420938873874632,\n",
      "\tAverage Recall: 0.46464\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.61128,\n",
      "\tAverage F1 Score: 0.6101501565672649,\n",
      "\tAverage Precision: 0.611855207920446,\n",
      "\tAverage Recall: 0.61128\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:32:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:33:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:33:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:34:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:35:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:36:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:36:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:37:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:39:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6157199999999999,\n",
      "\tAverage F1 Score: 0.6138927403269877,\n",
      "\tAverage Precision: 0.6127914355503502,\n",
      "\tAverage Recall: 0.6157199999999999\n",
      "\n",
      "bert-large report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264205\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264198\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264197\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264202\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264197\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264199\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264202\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264205\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264196\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264199\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6426000000000001,\n",
      "\tAverage F1 Score: 0.6299693647740976,\n",
      "\tAverage Precision: 0.6322163910628167,\n",
      "\tAverage Recall: 0.6426000000000001\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46588,\n",
      "\tAverage F1 Score: 0.44817903855902974,\n",
      "\tAverage Precision: 0.44177651757649467,\n",
      "\tAverage Recall: 0.4658800000000001\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.6501999999999999,\n",
      "\tAverage F1 Score: 0.6447032589858165,\n",
      "\tAverage Precision: 0.6424047225066876,\n",
      "\tAverage Recall: 0.6502\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:41:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:42:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:44:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:44:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:45:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:46:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:47:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:48:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:48:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6608400000000001,\n",
      "\tAverage F1 Score: 0.652009471449132,\n",
      "\tAverage Precision: 0.6532236681316143,\n",
      "\tAverage Recall: 0.6608400000000001\n",
      "\n",
      "Loading features for language model: gpt2...\n",
      "(25000, 768)\n",
      "gpt2 NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.2,\n",
      "\tAverage F1 Score: 0.06666666666666668,\n",
      "\tAverage Precision: 0.039999999999999994,\n",
      "\tAverage Recall: 0.2\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.2,\n",
      "\tAverage F1 Score: 0.06666666666666668,\n",
      "\tAverage Precision: 0.039999999999999994,\n",
      "\tAverage Recall: 0.2\n",
      "\n",
      "Cross-validating Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.2,\n",
      "\tAverage F1 Score: 0.06666666666666668,\n",
      "\tAverage Precision: 0.039999999999999994,\n",
      "\tAverage Recall: 0.2\n",
      "\n",
      "Cross-validating XGBoost...\n",
      "[06:50:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.2,\n",
      "\tAverage F1 Score: 0.06666666666666668,\n",
      "\tAverage Precision: 0.039999999999999994,\n",
      "\tAverage Recall: 0.2\n",
      "\n",
      "gpt2 report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005504 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3085\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3078\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3077\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3082\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3077\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3079\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3082\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3085\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3076\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3079\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.5863600000000001,\n",
      "\tAverage F1 Score: 0.562395381046802,\n",
      "\tAverage Precision: 0.5771633822640627,\n",
      "\tAverage Recall: 0.58636\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46580000000000005,\n",
      "\tAverage F1 Score: 0.44809567734579014,\n",
      "\tAverage Precision: 0.4416900856028672,\n",
      "\tAverage Recall: 0.46580000000000005\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.65316,\n",
      "\tAverage F1 Score: 0.6473924704203301,\n",
      "\tAverage Precision: 0.6491949294881479,\n",
      "\tAverage Recall: 0.65316\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:51:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:51:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:51:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:51:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:51:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:51:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:52:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:52:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:52:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:52:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.60848,\n",
      "\tAverage F1 Score: 0.5923663601486424,\n",
      "\tAverage Precision: 0.6018680715079624,\n",
      "\tAverage Recall: 0.60848\n",
      "\n",
      "Loading features for language model: roberta...\n",
      "(25000, 768)\n",
      "roberta NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.60068,\n",
      "\tAverage F1 Score: 0.5967739375049002,\n",
      "\tAverage Precision: 0.5953258961846222,\n",
      "\tAverage Recall: 0.60068\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46208,\n",
      "\tAverage F1 Score: 0.4433563046807171,\n",
      "\tAverage Precision: 0.4381207253731315,\n",
      "\tAverage Recall: 0.46208\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.60608,\n",
      "\tAverage F1 Score: 0.605523936609585,\n",
      "\tAverage Precision: 0.6080732680405319,\n",
      "\tAverage Recall: 0.60608\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:53:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:54:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:55:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:55:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:56:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:56:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:57:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:58:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:58:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:59:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.61324,\n",
      "\tAverage F1 Score: 0.6116759035348773,\n",
      "\tAverage Precision: 0.6107739039954705,\n",
      "\tAverage Recall: 0.61324\n",
      "\n",
      "roberta report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198918\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6436400000000001,\n",
      "\tAverage F1 Score: 0.6307396227281582,\n",
      "\tAverage Precision: 0.6350353645797515,\n",
      "\tAverage Recall: 0.6436400000000002\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46580000000000005,\n",
      "\tAverage F1 Score: 0.44809567734579014,\n",
      "\tAverage Precision: 0.4416900856028672,\n",
      "\tAverage Recall: 0.46580000000000005\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.65084,\n",
      "\tAverage F1 Score: 0.6446070080308319,\n",
      "\tAverage Precision: 0.6426040881687263,\n",
      "\tAverage Recall: 0.6508399999999999\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:01:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:02:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:02:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:03:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:03:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:04:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:05:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:05:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:06:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:06:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.66248,\n",
      "\tAverage F1 Score: 0.6538230923442533,\n",
      "\tAverage Precision: 0.6563331804798266,\n",
      "\tAverage Recall: 0.66248\n",
      "\n",
      "Loading features for language model: roberta-large...\n",
      "(25000, 1024)\n",
      "roberta-large NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.59304,\n",
      "\tAverage F1 Score: 0.5899696864833069,\n",
      "\tAverage Precision: 0.5891307996240707,\n",
      "\tAverage Recall: 0.59304\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.4627600000000001,\n",
      "\tAverage F1 Score: 0.4449272902245161,\n",
      "\tAverage Precision: 0.4387979323259101,\n",
      "\tAverage Recall: 0.46275999999999995\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.60112,\n",
      "\tAverage F1 Score: 0.6004264374151413,\n",
      "\tAverage Precision: 0.6054060739727211,\n",
      "\tAverage Recall: 0.60112\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:09:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:09:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:10:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:11:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:12:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:13:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:13:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:14:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:15:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:16:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.59884,\n",
      "\tAverage F1 Score: 0.5974356401890379,\n",
      "\tAverage Precision: 0.5969990362636171,\n",
      "\tAverage Recall: 0.59884\n",
      "\n",
      "roberta-large report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264205\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264198\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264197\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264202\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264197\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264199\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264202\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264205\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264196\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264199\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6422800000000001,\n",
      "\tAverage F1 Score: 0.6297434668554287,\n",
      "\tAverage Precision: 0.6362089618598464,\n",
      "\tAverage Recall: 0.6422800000000001\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46580000000000005,\n",
      "\tAverage F1 Score: 0.44809567734579014,\n",
      "\tAverage Precision: 0.4416900856028672,\n",
      "\tAverage Recall: 0.46580000000000005\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.6509199999999999,\n",
      "\tAverage F1 Score: 0.6451207917976477,\n",
      "\tAverage Precision: 0.6438885056537037,\n",
      "\tAverage Recall: 0.6509199999999999\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:18:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:19:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:20:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:21:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:21:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:22:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:23:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:24:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:25:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:25:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6585999999999999,\n",
      "\tAverage F1 Score: 0.6500238670863145,\n",
      "\tAverage Precision: 0.6546409766468402,\n",
      "\tAverage Recall: 0.6585999999999999\n",
      "\n",
      "Loading features for language model: xlnet...\n",
      "(25000, 768)\n",
      "xlnet NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6100800000000001,\n",
      "\tAverage F1 Score: 0.6056336238523017,\n",
      "\tAverage Precision: 0.6055040786757655,\n",
      "\tAverage Recall: 0.6100800000000001\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46040000000000003,\n",
      "\tAverage F1 Score: 0.4425772899568713,\n",
      "\tAverage Precision: 0.43567987177960427,\n",
      "\tAverage Recall: 0.46040000000000003\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.61276,\n",
      "\tAverage F1 Score: 0.6120141949883874,\n",
      "\tAverage Precision: 0.6138639753129974,\n",
      "\tAverage Recall: 0.61276\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:28:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:28:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:29:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:29:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:32:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:32:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:33:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6189199999999999,\n",
      "\tAverage F1 Score: 0.6172020889915908,\n",
      "\tAverage Precision: 0.6163647722812826,\n",
      "\tAverage Recall: 0.6189199999999999\n",
      "\n",
      "xlnet report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198918\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6526799999999999,\n",
      "\tAverage F1 Score: 0.6392844007663804,\n",
      "\tAverage Precision: 0.64205186010102,\n",
      "\tAverage Recall: 0.6526799999999999\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46580000000000005,\n",
      "\tAverage F1 Score: 0.44804067179984175,\n",
      "\tAverage Precision: 0.4416455420724352,\n",
      "\tAverage Recall: 0.46580000000000005\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.6510800000000001,\n",
      "\tAverage F1 Score: 0.643298287659543,\n",
      "\tAverage Precision: 0.6420031600766267,\n",
      "\tAverage Recall: 0.6510800000000001\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:35:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:36:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:36:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:37:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:38:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:38:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:39:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:39:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:40:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:41:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6711999999999999,\n",
      "\tAverage F1 Score: 0.6625196112557632,\n",
      "\tAverage Precision: 0.6635594798662775,\n",
      "\tAverage Recall: 0.6712\n",
      "\n",
      "Loading features for language model: xlnet-large...\n",
      "(25000, 1024)\n",
      "xlnet-large NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6026800000000001,\n",
      "\tAverage F1 Score: 0.597744886000564,\n",
      "\tAverage Precision: 0.5968867786328981,\n",
      "\tAverage Recall: 0.6026800000000001\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46595999999999993,\n",
      "\tAverage F1 Score: 0.4482145514326807,\n",
      "\tAverage Precision: 0.4425240925105715,\n",
      "\tAverage Recall: 0.46596000000000004\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.6149199999999999,\n",
      "\tAverage F1 Score: 0.6136921029831488,\n",
      "\tAverage Precision: 0.614827313249915,\n",
      "\tAverage Recall: 0.6149199999999999\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:44:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:44:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:45:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:46:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:47:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:48:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:48:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:49:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:50:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6100000000000001,\n",
      "\tAverage F1 Score: 0.6077342791856147,\n",
      "\tAverage Precision: 0.6066999422720247,\n",
      "\tAverage Recall: 0.6100000000000001\n",
      "\n",
      "xlnet-large report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264205\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264198\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264197\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264202\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264197\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264199\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264202\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264205\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264196\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 264199\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 1070\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.64464,\n",
      "\tAverage F1 Score: 0.6310054395706471,\n",
      "\tAverage Precision: 0.6337926608571418,\n",
      "\tAverage Recall: 0.64464\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46568000000000004,\n",
      "\tAverage F1 Score: 0.4479219668526385,\n",
      "\tAverage Precision: 0.4415254894731503,\n",
      "\tAverage Recall: 0.46568000000000004\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.65532,\n",
      "\tAverage F1 Score: 0.6487533935153509,\n",
      "\tAverage Precision: 0.6469594365970337,\n",
      "\tAverage Recall: 0.65532\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:52:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:53:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:54:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:55:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:55:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:56:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:57:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:58:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:59:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:59:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6648000000000002,\n",
      "\tAverage F1 Score: 0.6553025977164795,\n",
      "\tAverage Precision: 0.6571634929370913,\n",
      "\tAverage Recall: 0.6648000000000001\n",
      "\n",
      "Loading features for language model: albert...\n",
      "(25000, 768)\n",
      "albert NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.5906800000000001,\n",
      "\tAverage F1 Score: 0.5857896995349832,\n",
      "\tAverage Precision: 0.5843737619403762,\n",
      "\tAverage Recall: 0.5906800000000001\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46055999999999997,\n",
      "\tAverage F1 Score: 0.44325142519065547,\n",
      "\tAverage Precision: 0.43683416323994145,\n",
      "\tAverage Recall: 0.4605600000000001\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.60416,\n",
      "\tAverage F1 Score: 0.603338182522226,\n",
      "\tAverage Precision: 0.6031783031776241,\n",
      "\tAverage Recall: 0.60416\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:02:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:02:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:03:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:03:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:04:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:05:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:05:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:06:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:06:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:07:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.60364,\n",
      "\tAverage F1 Score: 0.6010189376480792,\n",
      "\tAverage Precision: 0.5996188417955122,\n",
      "\tAverage Recall: 0.60364\n",
      "\n",
      "albert report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198918\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198925\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198919\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.6304000000000001,\n",
      "\tAverage F1 Score: 0.6153781177460307,\n",
      "\tAverage Precision: 0.618660583706401,\n",
      "\tAverage Recall: 0.6304000000000001\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46584000000000003,\n",
      "\tAverage F1 Score: 0.44812441496669636,\n",
      "\tAverage Precision: 0.4417289808449727,\n",
      "\tAverage Recall: 0.46584000000000003\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.61392,\n",
      "\tAverage F1 Score: 0.6012100109110403,\n",
      "\tAverage Precision: 0.5977703457712435,\n",
      "\tAverage Recall: 0.61392\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:09:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:10:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:10:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:11:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:11:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:12:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:13:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:13:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:14:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:14:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6517200000000001,\n",
      "\tAverage F1 Score: 0.6413552144856574,\n",
      "\tAverage Precision: 0.643459110267881,\n",
      "\tAverage Recall: 0.65172\n",
      "\n",
      "Loading features for language model: albert-large...\n",
      "(25000, 4096)\n",
      "albert-large NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.419527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.398716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.459850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.429831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.404024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.403991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.409958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.399744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.396687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.400054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1044480\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4096\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.58724,\n",
      "\tAverage F1 Score: 0.5852691659711133,\n",
      "\tAverage Precision: 0.5841572900746,\n",
      "\tAverage Recall: 0.5872399999999999\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.4404,\n",
      "\tAverage F1 Score: 0.41960186686544354,\n",
      "\tAverage Precision: 0.4140290932459532,\n",
      "\tAverage Recall: 0.44040000000000007\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.57908,\n",
      "\tAverage F1 Score: 0.5796699996763792,\n",
      "\tAverage Precision: 0.5811776588966402,\n",
      "\tAverage Recall: 0.57908\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:22:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:25:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:28:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:34:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:37:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:42:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:45:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:48:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.5904400000000001,\n",
      "\tAverage F1 Score: 0.5890221014087631,\n",
      "\tAverage Precision: 0.5879751135317729,\n",
      "\tAverage Recall: 0.5904400000000001\n",
      "\n",
      "albert-large report+NLP\n",
      "Cross-validating Light GBM...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.428495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047565\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.375626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047558\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.473548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047557\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.376436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047562\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.362709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047557\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.444428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047559\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.346345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047562\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.416307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047565\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.342307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047556\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.474468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1047559\n",
      "[LightGBM] [Info] Number of data points in the train set: 22500, number of used features: 4142\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Light GBM cross-validation complete. \n",
      "\tAverage Accuracy: 0.62896,\n",
      "\tAverage F1 Score: 0.6147588930988345,\n",
      "\tAverage Precision: 0.6242534721188224,\n",
      "\tAverage Recall: 0.62896\n",
      "\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.46564000000000005,\n",
      "\tAverage F1 Score: 0.44790923270580923,\n",
      "\tAverage Precision: 0.44150092779699135,\n",
      "\tAverage Recall: 0.46564000000000005\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.6248400000000001,\n",
      "\tAverage F1 Score: 0.6212383978095845,\n",
      "\tAverage Precision: 0.619162212041738,\n",
      "\tAverage Recall: 0.6248400000000001\n",
      "\n",
      "Cross-validating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:57:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:00:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:03:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:06:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:09:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:12:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:15:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:18:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:21:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:24:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.6397200000000001,\n",
      "\tAverage F1 Score: 0.6296390800422309,\n",
      "\tAverage Precision: 0.6349870407235205,\n",
      "\tAverage Recall: 0.6397200000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def cross_validate_classification(model, X, y, kf=None):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=123) if kf is None else kf\n",
    "    \n",
    "    kf.get_n_splits(X, y)\n",
    "    \n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        # Create a clone of the model to ensure that the model's initial state is preserved\n",
    "        model_clone = clone(model)\n",
    "        # Split the data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Fit the model\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        # Make predictions\n",
    "        y_pred = model_clone.predict(X_test)\n",
    "        # Calculate accuracy\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        # Calculate F1 score\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "        # Calculate precision\n",
    "        precision_scores.append(precision_score(y_test, y_pred, average='macro'))\n",
    "        # Calculate recall\n",
    "        recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    # Calculate average scores\n",
    "    average_accuracy = np.mean(accuracy_scores)\n",
    "    average_f1 = np.mean(f1_scores)\n",
    "    average_precision = np.mean(precision_scores)\n",
    "    average_recall = np.mean(recall_scores)\n",
    "    return average_accuracy, average_f1, average_precision, average_recall\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "models = {\n",
    "    \"Light GBM\": lgb.LGBMClassifier(n_estimators=100, n_jobs=40),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_jobs=45),\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=2000, solver='lbfgs', multi_class='auto'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, n_jobs=45),\n",
    "#     \"Support Vector Classifier\": SVC(),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \n",
    "#     \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, n_jobs=45)\n",
    "}\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def perform_pca(features, n_components):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis (PCA) on the provided features.\n",
    "\n",
    "    Args:\n",
    "    features: numpy array of features\n",
    "    n_components: number of principal components to retain\n",
    "\n",
    "    Returns:\n",
    "    A numpy array where each row is the original feature vector projected onto the top `n_components` principal components.\n",
    "    \"\"\"\n",
    "    print(\"Starting PCA...\")\n",
    "    print(features.shape)\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "\n",
    "    print(\"PCA complete.\")\n",
    "    print(f\"Reduced data shape: {reduced_features.shape}\")\n",
    "\n",
    "    return reduced_features\n",
    "\n",
    "#'mt5', 'mt5-large'\n",
    "\n",
    "#\n",
    "model_names = ['bert', 'bert-large', 'gpt2', 'roberta', 'roberta-large', 'xlnet', 'xlnet-large','albert','albert-large']#[]\n",
    "\n",
    "# model_names = ['TART']\n",
    "\n",
    "feature_sets = ['report','NLP','report+NLP']\n",
    "\n",
    "# feature_sets = ['NLP','report+NLP']\n",
    "\n",
    "BASE = dt.drop(['Crash_Severity'],axis=1).values\n",
    "\n",
    "SX = dt.Crash_Severity.values\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "reportonce=False\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "import numpy as np\n",
    "\n",
    "def perform_pca_nmf_with_scaling(features, n_components_pca=16, n_components_nmf=16):\n",
    "    \"\"\"\n",
    "    Perform both PCA and NMF on the provided features, apply necessary scaling, \n",
    "    and concatenate the results.\n",
    "\n",
    "    Args:\n",
    "    features: numpy array of features\n",
    "    n_components_pca: number of principal components to retain for PCA (default is 16)\n",
    "    n_components_nmf: number of components to retain for NMF (default is 16)\n",
    "\n",
    "    Returns:\n",
    "    A numpy array where each row is the original feature vector projected onto the top \n",
    "    `n_components_pca` principal components from PCA and `n_components_nmf` components from NMF.\n",
    "    \"\"\"\n",
    "    # Scale features using Standard Scaler for PCA\n",
    "    print(\"Applying Standard Scaler for PCA...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features_pca = scaler.fit_transform(features)\n",
    "\n",
    "    # Perform PCA\n",
    "    print(\"Starting PCA...\")\n",
    "    pca = PCA(n_components=n_components_pca)\n",
    "    reduced_features_pca = pca.fit_transform(scaled_features_pca)\n",
    "    print(\"PCA complete.\")\n",
    "    \n",
    "    # Scale features using Quantile Transformer for NMF\n",
    "    print(\"Applying Quantile Transformer for NMF...\")\n",
    "    qt = QuantileTransformer(output_distribution='uniform')\n",
    "    scaled_features_nmf = qt.fit_transform(features)\n",
    "\n",
    "    # Perform NMF\n",
    "    print(\"Starting NMF...\")\n",
    "    nmf = NMF(n_components=n_components_nmf)\n",
    "    reduced_features_nmf = nmf.fit_transform(scaled_features_nmf)\n",
    "    print(\"NMF complete.\")\n",
    "    \n",
    "    # Concatenate PCA and NMF results\n",
    "    combined_features = np.concatenate([reduced_features_pca, reduced_features_nmf], axis=1)\n",
    "    \n",
    "    print(f\"Combined feature shape: {combined_features.shape}\")\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "\n",
    "for language_model in model_names:\n",
    "    print(f\"Loading features for language model: {language_model}...\")\n",
    "    NLP = load_features(language_model)\n",
    "    print(NLP.shape)\n",
    "#     from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "#     scaler = StandardScaler()\n",
    "#     NLP = scaler.fit_transform(NLP)\n",
    "# #     quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
    "#     from sklearn.decomposition import NMF\n",
    "#     NLP = quantile_transformer.fit_transform(NLP)\n",
    "    \n",
    "#     nmf = NMF(n_components=128)\n",
    "#     NLP = nmf.fit_transform(NLP)\n",
    "\n",
    "#     NLP = perform_pca(NLP,128)\n",
    "#     \n",
    "#     NLP=perform_pca_nmf_with_scaling(NLP,48,48)\n",
    "    \n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "    \n",
    "    XES=None\n",
    "    \n",
    "    \n",
    "    for feature_set in feature_sets:\n",
    "        if feature_set == 'NLP':\n",
    "            XES = NLP\n",
    "            \n",
    "        elif feature_set == 'report':\n",
    "            \n",
    "            if reportonce:\n",
    "                continue\n",
    "                \n",
    "            XES = BASE\n",
    "            reportonce=True\n",
    "        elif feature_set == 'report+NLP':\n",
    "            XES = np.concatenate([BASE,NLP],axis=1)\n",
    "            \n",
    "        print(language_model, feature_set)\n",
    "        \n",
    "#         quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
    "#         XES = quantile_transformer.fit_transform(XES)\n",
    "        \n",
    "        \n",
    "# #         scaler = StandardScaler()\n",
    "#         XES = scaler.fit_transform(XES)\n",
    "\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Cross-validating {model_name}...\")\n",
    "            average_accuracy, average_f1, average_precision, average_recall = cross_validate_classification(model, XES, SX-1)\n",
    "            results.append([model_name, language_model, feature_set, average_accuracy, average_f1, average_precision, average_recall])\n",
    "            print(f\"{model_name} cross-validation complete. \\n\\tAverage Accuracy: {average_accuracy},\\n\\tAverage F1 Score: {average_f1},\\n\\tAverage Precision: {average_precision},\\n\\tAverage Recall: {average_recall}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41cabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Model', 'Language Model','Features','Average Accuracy', 'Average F1 Score', 'Average Precision', 'Average Recall']).round(3)\n",
    "results_df\n",
    "results_df.loc[results_df['Features'] == 'report', 'Language Model'] = '-'\n",
    "results_df.to_csv('results-LLM-32-even-severity-flat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88516e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for language model: bert...\n",
      "report\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: bert-large...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: gpt2...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: roberta...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: roberta-large...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: mt5...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: mt5-large...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: xlnet...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: xlnet-large...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: albert...\n",
      "NLP\n",
      "report+NLP\n",
      "\n",
      "Loading features for language model: albert-large...\n",
      "NLP\n",
      "report+NLP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_names = ['bert', 'bert-large', 'gpt2', 'roberta', 'roberta-large', 'mt5', 'mt5-large', 'xlnet', 'xlnet-large','albert','albert-large']#[]\n",
    "\n",
    "# feature_sets = ['report','NLP','report+NLP']\n",
    "\n",
    "# reportonce=False\n",
    "\n",
    "# for language_model in model_names:\n",
    "#     print(f\"Loading features for language model: {language_model}...\")\n",
    "    \n",
    "    \n",
    "#     for feature_set in feature_sets:\n",
    "#         if feature_set == 'NLP':\n",
    "#             print('NLP')\n",
    "            \n",
    "#         elif feature_set == 'report':\n",
    "            \n",
    "#             if reportonce:\n",
    "#                 continue\n",
    "                \n",
    "#             print('report')\n",
    "#             reportonce=True\n",
    "#         elif feature_set == 'report+NLP':\n",
    "#             print('report+NLP')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c848be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c0106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d690678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde9cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 80\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.2.tar.gz (60.1 MB)\n",
      "     || 60.1 MB 1.6 kB/s             \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/agrigore/ENV/bin/python3 /home/agrigore/ENV/lib64/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-3931lc6z/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools >= 64.0' wheel jupyterlab 'conan (>=1.57, <=1.59)'\n",
      "       cwd: None\n",
      "  Complete output (2 lines):\n",
      "  ERROR: Could not find a version that satisfies the requirement setuptools>=64.0 (from versions: 0.6b1, 0.6b2, 0.6b3, 0.6b4, 0.6rc1, 0.6rc2, 0.6rc3, 0.6rc4, 0.6rc5, 0.6rc6, 0.6rc7, 0.6rc8, 0.6rc9, 0.6rc10, 0.6rc11, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.8, 0.9, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7, 0.9.8, 1.0, 1.1, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.2, 1.3, 1.3.1, 1.3.2, 1.4, 1.4.1, 1.4.2, 2.0, 2.0.1, 2.0.2, 2.1, 2.1.1, 2.1.2, 2.2, 3.0, 3.0.1, 3.0.2, 3.1, 3.2, 3.3, 3.4, 3.4.1, 3.4.2, 3.4.3, 3.4.4, 3.5, 3.5.1, 3.5.2, 3.6, 3.7, 3.7.1, 3.8, 3.8.1, 4.0, 4.0.1, 5.0, 5.0.1, 5.0.2, 5.1, 5.2, 5.3, 5.4, 5.4.1, 5.4.2, 5.5, 5.5.1, 5.6, 5.7, 5.8, 6.0.1, 6.0.2, 6.1, 7.0, 8.0, 8.0.1, 8.0.2, 8.0.3, 8.0.4, 8.1, 8.2, 8.2.1, 8.3, 9.0, 9.0.1, 9.1, 10.0, 10.0.1, 10.1, 10.2, 10.2.1, 11.0, 11.1, 11.2, 11.3, 11.3.1, 12.0, 12.0.1, 12.0.2, 12.0.3, 12.0.4, 12.0.5, 12.1, 12.2, 12.3, 12.4, 13.0.1, 13.0.2, 14.0, 14.1, 14.1.1, 14.2, 14.3, 14.3.1, 15.0, 15.1, 15.2, 16.0, 17.0, 17.1, 17.1.1, 18.0, 18.0.1, 18.1, 18.2, 18.3, 18.3.1, 18.3.2, 18.4, 18.5, 18.6, 18.6.1, 18.7, 18.7.1, 18.8, 18.8.1, 19.0, 19.1, 19.1.1, 19.2, 19.3, 19.4, 19.4.1, 19.5, 19.6, 19.6.1, 19.6.2, 19.7, 20.0, 20.1, 20.1.1, 20.2.2, 20.3, 20.3.1, 20.4, 20.6.6, 20.6.7, 20.6.8, 20.7.0, 20.8.0, 20.8.1, 20.9.0, 20.10.1, 21.0.0, 21.1.0, 21.2.0, 21.2.1, 21.2.2, 22.0.0, 22.0.1, 22.0.2, 22.0.4, 22.0.5, 23.0.0, 23.1.0, 23.2.0, 23.2.1, 24.0.0, 24.0.1, 24.0.2, 24.0.3, 24.1.0, 24.1.1, 24.2.0, 24.2.1, 24.3.0, 24.3.1, 25.0.0, 25.0.1, 25.0.2, 25.1.0, 25.1.1, 25.1.2, 25.1.3, 25.1.4, 25.1.5, 25.1.6, 25.2.0, 25.3.0, 25.4.0, 26.0.0, 26.1.0, 26.1.1, 27.0.0, 27.1.0, 27.1.2, 27.2.0, 27.3.0, 27.3.1, 28.0.0, 28.1.0, 28.2.0, 28.3.0, 28.4.0, 28.5.0, 28.6.0, 28.6.1, 28.7.0, 28.7.1, 28.8.0, 28.8.1, 29.0.0, 29.0.1, 30.0.0, 30.1.0, 30.2.0, 30.2.1, 30.3.0, 30.4.0, 31.0.0, 31.0.1, 32.0.0, 32.1.0, 32.1.1, 32.1.2, 32.1.3, 32.2.0, 32.3.0, 32.3.1, 33.1.0, 33.1.1, 34.0.0, 34.0.1, 34.0.2, 34.0.3, 34.1.0, 34.1.1, 34.2.0, 34.3.0, 34.3.1, 34.3.2, 34.3.3, 34.4.0, 34.4.1, 35.0.0, 35.0.1, 35.0.2, 36.0.1, 36.1.0, 36.1.1, 36.2.0, 36.2.1, 36.2.2, 36.2.3, 36.2.4, 36.2.5, 36.2.6, 36.2.7, 36.3.0, 36.4.0, 36.5.0, 36.6.0, 36.6.1, 36.7.0, 36.7.1, 36.7.2, 36.8.0, 37.0.0, 38.0.0, 38.1.0, 38.2.0, 38.2.1, 38.2.3, 38.2.4, 38.2.5, 38.3.0, 38.4.0, 38.4.1, 38.5.0, 38.5.1, 38.5.2, 38.6.0, 38.6.1, 38.7.0, 39.0.0, 39.0.1, 39.1.0, 39.2.0, 40.0.0, 40.1.0, 40.1.1, 40.2.0, 40.3.0, 40.4.0, 40.4.1, 40.4.2, 40.4.3, 40.5.0, 40.6.0, 40.6.1, 40.6.2, 40.6.3, 40.7.0, 40.7.1, 40.7.2, 40.7.3, 40.8.0, 40.9.0, 41.0.0, 41.0.1, 41.1.0, 41.2.0, 41.3.0, 41.4.0, 41.5.0, 41.5.1, 41.6.0, 42.0.0, 42.0.1, 42.0.2, 43.0.0, 44.0.0, 44.1.0, 44.1.1, 45.0.0, 45.1.0, 45.2.0, 45.3.0, 46.0.0, 46.1.0, 46.1.1, 46.1.2, 46.1.3, 46.2.0, 46.3.0, 46.3.1, 46.4.0, 47.0.0, 47.1.0, 47.1.1, 47.2.0, 47.3.0, 47.3.1, 47.3.2, 48.0.0, 49.0.0, 49.0.1, 49.1.0, 49.1.1, 49.1.2, 49.1.3, 49.2.0, 49.2.1, 49.3.0, 49.3.1, 49.3.2, 49.4.0, 49.5.0, 49.6.0, 50.0.0, 50.0.1, 50.0.2, 50.0.3, 50.1.0, 50.2.0, 50.3.0, 50.3.1, 50.3.2, 51.0.0, 51.1.0, 51.1.0.post20201221, 51.1.1, 51.1.2, 51.2.0, 51.3.0, 51.3.1, 51.3.2, 51.3.3, 52.0.0, 53.0.0, 53.1.0, 54.0.0, 54.1.0, 54.1.1, 54.1.2, 54.1.3, 54.2.0, 56.0.0, 56.1.0, 56.2.0, 57.0.0, 57.1.0, 57.2.0, 57.3.0, 57.4.0, 57.5.0, 58.0.0, 58.0.1, 58.0.2, 58.0.3, 58.0.4, 58.1.0, 58.2.0, 58.3.0, 58.4.0, 58.5.0, 58.5.1, 58.5.2, 58.5.3, 59.0.1, 59.1.0, 59.1.1, 59.2.0, 59.3.0, 59.4.0, 59.5.0, 59.6.0)\n",
      "  ERROR: No matching distribution found for setuptools>=64.0\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/d3/65/d55e51dab8f29b66827b4304eaf19f583506aaaf66413a8505a370181314/catboost-1.2.2.tar.gz#sha256=c9381af807369fa6a24288a028eebbcfcec939c897ffd95729ca0f891d1d49c4 (from https://pypi.org/simple/catboost/). Command errored out with exit status 1: /home/agrigore/ENV/bin/python3 /home/agrigore/ENV/lib64/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-3931lc6z/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools >= 64.0' wheel jupyterlab 'conan (>=1.57, <=1.59)' Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Downloading catboost-1.2.1.tar.gz (95.3 MB)\n",
      "     || 95.3 MB 4.5 kB/s             \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/agrigore/ENV/bin/python3 /home/agrigore/ENV/lib64/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-24atljbv/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools >= 64.0' wheel jupyterlab 'conan (>=1.57, <=1.59)'\n",
      "       cwd: None\n",
      "  Complete output (2 lines):\n",
      "  ERROR: Could not find a version that satisfies the requirement setuptools>=64.0 (from versions: 0.6b1, 0.6b2, 0.6b3, 0.6b4, 0.6rc1, 0.6rc2, 0.6rc3, 0.6rc4, 0.6rc5, 0.6rc6, 0.6rc7, 0.6rc8, 0.6rc9, 0.6rc10, 0.6rc11, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.8, 0.9, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7, 0.9.8, 1.0, 1.1, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.2, 1.3, 1.3.1, 1.3.2, 1.4, 1.4.1, 1.4.2, 2.0, 2.0.1, 2.0.2, 2.1, 2.1.1, 2.1.2, 2.2, 3.0, 3.0.1, 3.0.2, 3.1, 3.2, 3.3, 3.4, 3.4.1, 3.4.2, 3.4.3, 3.4.4, 3.5, 3.5.1, 3.5.2, 3.6, 3.7, 3.7.1, 3.8, 3.8.1, 4.0, 4.0.1, 5.0, 5.0.1, 5.0.2, 5.1, 5.2, 5.3, 5.4, 5.4.1, 5.4.2, 5.5, 5.5.1, 5.6, 5.7, 5.8, 6.0.1, 6.0.2, 6.1, 7.0, 8.0, 8.0.1, 8.0.2, 8.0.3, 8.0.4, 8.1, 8.2, 8.2.1, 8.3, 9.0, 9.0.1, 9.1, 10.0, 10.0.1, 10.1, 10.2, 10.2.1, 11.0, 11.1, 11.2, 11.3, 11.3.1, 12.0, 12.0.1, 12.0.2, 12.0.3, 12.0.4, 12.0.5, 12.1, 12.2, 12.3, 12.4, 13.0.1, 13.0.2, 14.0, 14.1, 14.1.1, 14.2, 14.3, 14.3.1, 15.0, 15.1, 15.2, 16.0, 17.0, 17.1, 17.1.1, 18.0, 18.0.1, 18.1, 18.2, 18.3, 18.3.1, 18.3.2, 18.4, 18.5, 18.6, 18.6.1, 18.7, 18.7.1, 18.8, 18.8.1, 19.0, 19.1, 19.1.1, 19.2, 19.3, 19.4, 19.4.1, 19.5, 19.6, 19.6.1, 19.6.2, 19.7, 20.0, 20.1, 20.1.1, 20.2.2, 20.3, 20.3.1, 20.4, 20.6.6, 20.6.7, 20.6.8, 20.7.0, 20.8.0, 20.8.1, 20.9.0, 20.10.1, 21.0.0, 21.1.0, 21.2.0, 21.2.1, 21.2.2, 22.0.0, 22.0.1, 22.0.2, 22.0.4, 22.0.5, 23.0.0, 23.1.0, 23.2.0, 23.2.1, 24.0.0, 24.0.1, 24.0.2, 24.0.3, 24.1.0, 24.1.1, 24.2.0, 24.2.1, 24.3.0, 24.3.1, 25.0.0, 25.0.1, 25.0.2, 25.1.0, 25.1.1, 25.1.2, 25.1.3, 25.1.4, 25.1.5, 25.1.6, 25.2.0, 25.3.0, 25.4.0, 26.0.0, 26.1.0, 26.1.1, 27.0.0, 27.1.0, 27.1.2, 27.2.0, 27.3.0, 27.3.1, 28.0.0, 28.1.0, 28.2.0, 28.3.0, 28.4.0, 28.5.0, 28.6.0, 28.6.1, 28.7.0, 28.7.1, 28.8.0, 28.8.1, 29.0.0, 29.0.1, 30.0.0, 30.1.0, 30.2.0, 30.2.1, 30.3.0, 30.4.0, 31.0.0, 31.0.1, 32.0.0, 32.1.0, 32.1.1, 32.1.2, 32.1.3, 32.2.0, 32.3.0, 32.3.1, 33.1.0, 33.1.1, 34.0.0, 34.0.1, 34.0.2, 34.0.3, 34.1.0, 34.1.1, 34.2.0, 34.3.0, 34.3.1, 34.3.2, 34.3.3, 34.4.0, 34.4.1, 35.0.0, 35.0.1, 35.0.2, 36.0.1, 36.1.0, 36.1.1, 36.2.0, 36.2.1, 36.2.2, 36.2.3, 36.2.4, 36.2.5, 36.2.6, 36.2.7, 36.3.0, 36.4.0, 36.5.0, 36.6.0, 36.6.1, 36.7.0, 36.7.1, 36.7.2, 36.8.0, 37.0.0, 38.0.0, 38.1.0, 38.2.0, 38.2.1, 38.2.3, 38.2.4, 38.2.5, 38.3.0, 38.4.0, 38.4.1, 38.5.0, 38.5.1, 38.5.2, 38.6.0, 38.6.1, 38.7.0, 39.0.0, 39.0.1, 39.1.0, 39.2.0, 40.0.0, 40.1.0, 40.1.1, 40.2.0, 40.3.0, 40.4.0, 40.4.1, 40.4.2, 40.4.3, 40.5.0, 40.6.0, 40.6.1, 40.6.2, 40.6.3, 40.7.0, 40.7.1, 40.7.2, 40.7.3, 40.8.0, 40.9.0, 41.0.0, 41.0.1, 41.1.0, 41.2.0, 41.3.0, 41.4.0, 41.5.0, 41.5.1, 41.6.0, 42.0.0, 42.0.1, 42.0.2, 43.0.0, 44.0.0, 44.1.0, 44.1.1, 45.0.0, 45.1.0, 45.2.0, 45.3.0, 46.0.0, 46.1.0, 46.1.1, 46.1.2, 46.1.3, 46.2.0, 46.3.0, 46.3.1, 46.4.0, 47.0.0, 47.1.0, 47.1.1, 47.2.0, 47.3.0, 47.3.1, 47.3.2, 48.0.0, 49.0.0, 49.0.1, 49.1.0, 49.1.1, 49.1.2, 49.1.3, 49.2.0, 49.2.1, 49.3.0, 49.3.1, 49.3.2, 49.4.0, 49.5.0, 49.6.0, 50.0.0, 50.0.1, 50.0.2, 50.0.3, 50.1.0, 50.2.0, 50.3.0, 50.3.1, 50.3.2, 51.0.0, 51.1.0, 51.1.0.post20201221, 51.1.1, 51.1.2, 51.2.0, 51.3.0, 51.3.1, 51.3.2, 51.3.3, 52.0.0, 53.0.0, 53.1.0, 54.0.0, 54.1.0, 54.1.1, 54.1.2, 54.1.3, 54.2.0, 56.0.0, 56.1.0, 56.2.0, 57.0.0, 57.1.0, 57.2.0, 57.3.0, 57.4.0, 57.5.0, 58.0.0, 58.0.1, 58.0.2, 58.0.3, 58.0.4, 58.1.0, 58.2.0, 58.3.0, 58.4.0, 58.5.0, 58.5.1, 58.5.2, 58.5.3, 59.0.1, 59.1.0, 59.1.1, 59.2.0, 59.3.0, 59.4.0, 59.5.0, 59.6.0)\n",
      "  ERROR: No matching distribution found for setuptools>=64.0\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f0/9d/7cf091433d39e11d93e7b0bd33b4e6d0aed4b489ea843ae33df8373eee09/catboost-1.2.1.tar.gz#sha256=01e67ee02b8f958b2b74e98470a08350c08856a052c6e4474f7604ef7d4cf44b (from https://pypi.org/simple/catboost/). Command errored out with exit status 1: /home/agrigore/ENV/bin/python3 /home/agrigore/ENV/lib64/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-24atljbv/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools >= 64.0' wheel jupyterlab 'conan (>=1.57, <=1.59)' Check the logs for full command output.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Downloading catboost-1.2.tar.gz (95.9 MB)\n",
      "     || 95.9 MB 1.3 kB/s             \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/agrigore/ENV/bin/python3 /home/agrigore/ENV/lib64/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-dnmz6qid/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools >= 64.0' wheel jupyterlab 'conan (>=1.57, <=1.59)'\n",
      "       cwd: None\n",
      "  Complete output (2 lines):\n",
      "  ERROR: Could not find a version that satisfies the requirement setuptools>=64.0 (from versions: 0.6b1, 0.6b2, 0.6b3, 0.6b4, 0.6rc1, 0.6rc2, 0.6rc3, 0.6rc4, 0.6rc5, 0.6rc6, 0.6rc7, 0.6rc8, 0.6rc9, 0.6rc10, 0.6rc11, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.8, 0.9, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7, 0.9.8, 1.0, 1.1, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.2, 1.3, 1.3.1, 1.3.2, 1.4, 1.4.1, 1.4.2, 2.0, 2.0.1, 2.0.2, 2.1, 2.1.1, 2.1.2, 2.2, 3.0, 3.0.1, 3.0.2, 3.1, 3.2, 3.3, 3.4, 3.4.1, 3.4.2, 3.4.3, 3.4.4, 3.5, 3.5.1, 3.5.2, 3.6, 3.7, 3.7.1, 3.8, 3.8.1, 4.0, 4.0.1, 5.0, 5.0.1, 5.0.2, 5.1, 5.2, 5.3, 5.4, 5.4.1, 5.4.2, 5.5, 5.5.1, 5.6, 5.7, 5.8, 6.0.1, 6.0.2, 6.1, 7.0, 8.0, 8.0.1, 8.0.2, 8.0.3, 8.0.4, 8.1, 8.2, 8.2.1, 8.3, 9.0, 9.0.1, 9.1, 10.0, 10.0.1, 10.1, 10.2, 10.2.1, 11.0, 11.1, 11.2, 11.3, 11.3.1, 12.0, 12.0.1, 12.0.2, 12.0.3, 12.0.4, 12.0.5, 12.1, 12.2, 12.3, 12.4, 13.0.1, 13.0.2, 14.0, 14.1, 14.1.1, 14.2, 14.3, 14.3.1, 15.0, 15.1, 15.2, 16.0, 17.0, 17.1, 17.1.1, 18.0, 18.0.1, 18.1, 18.2, 18.3, 18.3.1, 18.3.2, 18.4, 18.5, 18.6, 18.6.1, 18.7, 18.7.1, 18.8, 18.8.1, 19.0, 19.1, 19.1.1, 19.2, 19.3, 19.4, 19.4.1, 19.5, 19.6, 19.6.1, 19.6.2, 19.7, 20.0, 20.1, 20.1.1, 20.2.2, 20.3, 20.3.1, 20.4, 20.6.6, 20.6.7, 20.6.8, 20.7.0, 20.8.0, 20.8.1, 20.9.0, 20.10.1, 21.0.0, 21.1.0, 21.2.0, 21.2.1, 21.2.2, 22.0.0, 22.0.1, 22.0.2, 22.0.4, 22.0.5, 23.0.0, 23.1.0, 23.2.0, 23.2.1, 24.0.0, 24.0.1, 24.0.2, 24.0.3, 24.1.0, 24.1.1, 24.2.0, 24.2.1, 24.3.0, 24.3.1, 25.0.0, 25.0.1, 25.0.2, 25.1.0, 25.1.1, 25.1.2, 25.1.3, 25.1.4, 25.1.5, 25.1.6, 25.2.0, 25.3.0, 25.4.0, 26.0.0, 26.1.0, 26.1.1, 27.0.0, 27.1.0, 27.1.2, 27.2.0, 27.3.0, 27.3.1, 28.0.0, 28.1.0, 28.2.0, 28.3.0, 28.4.0, 28.5.0, 28.6.0, 28.6.1, 28.7.0, 28.7.1, 28.8.0, 28.8.1, 29.0.0, 29.0.1, 30.0.0, 30.1.0, 30.2.0, 30.2.1, 30.3.0, 30.4.0, 31.0.0, 31.0.1, 32.0.0, 32.1.0, 32.1.1, 32.1.2, 32.1.3, 32.2.0, 32.3.0, 32.3.1, 33.1.0, 33.1.1, 34.0.0, 34.0.1, 34.0.2, 34.0.3, 34.1.0, 34.1.1, 34.2.0, 34.3.0, 34.3.1, 34.3.2, 34.3.3, 34.4.0, 34.4.1, 35.0.0, 35.0.1, 35.0.2, 36.0.1, 36.1.0, 36.1.1, 36.2.0, 36.2.1, 36.2.2, 36.2.3, 36.2.4, 36.2.5, 36.2.6, 36.2.7, 36.3.0, 36.4.0, 36.5.0, 36.6.0, 36.6.1, 36.7.0, 36.7.1, 36.7.2, 36.8.0, 37.0.0, 38.0.0, 38.1.0, 38.2.0, 38.2.1, 38.2.3, 38.2.4, 38.2.5, 38.3.0, 38.4.0, 38.4.1, 38.5.0, 38.5.1, 38.5.2, 38.6.0, 38.6.1, 38.7.0, 39.0.0, 39.0.1, 39.1.0, 39.2.0, 40.0.0, 40.1.0, 40.1.1, 40.2.0, 40.3.0, 40.4.0, 40.4.1, 40.4.2, 40.4.3, 40.5.0, 40.6.0, 40.6.1, 40.6.2, 40.6.3, 40.7.0, 40.7.1, 40.7.2, 40.7.3, 40.8.0, 40.9.0, 41.0.0, 41.0.1, 41.1.0, 41.2.0, 41.3.0, 41.4.0, 41.5.0, 41.5.1, 41.6.0, 42.0.0, 42.0.1, 42.0.2, 43.0.0, 44.0.0, 44.1.0, 44.1.1, 45.0.0, 45.1.0, 45.2.0, 45.3.0, 46.0.0, 46.1.0, 46.1.1, 46.1.2, 46.1.3, 46.2.0, 46.3.0, 46.3.1, 46.4.0, 47.0.0, 47.1.0, 47.1.1, 47.2.0, 47.3.0, 47.3.1, 47.3.2, 48.0.0, 49.0.0, 49.0.1, 49.1.0, 49.1.1, 49.1.2, 49.1.3, 49.2.0, 49.2.1, 49.3.0, 49.3.1, 49.3.2, 49.4.0, 49.5.0, 49.6.0, 50.0.0, 50.0.1, 50.0.2, 50.0.3, 50.1.0, 50.2.0, 50.3.0, 50.3.1, 50.3.2, 51.0.0, 51.1.0, 51.1.0.post20201221, 51.1.1, 51.1.2, 51.2.0, 51.3.0, 51.3.1, 51.3.2, 51.3.3, 52.0.0, 53.0.0, 53.1.0, 54.0.0, 54.1.0, 54.1.1, 54.1.2, 54.1.3, 54.2.0, 56.0.0, 56.1.0, 56.2.0, 57.0.0, 57.1.0, 57.2.0, 57.3.0, 57.4.0, 57.5.0, 58.0.0, 58.0.1, 58.0.2, 58.0.3, 58.0.4, 58.1.0, 58.2.0, 58.3.0, 58.4.0, 58.5.0, 58.5.1, 58.5.2, 58.5.3, 59.0.1, 59.1.0, 59.1.1, 59.2.0, 59.3.0, 59.4.0, 59.5.0, 59.6.0)\n",
      "  ERROR: No matching distribution found for setuptools>=64.0\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/8c/d5/d689925ae9d7bc86e12fd94e606445b8f802c25b9f8cdf7edda0c7641bf5/catboost-1.2.tar.gz#sha256=39e53403727ecfbb48156773ef3006b4e71cc35ab49cc9a0cf044b474c34be0c (from https://pypi.org/simple/catboost/). Command errored out with exit status 1: /home/agrigore/ENV/bin/python3 /home/agrigore/ENV/lib64/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-dnmz6qid/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools >= 64.0' wheel jupyterlab 'conan (>=1.57, <=1.59)' Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Downloading catboost-1.1.1-cp36-none-manylinux1_x86_64.whl (76.6 MB)\n",
      "     || 76.6 MB 5.8 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /home/agrigore/ENV/lib/python3.6/site-packages (from catboost) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/agrigore/ENV/lib/python3.6/site-packages (from catboost) (1.1.5)\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "     || 15.6 MB 133.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/agrigore/ENV/lib/python3.6/site-packages (from catboost) (3.3.4)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.19.1-py3-none-any.whl (46 kB)\n",
      "     || 46 kB 75 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: six in /home/agrigore/ENV/lib/python3.6/site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: scipy in /home/agrigore/ENV/lib/python3.6/site-packages (from catboost) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/agrigore/ENV/lib/python3.6/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/agrigore/ENV/lib/python3.6/site-packages (from pandas>=0.24.0->catboost) (2023.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/agrigore/ENV/lib/python3.6/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/agrigore/ENV/lib/python3.6/site-packages (from matplotlib->catboost) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/agrigore/ENV/lib/python3.6/site-packages (from matplotlib->catboost) (3.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/agrigore/ENV/lib/python3.6/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: packaging in /home/agrigore/ENV/lib/python3.6/site-packages (from plotly->catboost) (21.3)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly, graphviz, catboost\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc7853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2315183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef80521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Language Model</th>\n",
       "      <th>Features</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average F1 Score</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Light GBM</td>\n",
       "      <td>bert</td>\n",
       "      <td>report</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>bert</td>\n",
       "      <td>report</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>bert</td>\n",
       "      <td>report</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>bert</td>\n",
       "      <td>report</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Light GBM</td>\n",
       "      <td>bert</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>albert-large</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Light GBM</td>\n",
       "      <td>albert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>albert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>albert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>albert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Language Model    Features  Average Accuracy  \\\n",
       "0             Light GBM           bert      report             0.586   \n",
       "1   K-Nearest Neighbors           bert      report             0.494   \n",
       "2         Random Forest           bert      report             0.658   \n",
       "3               XGBoost           bert      report             0.606   \n",
       "4             Light GBM           bert         NLP             0.538   \n",
       "..                  ...            ...         ...               ...   \n",
       "71              XGBoost   albert-large         NLP             0.542   \n",
       "72            Light GBM   albert-large  report+NLP             0.577   \n",
       "73  K-Nearest Neighbors   albert-large  report+NLP             0.437   \n",
       "74        Random Forest   albert-large  report+NLP             0.582   \n",
       "75              XGBoost   albert-large  report+NLP             0.593   \n",
       "\n",
       "    Average F1 Score  Average Precision  Average Recall  \n",
       "0              0.563              0.578           0.586  \n",
       "1              0.481              0.474           0.494  \n",
       "2              0.650              0.655           0.658  \n",
       "3              0.589              0.597           0.606  \n",
       "4              0.528              0.523           0.538  \n",
       "..               ...                ...             ...  \n",
       "71             0.538              0.534           0.542  \n",
       "72             0.553              0.563           0.577  \n",
       "73             0.421              0.415           0.437  \n",
       "74             0.559              0.570           0.582  \n",
       "75             0.577              0.584           0.593  \n",
       "\n",
       "[76 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b318c8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dddc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_average(df):\n",
    "#     df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "#     # Group by 'Features' and 'Model', then calculate mean for 'Average RMSE' and 'Average MAPE'\n",
    "#     df_avg = df.groupby(['Language Model','Features', 'Model'], as_index=False).mean()\n",
    "    \n",
    "#     return df_avg\n",
    "# results_df = calculate_average(results_df)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Language Model','Features','Average Accuracy', 'Average F1 Score', 'Average Precision', 'Average Recall']).round(3)\n",
    "results_df\n",
    "results_df.loc[results_df['Features'] == 'report', 'Language Model'] = '-'\n",
    "results_df.to_csv('results-LLM-32-even-severity-final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68910f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceff811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c3296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af285165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ff40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Language Model', 'Features', 'Model', 'Average RMSE', 'Average MAPE']).round(2)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b1c289",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "5 columns passed, passed data had 7 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    688\u001b[0m             raise AssertionError(\n\u001b[0;32m--> 689\u001b[0;31m                 \u001b[0;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m                 \u001b[0;34mf\"{len(content)} columns\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 5 columns passed, passed data had 7 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2c98cab2de77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Language Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Average RMSE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Average MAPE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Language Model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# columns if columns is not None else []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         return _list_of_dict_to_arrays(\n",
      "\u001b[0;32m~/ENV/lib64/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 5 columns passed, passed data had 7 columns"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Language Model', 'Features', 'Model', 'Average RMSE', 'Average MAPE']).round(2)\n",
    "print(results_df)\n",
    "\n",
    "results_df.loc[results_df['Features'] == 'report', 'Language Model'] = '-'\n",
    "\n",
    "results_df.to_csv('results-LLM-32-even-duration-final-BL.csv')\n",
    "\n",
    "def calculate_average(df):\n",
    "    df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "    # Group by 'Features' and 'Model', then calculate mean for 'Average RMSE' and 'Average MAPE'\n",
    "    df_avg = df.groupby(['Language Model','Features', 'Model'], as_index=False).mean().round(2)\n",
    "    \n",
    "    return df_avg\n",
    "results_df = calculate_average(results_df)\n",
    "results_df.to_csv('results-LLM-32-even-duration-final-BL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f439f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480045a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2224ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cac30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcd149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54645adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3b0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be013d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545474b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f2d9d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I assume 'dt' is your DataFrame and 'BASE' and 'SX' are numpy arrays\n",
    "# BASE = dt.drop(['Crash_Severity'], axis=1).values\n",
    "# SX = dt.Crash_Severity.values\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(dt.drop(['Crash_Severity'],axis=1), SX)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = dt.drop(['Crash_Severity'], axis=1).columns\n",
    "\n",
    "# Sort feature importances in descending order and get the indices\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [feature_names[i] for i in indices]\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(BASE.shape[1]), feature_importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(BASE.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553d51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc337a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead42e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72574d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d8693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e403e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fe05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfe470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e51951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d674af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37121acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e30e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=50, n_jobs=1) \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.expm1(y_true), np.expm1(y_pred)  # Apply expm1 before calculating the metrics\n",
    "    return np.mean(np.where(y_true != 0, np.abs((y_true - y_pred) / y_true), 0)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0b3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def preprocess_and_encode(df):\n",
    "#     # Create a copy of the dataframe\n",
    "#     df_encoded = df.copy()\n",
    "\n",
    "#     # Fill NaN values with -1\n",
    "#     df_encoded.fillna(-1, inplace=True)\n",
    "\n",
    "#     # Label encoding for categorical columns and numerical columns with 30 or fewer unique values\n",
    "#     label_encoders = {}\n",
    "#     for col in df_encoded.select_dtypes(include=['object']).columns:\n",
    "#         le = LabelEncoder()\n",
    "#         df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "#         label_encoders[col] = le\n",
    "\n",
    "#     for col in df_encoded.select_dtypes(include=['int64']).columns:\n",
    "#         if df_encoded[col].nunique() <= 30:\n",
    "#             le = LabelEncoder()\n",
    "#             df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "#             label_encoders[col] = le\n",
    "\n",
    "#     # Convert boolean columns to int\n",
    "#     for col in df_encoded.select_dtypes(include=['bool']).columns:\n",
    "#         df_encoded[col] = df_encoded[col].astype(int)\n",
    "        \n",
    "#     return df_encoded #, label_encoders\n",
    "\n",
    "\n",
    "# BASE = preprocess_and_encode(dt.drop(['Duration','Severity','FullText','Description'],axis=1)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceebe95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c698876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBRegressor(n_estimators=50, n_jobs=1) \n",
    "# model.fit(BASE,DX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e96ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validate(model, X, y, kf):\n",
    "#     rmse_scores = []\n",
    "#     mape_scores = []\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         # Create a clone of the model to ensure that the model's initial state is preserved\n",
    "#         model_clone = clone(model)\n",
    "#         # Split the data\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         # Fit the model\n",
    "#         model_clone.fit(X_train, y_train)\n",
    "#         # Make predictions\n",
    "#         y_pred = model_clone.predict(X_test)\n",
    "#         # Calculate RMSE\n",
    "#         mse_score = np.mean((np.expm1(y_test) - np.expm1(y_pred)) ** 2)\n",
    "#         rmse_score = np.sqrt(mse_score)\n",
    "#         rmse_scores.append(rmse_score)\n",
    "#         # Calculate MAPE\n",
    "#         mape_score = mean_absolute_percentage_error(y_test, y_pred)\n",
    "#         mape_scores.append(mape_score)\n",
    "#     # Calculate average scores\n",
    "#     average_rmse = np.mean(rmse_scores)\n",
    "#     average_mape = np.mean(mape_scores)\n",
    "#     return average_rmse, average_mape\n",
    "\n",
    "\n",
    "# cross_validate(model, BASE, np.log1p(DX), kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebffb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import QuantileTransformer\n",
    "# quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
    "# BASE = quantile_transformer.fit_transform(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "934b5d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for language model: bert-large...\n",
      "Starting PCA...\n",
      "(48714, 1024)\n",
      "PCA complete.\n",
      "Reduced data shape: (48714, 32)\n",
      "bert-large report\n",
      "Cross-validating Linear Regression with report features from bert-large...\n",
      "Linear Regression cross-validation complete. \n",
      "\tAverage RMSE: 99.89293636694904,\n",
      "\tAverage MAPE: 72.2036299456897\n",
      "\n",
      "Cross-validating K-Nearest Neighbors with report features from bert-large...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage RMSE: 80.74113198432069,\n",
      "\tAverage MAPE: 55.55598514954463\n",
      "\n",
      "Cross-validating ElasticNet with report features from bert-large...\n",
      "ElasticNet cross-validation complete. \n",
      "\tAverage RMSE: 102.65388986812445,\n",
      "\tAverage MAPE: 73.61024500620962\n",
      "\n",
      "Cross-validating DecisionTree with report features from bert-large...\n",
      "DecisionTree cross-validation complete. \n",
      "\tAverage RMSE: 79.95855623011502,\n",
      "\tAverage MAPE: 55.90011444501965\n",
      "\n",
      "Cross-validating RandomForest with report features from bert-large...\n",
      "RandomForest cross-validation complete. \n",
      "\tAverage RMSE: 58.919060891833865,\n",
      "\tAverage MAPE: 36.43038587724071\n",
      "\n",
      "Cross-validating XGBoost with report features from bert-large...\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage RMSE: 60.94378780973936,\n",
      "\tAverage MAPE: 37.95047554393552\n",
      "\n",
      "bert-large NLP\n",
      "Cross-validating Linear Regression with NLP features from bert-large...\n",
      "Linear Regression cross-validation complete. \n",
      "\tAverage RMSE: 99.70580237360745,\n",
      "\tAverage MAPE: 71.70857364234269\n",
      "\n",
      "Cross-validating K-Nearest Neighbors with NLP features from bert-large...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage RMSE: 91.21978395440512,\n",
      "\tAverage MAPE: 62.14481491937003\n",
      "\n",
      "Cross-validating ElasticNet with NLP features from bert-large...\n",
      "ElasticNet cross-validation complete. \n",
      "\tAverage RMSE: 114.07505313665709,\n",
      "\tAverage MAPE: 78.35561332217992\n",
      "\n",
      "Cross-validating DecisionTree with NLP features from bert-large...\n",
      "DecisionTree cross-validation complete. \n",
      "\tAverage RMSE: 122.62141543889281,\n",
      "\tAverage MAPE: 108.91935775677989\n",
      "\n",
      "Cross-validating RandomForest with NLP features from bert-large...\n",
      "RandomForest cross-validation complete. \n",
      "\tAverage RMSE: 93.77247690333222,\n",
      "\tAverage MAPE: 63.34966407962985\n",
      "\n",
      "Cross-validating XGBoost with NLP features from bert-large...\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage RMSE: 93.57199877359469,\n",
      "\tAverage MAPE: 65.4208978149272\n",
      "\n",
      "bert-large report+NLP\n",
      "Cross-validating Linear Regression with report+NLP features from bert-large...\n",
      "Linear Regression cross-validation complete. \n",
      "\tAverage RMSE: 94.08319149565722,\n",
      "\tAverage MAPE: 66.51293706044805\n",
      "\n",
      "Cross-validating K-Nearest Neighbors with report+NLP features from bert-large...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage RMSE: 80.74113198432069,\n",
      "\tAverage MAPE: 55.55598514954463\n",
      "\n",
      "Cross-validating ElasticNet with report+NLP features from bert-large...\n",
      "ElasticNet cross-validation complete. \n",
      "\tAverage RMSE: 102.65388986812445,\n",
      "\tAverage MAPE: 73.61024500620962\n",
      "\n",
      "Cross-validating DecisionTree with report+NLP features from bert-large...\n",
      "DecisionTree cross-validation complete. \n",
      "\tAverage RMSE: 80.66780200430466,\n",
      "\tAverage MAPE: 56.03109638812614\n",
      "\n",
      "Cross-validating RandomForest with report+NLP features from bert-large...\n",
      "RandomForest cross-validation complete. \n",
      "\tAverage RMSE: 59.058971580712026,\n",
      "\tAverage MAPE: 35.46188418758045\n",
      "\n",
      "Cross-validating XGBoost with report+NLP features from bert-large...\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage RMSE: 61.68100274338151,\n",
      "\tAverage MAPE: 37.34189586340568\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfe5cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Language Model    Features                Model  Average RMSE  Average MAPE\n",
      "0      bert-large      report    Linear Regression         99.89         72.20\n",
      "1      bert-large      report  K-Nearest Neighbors         80.74         55.56\n",
      "2      bert-large      report           ElasticNet        102.65         73.61\n",
      "3      bert-large      report         DecisionTree         79.96         55.90\n",
      "4      bert-large      report         RandomForest         58.92         36.43\n",
      "5      bert-large      report              XGBoost         60.94         37.95\n",
      "6      bert-large         NLP    Linear Regression         99.71         71.71\n",
      "7      bert-large         NLP  K-Nearest Neighbors         91.22         62.14\n",
      "8      bert-large         NLP           ElasticNet        114.08         78.36\n",
      "9      bert-large         NLP         DecisionTree        122.62        108.92\n",
      "10     bert-large         NLP         RandomForest         93.77         63.35\n",
      "11     bert-large         NLP              XGBoost         93.57         65.42\n",
      "12     bert-large  report+NLP    Linear Regression         94.08         66.51\n",
      "13     bert-large  report+NLP  K-Nearest Neighbors         80.74         55.56\n",
      "14     bert-large  report+NLP           ElasticNet        102.65         73.61\n",
      "15     bert-large  report+NLP         DecisionTree         80.67         56.03\n",
      "16     bert-large  report+NLP         RandomForest         59.06         35.46\n",
      "17     bert-large  report+NLP              XGBoost         61.68         37.34\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0e6e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2474df88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad741b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2eee94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea8da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3f1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6e2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f125784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432c7179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910ebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5483a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# BASE = scaler.fit_transform(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96c35fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import QuantileTransformer\n",
    "# quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
    "    \n",
    "    \n",
    "# BASE = quantile_transformer.fit_transform(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c437db",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    NLP = scaler.fit_transform(NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4aea58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bc4380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for language model: bert-large...\n",
      "Starting PCA...\n",
      "(48714, 1024)\n",
      "PCA complete.\n",
      "Reduced data shape: (48714, 32)\n",
      "bert-large report\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.6696844305579855,\n",
      "\tAverage F1 Score: 0.663122557168384,\n",
      "\tAverage Precision: 0.6643540525477458,\n",
      "\tAverage Recall: 0.6677183168590339\n",
      "\n",
      "Cross-validating Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression cross-validation complete. \n",
      "\tAverage Accuracy: 0.6089828621117778,\n",
      "\tAverage F1 Score: 0.5935247725688361,\n",
      "\tAverage Precision: 0.5977611040151338,\n",
      "\tAverage Recall: 0.6080896154455994\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.8117379288770138,\n",
      "\tAverage F1 Score: 0.8082336769432048,\n",
      "\tAverage Precision: 0.8098286300820838,\n",
      "\tAverage Recall: 0.8108928901044129\n",
      "\n",
      "Cross-validating Decision Tree...\n",
      "Decision Tree cross-validation complete. \n",
      "\tAverage Accuracy: 0.7412447719302504,\n",
      "\tAverage F1 Score: 0.7401001413335596,\n",
      "\tAverage Precision: 0.7401227877014839,\n",
      "\tAverage Recall: 0.7401574762255465\n",
      "\n",
      "Cross-validating XGBoost...\n",
      "[13:23:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:23:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.8188407632855421,\n",
      "\tAverage F1 Score: 0.8163386482511636,\n",
      "\tAverage Precision: 0.8162986824139707,\n",
      "\tAverage Recall: 0.8180122561811991\n",
      "\n",
      "bert-large NLP\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.8509052057028648,\n",
      "\tAverage F1 Score: 0.8495456935044998,\n",
      "\tAverage Precision: 0.8517646021012872,\n",
      "\tAverage Recall: 0.850343527394881\n",
      "\n",
      "Cross-validating Logistic Regression...\n",
      "Logistic Regression cross-validation complete. \n",
      "\tAverage Accuracy: 0.8281810868182357,\n",
      "\tAverage F1 Score: 0.8263087262198656,\n",
      "\tAverage Precision: 0.8264441228972871,\n",
      "\tAverage Recall: 0.8277180310725207\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.837315898793132,\n",
      "\tAverage F1 Score: 0.8361124507286009,\n",
      "\tAverage Precision: 0.8359534529563808,\n",
      "\tAverage Recall: 0.8369847009039642\n",
      "\n",
      "Cross-validating Decision Tree...\n",
      "Decision Tree cross-validation complete. \n",
      "\tAverage Accuracy: 0.7192387910218279,\n",
      "\tAverage F1 Score: 0.7188892496217152,\n",
      "\tAverage Precision: 0.7190638386061304,\n",
      "\tAverage Recall: 0.7188162737607816\n",
      "\n",
      "Cross-validating XGBoost...\n",
      "[13:26:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.8519728873575355,\n",
      "\tAverage F1 Score: 0.851016610084328,\n",
      "\tAverage Precision: 0.8510596459690193,\n",
      "\tAverage Recall: 0.8516769988074339\n",
      "\n",
      "bert-large report+NLP\n",
      "Cross-validating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors cross-validation complete. \n",
      "\tAverage Accuracy: 0.6696844305579855,\n",
      "\tAverage F1 Score: 0.663122557168384,\n",
      "\tAverage Precision: 0.6643540525477458,\n",
      "\tAverage Recall: 0.6677183168590339\n",
      "\n",
      "Cross-validating Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression cross-validation complete. \n",
      "\tAverage Accuracy: 0.6093935144123981,\n",
      "\tAverage F1 Score: 0.5948169860386922,\n",
      "\tAverage Precision: 0.5984116354128882,\n",
      "\tAverage Recall: 0.6085679457508283\n",
      "\n",
      "Cross-validating Random Forest...\n",
      "Random Forest cross-validation complete. \n",
      "\tAverage Accuracy: 0.875169449801597,\n",
      "\tAverage F1 Score: 0.8738467855907736,\n",
      "\tAverage Precision: 0.8748978506578655,\n",
      "\tAverage Recall: 0.8746370391900868\n",
      "\n",
      "Cross-validating Decision Tree...\n",
      "Decision Tree cross-validation complete. \n",
      "\tAverage Accuracy: 0.7991748397657933,\n",
      "\tAverage F1 Score: 0.7985536221847223,\n",
      "\tAverage Precision: 0.798686420234453,\n",
      "\tAverage Recall: 0.7985762748021037\n",
      "\n",
      "Cross-validating XGBoost...\n",
      "[13:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost cross-validation complete. \n",
      "\tAverage Accuracy: 0.890134454981208,\n",
      "\tAverage F1 Score: 0.8893200796036116,\n",
      "\tAverage Precision: 0.8897662191948772,\n",
      "\tAverage Recall: 0.8897179489643507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def cross_validate_classification(model, X, y, kf=None):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=123) if kf is None else kf\n",
    "    \n",
    "    kf.get_n_splits(X, y)\n",
    "    \n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        # Create a clone of the model to ensure that the model's initial state is preserved\n",
    "        model_clone = clone(model)\n",
    "        # Split the data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Fit the model\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        # Make predictions\n",
    "        y_pred = model_clone.predict(X_test)\n",
    "        # Calculate accuracy\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        # Calculate F1 score\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "        # Calculate precision\n",
    "        precision_scores.append(precision_score(y_test, y_pred, average='macro'))\n",
    "        # Calculate recall\n",
    "        recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    # Calculate average scores\n",
    "    average_accuracy = np.mean(accuracy_scores)\n",
    "    average_f1 = np.mean(f1_scores)\n",
    "    average_precision = np.mean(precision_scores)\n",
    "    average_recall = np.mean(recall_scores)\n",
    "    return average_accuracy, average_f1, average_precision, average_recall\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=50, n_jobs=1),\n",
    "# #     \"Support Vector Classifier\": SVC(),\n",
    "#     \"XGBoost\": XGBClassifier(n_estimators=50, n_jobs=1)\n",
    "# }\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "models = {\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_jobs=40),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto'),\n",
    "    \n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "#     \"Support Vector Classifier\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \n",
    "#     \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=50, n_jobs=40)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model_names = ['bert-large']#['bert', 'gpt2', 'roberta', 'mt5', 'mt5-large', 'xlnet', 'xlnet-large']\n",
    "\n",
    "feature_sets = ['report','NLP','report+NLP']\n",
    "\n",
    "\n",
    "\n",
    "SX = dt.Severity.values\n",
    "DX = dt.Duration.values\n",
    "\n",
    "results = []\n",
    "\n",
    "for language_model in model_names:\n",
    "    print(f\"Loading features for language model: {language_model}...\")\n",
    "    FX = load_features(language_model)\n",
    "    NLP = perform_pca(FX, 32)\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    NLP = scaler.fit_transform(NLP)\n",
    "#     from sklearn.preprocessing import QuantileTransformer\n",
    "#     quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
    "#     NLP = quantile_transformer.fit_transform(NLP)\n",
    "\n",
    "    \n",
    "    XES=None\n",
    "    \n",
    "    for feature_set in feature_sets:\n",
    "        if feature_set == 'NLP':\n",
    "            XES = NLP\n",
    "        elif feature_set == 'report':\n",
    "            XES = BASE\n",
    "        elif feature_set == 'report+NLP':\n",
    "            XES = np.concatenate([BASE,NLP],axis=1)\n",
    "            \n",
    "        print(language_model, feature_set)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Cross-validating {model_name}...\")\n",
    "            average_accuracy, average_f1, average_precision, average_recall = cross_validate_classification(model, XES, SX-1)\n",
    "            results.append([model_name, language_model, feature_set, average_accuracy, average_f1, average_precision, average_recall])\n",
    "            print(f\"{model_name} cross-validation complete. \\n\\tAverage Accuracy: {average_accuracy},\\n\\tAverage F1 Score: {average_f1},\\n\\tAverage Precision: {average_precision},\\n\\tAverage Recall: {average_recall}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a37c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d074a977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Language Model</th>\n",
       "      <th>Features</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average F1 Score</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>bert-large</td>\n",
       "      <td>report+NLP</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Language Model    Features  Average Accuracy  \\\n",
       "0   K-Nearest Neighbors     bert-large      report             0.670   \n",
       "1   Logistic Regression     bert-large      report             0.609   \n",
       "2         Random Forest     bert-large      report             0.812   \n",
       "3         Decision Tree     bert-large      report             0.741   \n",
       "4               XGBoost     bert-large      report             0.819   \n",
       "5   K-Nearest Neighbors     bert-large         NLP             0.851   \n",
       "6   Logistic Regression     bert-large         NLP             0.828   \n",
       "7         Random Forest     bert-large         NLP             0.837   \n",
       "8         Decision Tree     bert-large         NLP             0.719   \n",
       "9               XGBoost     bert-large         NLP             0.852   \n",
       "10  K-Nearest Neighbors     bert-large  report+NLP             0.670   \n",
       "11  Logistic Regression     bert-large  report+NLP             0.609   \n",
       "12        Random Forest     bert-large  report+NLP             0.875   \n",
       "13        Decision Tree     bert-large  report+NLP             0.799   \n",
       "14              XGBoost     bert-large  report+NLP             0.890   \n",
       "\n",
       "    Average F1 Score  Average Precision  Average Recall  \n",
       "0              0.663              0.664           0.668  \n",
       "1              0.594              0.598           0.608  \n",
       "2              0.808              0.810           0.811  \n",
       "3              0.740              0.740           0.740  \n",
       "4              0.816              0.816           0.818  \n",
       "5              0.850              0.852           0.850  \n",
       "6              0.826              0.826           0.828  \n",
       "7              0.836              0.836           0.837  \n",
       "8              0.719              0.719           0.719  \n",
       "9              0.851              0.851           0.852  \n",
       "10             0.663              0.664           0.668  \n",
       "11             0.595              0.598           0.609  \n",
       "12             0.874              0.875           0.875  \n",
       "13             0.799              0.799           0.799  \n",
       "14             0.889              0.890           0.890  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e3b3964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96d681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Model', 'Language Model','Features','Average Accuracy', 'Average F1 Score', 'Average Precision', 'Average Recall']).round(3)\n",
    "results_df\n",
    "\n",
    "\n",
    "results_df.loc[results_df['Features'] == 'report', 'Language Model'] = '-'\n",
    "\n",
    "results_df.to_csv('results-LLM-32-even-severity-final-BL.csv')\n",
    "\n",
    "def calculate_average(df):\n",
    "    df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "    # Group by 'Features' and 'Model', then calculate mean for 'Average RMSE' and 'Average MAPE'\n",
    "    df_avg = df.groupby(['Language Model','Features', 'Model'], as_index=False).mean()\n",
    "    \n",
    "    return df_avg\n",
    "results_df = calculate_average(results_df)\n",
    "results_df.to_csv('results-LLM-32-even-severity-final-BL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07f720e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48714, 75)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa6e8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = pd.read_csv('results-LLM-32-even-severity-final-0.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb39f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt.loc[xt['Feature Set'] == 'report', 'Language Model'] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13b68697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(df):\n",
    "    df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "    # Group by 'Features' and 'Model', then calculate mean for 'Average RMSE' and 'Average MAPE'\n",
    "    df_avg = df.groupby(['Language Model','Feature Set', 'Model'], as_index=False).mean().round(3)\n",
    "    \n",
    "    return df_avg\n",
    "xt = calculate_average(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc1bd2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt.to_csv('results-LLM-32-even-severity-final-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bc163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c23a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1349da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29021b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34512046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b202d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=50, n_jobs=1) \n",
    "}\n",
    "\n",
    "# List of language models\n",
    "language_models = ['roberta', 'bert', 'gpt2', 'mt5','xlnet']\n",
    "\n",
    "# language_models = ['xlnet']\n",
    "\n",
    "# Feature sets\n",
    "feature_sets = ['VX', 'TX', 'VX+TX']\n",
    "\n",
    "feature_sets = ['VX', 'VX+TX']\n",
    "\n",
    "# Function to compute the Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Function to perform cross-validation\n",
    "def cross_validate(model, X, y, kf):\n",
    "    rmse_scores = []\n",
    "    mape_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Create a clone of the model to ensure that the model's initial state is preserved\n",
    "        model_clone = clone(model)\n",
    "        # Split the data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Fit the model\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        # Make predictions\n",
    "        y_pred = model_clone.predict(X_test)\n",
    "        # Calculate RMSE\n",
    "        mse_score = np.mean((np.expm1(y_test) - np.expm1(y_pred)) ** 2)\n",
    "        rmse_score = np.sqrt(mse_score)\n",
    "        rmse_scores.append(rmse_score)\n",
    "        # Calculate MAPE\n",
    "        mape_score = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        mape_scores.append(mape_score)\n",
    "    # Calculate average scores\n",
    "    average_rmse = np.mean(rmse_scores)\n",
    "    average_mape = np.mean(mape_scores)\n",
    "    return average_rmse, average_mape\n",
    "\n",
    "\n",
    "results = []\n",
    "y = np.log1p(DX)\n",
    "# Calculate cross-validation score for each model\n",
    "for language_model in language_models:\n",
    "    print(f\"Loading features for language model: {language_model}...\")\n",
    "    FX = load_features(language_model)[filtered]\n",
    "    VX = perform_pca(FX, 32)\n",
    "    for feature_set in feature_sets:\n",
    "        if feature_set == 'VX':\n",
    "            X = VX\n",
    "        elif feature_set == 'TX':\n",
    "            X = TX\n",
    "        elif feature_set == 'VX+TX':\n",
    "            X = np.concatenate([VX,TX],axis=1)\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Cross-validating {model_name} with {feature_set} features from {language_model}...\")\n",
    "            average_rmse, average_mape = cross_validate(model, XES, YES, kf)\n",
    "            results.append([language_model, feature_set, model_name, average_rmse, average_mape])\n",
    "            print(f\"{model_name} cross-validation complete. Average RMSE: {average_rmse}, Average MAPE: {average_mape}\\n\")\n",
    "\n",
    "# Convert results to DataFrame for a nice table display\n",
    "results_df = pd.DataFrame(results, columns=['Language Model', 'Features', 'Model', 'Average RMSE', 'Average MAPE']).round(2)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a50d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d057eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a548fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c586a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd67b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_and_encode(df):\n",
    "    # Create a copy of the dataframe\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    # Fill NaN values with -1\n",
    "    df_encoded.fillna(-1, inplace=True)\n",
    "\n",
    "    # Label encoding for categorical columns\n",
    "    label_encoders = {}\n",
    "    for col in df_encoded.select_dtypes(include=['object', 'int64']).columns:\n",
    "        le = LabelEncoder()\n",
    "        # Convert everything to string before encoding\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Convert boolean columns to int\n",
    "    for col in df_encoded.select_dtypes(include=['bool']).columns:\n",
    "        df_encoded[col] = df_encoded[col].astype(int)\n",
    "        \n",
    "    return df_encoded #, label_encoders\n",
    "\n",
    "TX = preprocess_and_encode(dt.drop(['Duration','Severity','FullText','Description'],axis=1)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f7d627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48714, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SX = dt.Severity.values\n",
    "DX = dt.Duration.values\n",
    "FX = load_features('bert')\n",
    "FX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9094359b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48714,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e4780aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered = DX<720\n",
    "# FX=FX[filtered]\n",
    "# SX=SX[filtered]\n",
    "# TX=TX[filtered]\n",
    "# DX=DX[filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a907fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEYCAYAAABiECzgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABEz0lEQVR4nO3deVxU9f748dcA4sYmKOBCmqZe0gTNHdREARURVGjP1Mq1yCxL8rqmZmXq1+yWZlne2697RQVSzCXMNZdu6jWLSisUTQYCBERgWM7vjyOjyDbCbOj7+XjMY5gzn3PO+xzgPZ/5nM/5fDSKoigIIYQwOxtLByCEEHcrScBCCGEhkoCFEMJCJAELIYSFSAIWQggLkQQshBAWIgnYAkJCQjh27Jilw7CoPXv2MGjQILp3785PP/1ksv38/vvvhIWF0b17dzZu3EhBQQFTpkzhwQcfJCoqii+//JKJEyeabP+3Y968ebz//vtVvt+5c2fOnz9vxogs747/X1GEUQ0ePFg5fPhwuWVbtmxRHn300dvaTkpKitKpUyelqKjImOFZjSFDhih79uypsPzSpUuKr6+v/tGpUyfFx8dH//q77767rf1ER0crS5Ys0b+OjY1Vxo4da7Tzunr1aqVTp07KqVOnjLK96nTq1ElJTk6u83Zee+01ZcWKFTXuq+y89+7dWxk3bpySkJBQ533XNa47jZ2lPwCEZRQXF2NnZ7lf/59//knHjh0rLG/VqhUnT57Uv+7cuTPx8fG0bdu2QllDjuHPP/8kJCSk3Ot27doZ5dgVRSEuLg4XFxfi4uLw8fGp8zatSdl5z8zM5MCBAyxatIjff/+d559//ra3Zem/N2slTRAWEBAQwLfffgvA6dOnGTNmDD169KB///68+eabADz55JMA9OrVi+7du3Py5ElKS0v5xz/+weDBg+nXrx+vvvoqubm5+u3GxcUxePBg+vTpw/vvv19uP++99x5RUVG88sor9OjRg9jYWE6fPs0jjzxCz5498ff3Z9GiReh0Ov32OnfuzOeff05QUBDdu3dn1apVXLhwgUcffZQePXrw4osvlit/s6pi1el0dO/enZKSEsLCwhg6dKjB523r1q08+uijLF26lD59+vDee+9x4cIFxo0bR58+fejTpw8vv/wyOTk5AIwbN45jx46xaNEiunfvzsyZM/nHP/7BV199Rffu3YmJiWHr1q089thj+n2cPXuWCRMm0Lt3b/r378+HH35YZTz//e9/SU9PZ86cOezYsaPcuSgoKGDZsmUMHjyYBx98kMcee4yCggL9eo8++ig9e/Zk0KBBbN26FYDZs2ezcuVK/TbWr1+Pv78//v7+bN68udy+dTodb731Fg899BD9+/dn3rx5+u0fO3aMgQMH8sknn9CvXz/8/f3ZsmULAP/5z3/Ytm0bH3/8Md27d2fKlCk1nndXV1fCw8NZsGABa9euJSsrCyj/dwzq39grr7wCwMWLF+ncuTMxMTE89NBDPP300wBERUXh5+fHgw8+yBNPPMHZs2erjevmfeh0OpYsWaI/J0uWLNGf8+qO2apZugp+pzGkCeLmMg8//LASGxurKIqiXL16VTl58qSiKJU3QcTExChDhw5VLly4oFy9elWZPn268sorryiKoihnz57Vf0UvLCxUli1bptx///36/axevVq5//77lT179iglJSVKfn6+8sMPPygnT55UioqKlJSUFGXYsGHKhg0b9Pvr1KmTMmXKFCU3N1f59ddflS5duijjxo1TLly4oOTk5CjDhw9Xtm7dWul5qC7Wsm0b8nX65nJbtmxRvL29lY0bNypFRUVKfn6+kpycrBw6dEgpLCxUMjIylMcff1xZvHixfv0nn3xS2bRpk/716tWrlZdffrnS301ubq7i5+enfPzxx0pBQYGSm5tbbdNCdHS0EhUVpeh0OqV3797Kzp079e8tWLBAefLJJ5XU1FSluLhY+f7775XCwkLl4sWLiq+vr7Jt2zZFp9MpmZmZyk8//aQoSvmv4Pv371f69eun/PLLL0peXp4yc+bMcudiyZIlyuTJk5WsrCwlNzdXmTx5srJ8+XJFURTl6NGjire3t7Jq1SpFp9Mp+/btU7p166ZcuXKlwn4MOe9ldDqd4u3trezbt09RlIp/6zef27K/31mzZil5eXlKfn6+oijq30Vubq5SWFioLF68WBk1apR+/criunkfq1atUiIjI5W//vpLycjIUB555BFl5cqVBh2ztZIasAlMnz6dnj176h8LFy6ssqydnR0XLlwgMzOTpk2b4uvrW2XZbdu2MX78eLy8vGjatCkzZ85kx44dFBcXs3PnTgYPHkzPnj2xt7cnKioKjUZTbn1fX1+GDh2KjY0NjRo1omvXrvj6+mJnZ0ebNm145JFH+O6778qt8+yzz+Lg4EDHjh3p1KkTfn5+eHl54ejoyMCBA6u8gFZdrHXh7u7OU089hZ2dHY0aNaJt27b4+flhb2+Pq6srEyZMqHAMhtq3bx/Nmzdn4sSJNGzYEAcHhyqbFfLz89m5cyehoaE0aNCA4OBg4uLiALX2v2XLFubMmYOHhwe2trb06NEDe3t7tm/fTv/+/Rk5ciQNGjSgWbNmeHt7V9j+V199xZgxY+jUqRNNmjQp97VfURQ2bdrE66+/jouLCw4ODkyePJmEhAR9GTs7O6ZPn06DBg0YNGgQTZo04Y8//qjVeSlTFm92drbB67zwwgs0adKERo0aARAREYGDgwP29va88MIL/Pzzz+W+xVVn27ZtTJ8+HTc3N1xdXZk+fTpffvml/n1THLOpSaOMCbz//vv0799f/3rr1q3ExMRUWnbJkiWsXr2a4cOH06ZNG55//nkGDx5cadm0tDRat26tf926dWuKi4vJyMggLS0NT09P/XuNGzfGxcWl3Po3vw/wxx9/sGzZMs6cOUN+fj4lJSV06dKlXJnmzZvrf27YsGGF13/99ddtx+rh4VHpOoa49Rj++usvlixZwn//+1/y8vJQFAUnJ6dabfvy5cvcc889BpXds2cPdnZ2DBw4EIDQ0FAmTJhAZmYmiqJQWFiIl5dXrfeRlpZG165d9a9vPpeZmZnk5+czZswY/TJFUSgtLdW/dnFxKdfm2rhxY65du2bQsVWlqKiIzMxMnJ2dDV7n5t9XSUkJK1euZOfOnWRmZmJjo9b/srKycHR0rHFbaWlptGrVSv+6VatWpKWl6V+b4phNTRKwhbVr144VK1ZQWlrK7t27iYqK4tixYxVqr6DW/i5duqR//eeff2JnZ4ebmxvu7u7lPu0LCgq4cuVKufVv3eaCBQu4//77effdd3FwcODTTz9l165dRjmu6mKti1uPYcWKFWg0GrZt24aLiwtff/01ixYtqtW2W7ZsyY4dOwwqGxcXx7Vr1/QfloqiUFRUxLZt23jqqado2LAhKSkp/O1vf6uwj9OnT9e4fXd3dy5fvqx//eeff+p/btasGY0aNSIhIaFWH2aV/W0ZIjExEVtbW7p16waoCS4/P1//fnp6erX72rZtG4mJiWzYsIE2bdqQm5tLr169UK4PyFhTXO7u7uUu3l6+fBl3d/daHYu1kCYIC4uPj9fXBspqbjY2Nri6umJjY0NKSoq+7MiRI/nss89ISUkhLy+PlStXMnz4cOzs7AgODmbv3r2cOHECnU7He++9p//DrkpeXh5NmzaladOm/Pbbb3zxxRdGO67qYjWmvLw8mjRpgqOjI1qtlvXr19d6Ww899BDp6el8+umn6HQ6rl69yv/+978K5bRaLUeOHOHDDz8kLi6OuLg44uPjee6554iPj8fGxoaxY8fy5ptvotVqKSkp4eTJk+h0OkJDQ/n222/1zTFZWVkkJSVV2MewYcOIjY3l3Llz5Ofns2bNGv17NjY2REZGsnTpUjIyMvQxHTx40KDjdHNz4+LFiwaflytXrvDll1+yaNEinnvuOZo1awbA3/72N3bs2EFRURE//PBDjR/eeXl52Nvb06xZM/Lz81mxYsVtxRUSEsIHH3xAZmYmmZmZvP/++4SGhhp8HNZIErCFHTx4kJCQELp3786SJUtYuXIljRo1onHjxkyZMoXHHnuMnj17curUKcaOHcuoUaN48sknGTJkCPb29sydOxeAjh07MnfuXGbOnMmAAQNo0qQJrq6u2NvbV7nv1157je3bt9OjRw/mzp3LiBEjjHZc1cVqTM8//zw//fQTPXv2ZNKkSQQFBdV6Ww4ODnzyySd88803+Pn5ERwcXOlNAPHx8Xh7e+Pv70+LFi30j6eeeopffvmFX3/9lddee41OnToRERFB7969Wb58OaWlpbRq1YqPPvqIDRs20Lt3b8LDw/n5558r7GPQoEE8/fTTPP300wQGBtK3b99y78+aNYu2bdvy8MMP06NHD8aPH29we2dERATnzp2jZ8+eTJs2rcpyZTewBAUFERMTQ3R0NC+++KL+/RkzZnDhwgV69+7Ne++9V2MyDA8Pp1WrVgwYMICQkJAK1ztqimvatGl07dqVUaNGMWrUKLp06VJt/PWBRqmpmiTqpby8PHr16sWuXbsqbYsUQlie1IDvIHv37iU/P59r167x1ltv0alTJ9q0aWPpsIQQVZAEfAdJTExkwIABDBgwgPPnz+svUAkhrJM0QQghhIVIDVgIISzkrusHfOLECRo3bmzpMCgsLKRhw4aWDgOwnlisJQ6QWKw5DrCeWAoLC6u9e7Umd10C1mg0ld76aW5JSUlWEQdYTyzWEgdILNYcB1hPLJX14b4d0gQhhBAWIglYCCEsRBKwEEJYiCRgIYSwEEnAQghhIZKAhRDCQiQBCyGEhUgCFkIIC5EELIQQFnLX3QlXF1lZYMh8hM7OcH3SACGEqJLJEnB0dDT79u3Dzc2N7du3l3vvk08+4a233uLIkSO4urqiKApLlixh//79NGrUiGXLluknh4yNjeWDDz4AYOrUqYwePRqAM2fOEB0dTUFBAYMGDWLOnDkmH3oxOxs+/bTmcuPHSwIWQtTMZE0QY8aMqXR+rsuXL3P48OFys5seOHCA5ORkdu/ezRtvvMGCBQsAdS6qNWvWsGnTJmJiYlizZo1+SuwFCxbwxhtvsHv3bpKTkzlw4ICpDkUIIUzCZAm4V69elU5f/eabbzJr1qxytdXExETCw8PRaDT4+vqSk5NDWloahw4dws/PDxcXF5ydnfHz8+PgwYOkpaVx9epVfH190Wg0hIeHk5iYaKpDEUIIkzDrRbivv/4ad3f3ClN1a7VaPD099a89PT3RarUVlnt4eFS6vKy8EELUJ2a7CJefn8/atWv55JNPzLXLSpWWltZ6CDmdriXp6UU1lsvObkBS0uVqyxQUFNR5KDtjsZZYrCUOkFisOQ6wrljqwmwJ+MKFC1y8eJGwsDAAUlNTGTNmDDExMXh4eJCamqovm5qaioeHBx4eHhw/fly/XKvV0rt37yrLG8LGxqbW44gmJ0OLFjWXc3aGdu1cqi1jLeOZgvXEYi1xgMRizXGA9cRSb8YD7ty5M0eOHGHv3r3s3bsXT09Ptm7dSosWLQgICCAuLg5FUTh16hSOjo64u7vj7+/PoUOHyM7OJjs7m0OHDuHv74+7uzsODg6cOnUKRVGIi4tjyJAh5joUIYQwCpPVgGfOnMnx48fJyspi4MCBvPDCC0RGRlZadtCgQezfv5/AwEAaN27M0qVLAXBxcWHatGlEREQAMH36dFxcXACYP3++vhvawIEDGThwoKkORQghTMJkCXjFihXVvr937179zxqNhvnz51daLiIiQp+Ab/bAAw9U6F8shBD1idyKLIQQFiIJWAghLEQSsBBCWIgkYCGEsBBJwEIIYSGSgIUQwkIkAQshhIVIAhZCCAuRBCyEEBYiCVgIISxEErAQQliIJGAhhLAQScBCCGEhkoCFEMJCJAELIYSFSAIWQggLkQQshBAWIglYCCEsRBKwEEJYiCRgIYSwEEnAQghhISZLwNHR0fTr14+RI0fql7311lsMGzaM0NBQpk+fTk5Ojv69tWvXEhgYSHBwMAcPHtQvP3DgAMHBwQQGBrJu3Tr98pSUFCIjIwkMDGTGjBnodDpTHYoQQpiEyRLwmDFjWL9+fbllfn5+bN++nW3bttGuXTvWrl0LwLlz50hISCAhIYH169ezcOFCSkpKKCkpYdGiRaxfv56EhAS2b9/OuXPnAFi+fDnjx49nz549ODk5sXnzZlMdihBCmITJEnCvXr1wdnYut8zf3x87OzsAfH19SU1NBSAxMZGQkBDs7e3x8vKibdu2nD59mtOnT9O2bVu8vLywt7cnJCSExMREFEXh6NGjBAcHAzB69GgSExNNdShCCGESdpba8ZYtWxg+fDgAWq0WHx8f/XseHh5otVoAPD09yy0/ffo0WVlZODk56ZO5p6envnxNSktLSUpKqlXMOl1L0tOLaiyXnd2ApKTL1ZYpKCiodRzGZi2xWEscILFYcxxgXbHUhUUS8AcffICtrS2jRo0y+75tbGzw9vau1brJydCiRc3lnJ2hXTuXasskJSXVOg5js5ZYrCUOkFisOQ6wnljq+iFg9gS8detW9u3bx6effopGowHUmm1ZcwSoNWIPDw+ASpc3a9aMnJwciouLsbOzIzU1VV9eCCHqC7N2Qztw4ADr16/ngw8+oHHjxvrlAQEBJCQkoNPpSElJITk5mW7duvHAAw+QnJxMSkoKOp2OhIQEAgIC0Gg09OnTh127dgEQGxtLQECAOQ9FCCHqzGQ14JkzZ3L8+HGysrIYOHAgL7zwAuvWrUOn0zFhwgQAfHx8WLRoER07dmT48OGMGDECW1tb5s2bh62tLQDz5s3j2WefpaSkhLFjx9KxY0cAZs2axUsvvcSqVavw9vYmMjLSVIcihBAmYbIEvGLFigrLqkuSU6dOZerUqRWWDxo0iEGDBlVY7uXlJV3PhBD1mtwJJ4QQFiIJWAghLEQSsBBCWIgkYCGEsBBJwEIIYSGSgIUQwkIkAQshhIVIAhZCCAuRBCyEEBYiCVgIISxEErAQQliIJGAhhLAQScBCCGEhkoCFEMJCJAELIYSFSAIWQggLkQQshBAWIglYCCEsRBKwEEJYiCRgIYSwEJMl4OjoaPr168fIkSP1y65cucKECRMICgpiwoQJZGdnA6AoCosXLyYwMJDQ0FB+/PFH/TqxsbEEBQURFBREbGysfvmZM2cIDQ0lMDCQxYsXoyiKqQ5FCCFMwmQJeMyYMaxfv77csnXr1tGvXz92795Nv379WLduHQAHDhwgOTmZ3bt388Ybb7BgwQJATdhr1qxh06ZNxMTEsGbNGn3SXrBgAW+88Qa7d+8mOTmZAwcOmOpQhBDCJEyWgHv16oWzs3O5ZYmJiYSHhwMQHh7O119/XW65RqPB19eXnJwc0tLSOHToEH5+fri4uODs7Iyfnx8HDx4kLS2Nq1ev4uvri0ajITw8nMTERFMdihBCmISdOXeWkZGBu7s7AC1atCAjIwMArVaLp6envpynpydarbbCcg8Pj0qXl5U3RGlpKUlJSbWKX6drSXp6UY3lsrMbkJR0udoyBQUFtY7D2KwlFmuJAyQWa44DrCuWujBrAr6ZRqNBo9GYfb82NjZ4e3vXat3kZGjRouZyzs7Qrp1LtWWSkpJqHYexWUss1hIHSCzWHAdYTyx1/RAway8INzc30tLSAEhLS8PV1RVQa7apqan6cqmpqXh4eFRYrtVqK11eVl4IIeoTsybggIAA4uLiAIiLi2PIkCHlliuKwqlTp3B0dMTd3R1/f38OHTpEdnY22dnZHDp0CH9/f9zd3XFwcODUqVMoilJuW0IIUV+YrAli5syZHD9+nKysLAYOHMgLL7zApEmTmDFjBps3b6ZVq1asWrUKgEGDBrF//34CAwNp3LgxS5cuBcDFxYVp06YREREBwPTp03FxcQFg/vz5REdHU1BQwMCBAxk4cKCpDkUIIUzCZAl4xYoVlS7/7LPPKizTaDTMnz+/0vIRERH6BHyzBx54gO3bt9ctSCGEsCC5E04IISxEErAQQliIJGAhhLAQScBCCGEhkoCFEMJCJAELIYSFSAIWQggLkQQshBAWIglYCCEsRBKwERQVwaFDkJ5u6UiEEPWJJGAj2L8fEhPhgw8gIQGuD3MshBDVkgRcR+npcOQIdOkCPXvC99/DxIkgU9QJIWpisQHZ7wSKotZ47e1h+HBo2hTc3dVlBw+CDNAmhKiO1IDr4Icf4Px5GDpUTb4APj7g4gLXR9oUQogqSQKug+++U2u8PXrcWNagATz2GMTHq1MYCSFEVSQB11JhIVy6BJ06wa1T2z35pLpszRrLxCaEqB8kAdfShQtqG/C991Z8r1UriIiA9evh6lXzxyaEqB8kAdfSH3+ArS14eVX+/vTpkJ0N27aZNy4hRP1hUAL+/vvvDVp2N0lOhjZt1DbfyvTvD82bqz0ihBCiMgYl4MWLFxu07G6Rnw+XL0O7dlWXsbWFESPgq6+gpMRsoQkh6pFq+wGfPHmSkydPkpmZyYYNG/TLr169Skkdssqnn35KTEwMGo2GTp068eabb5KWlsbMmTO5cuUKXbp04e2338be3h6dTserr77Kjz/+iIuLCytXrqRNmzYArF27ls2bN2NjY8Pf//53BgwYUOuYbsf58+pzZe2/NwsJgY0b4ehR8PMzfVxCiPql2hpwUVER165do6SkhLy8PP3DwcGB1atX12qHWq2WjRs3smXLFrZv305JSQkJCQksX76c8ePHs2fPHpycnNi8eTMAMTExODk5sWfPHsaPH8/y5csBOHfuHAkJCSQkJLB+/XoWLlxYpw+F2/HHH2BnB61bV18uKEgtJ5M3CyEqU20NuHfv3vTu3ZvRo0fTuqZscxtKSkooKCjAzs6OgoICWrRowdGjR3n33XcBGD16NGvWrOHxxx9n7969PP/88wAEBwezaNEiFEUhMTGRkJAQ7O3t8fLyom3btpw+fZru3bsbLc6qJCfDPfeoybU6Li4wYIDaDvzmmyYPSwhRzxh0K7JOp2Pu3LlcunSJ4uJi/fKNGzfe9g49PDyYOHEigwcPpmHDhvj5+dGlSxecnJywu57RPD090Wq1gFpjbtmypRqsnR2Ojo5kZWWh1Wrx8fEpt92ydapTWlpKUlLSbccNoNO15MKFYtLSmnPvvVdJT8+vtFx2dgOSki4D0LOnK++840Fi4llatbpx7goKCmodh7FZSyzWEgdILNYcB1hXLHVhUAJ+8cUXefTRR4mMjMTGpm4917Kzs0lMTCQxMRFHR0defPFFDh48WKdt3g4bGxu8vb1rtW5ysjr0JEDnzg60aOFQaTlnZ2jXzgWAZ56Bd96BX3/tyJAhN8okJSXVOg5js5ZYrCUOkFisOQ6wnljq+iFgUAK2s7Pj8ccfr9OOynz77be0adMGV1dXAIKCgjhx4gQ5OTkUFxdjZ2dHamoqHh4egFqzvXz5Mp6enhQXF5Obm0uzZs3w8PAgNTVVv12tVqtfx5TS0tRnd3fDynfqBB06qM0QU6eaLi4hRP1jUHV28ODBfP7556SlpXHlyhX9ozZatWrF//73P/Lz81EUhSNHjnDffffRp08fdu3aBUBsbCwBAQEABAQEEBsbC8CuXbvo27cvGo2GgIAAEhIS0Ol0pKSkkJycTLdu3WoV0+1IS1MH3ikbfKcmGo16MW7//hu1ZyGEAANrwGUJ8OOPP9Yv02g0JCYm3vYOfXx8CA4OZvTo0djZ2eHt7c0jjzzCQw89xEsvvcSqVavw9vYmMjISgIiICGbNmkVgYCDOzs6sXLkSgI4dOzJ8+HBGjBiBra0t8+bNw9bW9rbjuV3p6YbXfssMGaIO1n78uHRHE0LcYFAC3rt3r1F3GhUVRVRUVLllXl5e+q5nN2vYsGGVXd6mTp3KVDN+ry8tVWvAt9vRYvBgtSacmCgJWAhxg0EJOC4urtLl4eHhRgzF+l26pDYj3G4N2NVVHbIyMRHmzTNNbEKI+segBPzDDz/ofy4sLOTIkSN06dLlrkvAv/yiPt9uAga1GWLlSsjLM7z9WAhxZzMoAc+dO7fc65ycHF566SWTBGTNfv1Vfa5tAn77bXWqomHDjBuXEKJ+qlWn3saNG3Px4kVjx2L1fv1V7ePbsOHtr+vvr84dV4vrlkKIO5RBNeApU6bofy4tLeW3335j+PDhJgvKWv3yS+1qvwBNmkC/fpKAhRA3GJSAJ06cqP/Z1taW1q1b4+npabKgrFFREfz2G/TqVfttDB0Kc+fCX38ZLy4hRP1lUBNE7969ad++PXl5eeTk5NCgqlHI72Bnz9auB8TNym5F/uYb48QkhKjfDErAO3bsIDIykp07d/LVV1/pf76bnDmjPtflbudevcDRUZohhBAqg5ogPvzwQzZv3oybmxsAmZmZjB8/nmF30eX8M2fAxkadZqi27Oxg0CA1Ab/4ovFiE0LUTwYlYEVR9MkXwMXFBUVRTBaUNUpKMmwM4DLJyZUv79FDHaA9ObkVnp7QrJnRQhRC1DMGpRN/f3+eeeYZQkJCALVJYuDAgSYNzNqcO1f9HHA3u3oVKrmrGlDHkgB45x0bPvlEErAQd7NqE/D58+f566+/eO2119i9e7d+JmRfX19GjRpllgCtgaKoCXjs2Lpvq0UL9U64ixfvvguZQojyqr0It3TpUhwc1EHHg4KCiI6OJjo6msDAQJYuXWqWAK1BWppaq23btu7b0mjUyTwvXWrAXdaKI4S4RbUJ+K+//qJz584Vlnfu3JlLly6ZLChr89tv6rOhTRA1ufdeuHbNlrNnjbM9IUT9VG0Czs3NrfK9goICowdjrc6dU5+NUQMGaN9efT582DjbE0LUT9Um4K5du7Jp06YKy2NiYujSpYvJgrI2586pXdDatDHO9lxcwMmphEOHjLM9IUT9VO1FuNdff53nn3+ebdu26RPumTNnKCoqYs2aNWYJ0BqcO6fWfu3tjbfNtm11fPttYwoKoFEj421XCFF/VJuAmzdvzr///W+OHj3K2esNloMGDaJfv35mCc5anDunTqxpTPfco+OHHxqzb58MTynE3cqgfsB9+/alb9++po7Fap07B488Ytxttm6to1Ej2LFDErAQd6tajQd8N8nMhKwsuO8+427Xzg7691enq5fuaELcnSySgHNycoiKimLYsGEMHz6ckydPcuXKFSZMmEBQUBATJkwgOzsbUG+DXrx4MYGBgYSGhvLjjz/qtxMbG0tQUBBBQUH6mZuNrawLmrETMMBDD8HvvyPd0YS4S1kkAS9ZsoQBAwawc+dO4uPj6dChA+vWraNfv37s3r2bfv36sW7dOgAOHDhAcnIyu3fv5o033mDBggUAXLlyhTVr1rBp0yZiYmJYs2aNPmkbU1kXNFMk4MGD1eeEBONvWwhh/cyegHNzc/nuu++IiIgAwN7eHicnJxITE/WTfIaHh/P1118D6JdrNBp8fX3JyckhLS2NQ4cO4efnh4uLC87Ozvj5+XHw4EGjx1uWgMv67hpTmzZw//1qO7AQ4u5j4NhexnPx4kVcXV2Jjo7m559/pkuXLsyZM4eMjAzcr4923qJFCzIyMgDQarXlZt/w9PREq9VWWO7h4YFWq61x/6WlpSQlJRkc7/fft8TTsynJyefQ6VqSnl5U4zo6nTPp6dXXxouLi8nOvkKfPiX885+u/Pe/v9K0aanBcRlTQUHBbZ2TOz0OkFisOQ6wrljqwuwJuLi4mJ9++om5c+fi4+PD4sWL9c0NZTQaDRqNxiT7t7Gxwdvb2+Dy6enQuTN4e3uTnKwOplMTe3v1Q6T67abj7OzC+PGwYQOcO9eZRx81OCyjSkpKuq1zcqfHARKLNccB1hNLXT8EzN4E4enpiaenJz4+PgAMGzaMn376CTc3N9LS0gBIS0vD1dUVUGu2qamp+vVTU1Px8PCosFyr1eJRl+kqqvDbb6Zp/y3j5weenhATY7p9CCGsk9kTcIsWLfD09OT3338H4MiRI3To0IGAgADi4uIAiIuLY8j1CdTKliuKwqlTp3B0dMTd3R1/f38OHTpEdnY22dnZHDp0CH9/f6PGmpsLWq1pE7CtrTrM5Y4d6ohrQoi7h9mbIADmzp3LK6+8QlFREV5eXrz55puUlpYyY8YMNm/eTKtWrVi1ahWg3nm3f/9+AgMDady4sX4YTBcXF6ZNm6a/mDd9+nRcXFyMGqcpu6DdLDIS3n9fnSnDUs0QQgjzs0gC9vb2ZuvWrRWWf/bZZxWWaTQa5s+fX+l2IiIi9AnYFEzZBe1m/v43miEkAQtx95A74apRloCNPQ7EraQZQoi7kyTgapw7p05D7+ho+n1FRkJBgdyUIcTdRBJwNUwxClpVypohPv/cPPsTQlieJOBqmLoL2s1sbWHcOLUZ4vJl8+xTCGFZkoCrkJ8PFy+aLwEDTJwIJSWwcaP59imEsBxJwFW43k3ZrAm4c2cYMAA+/liGqBTibiAJuArm6oJ2q2eeUYenNMG4QkIIKyMJuAqWSsAREWqvi48/Nu9+hRDmJwm4CufOgasrNGtm3v02bQqPP67elJGVZd59CyHMSxJwFX77zXxd0G41dap6EfCWQeKEEHcYScBVOHfO/M0PZXx8YMgQWL0adDrLxCCEMD1JwJXQ6eD8efMk4OTkyh9PPgl//qkO0iNNEULcmSwyGI+1S06G0lLTJ+CrV2Hz5srfUxR18Pe334bwcPO3RQshTE9qwJWwVA+Im2k00LcvpKbCkSOWi0MIYTqSgCthDQkYoFs3tVfEP/5h2TiEEKYhCbgS586Bg4Nh87+Zkp2dOmXR4cNyY4YQdyJJwJU4exY6dlSbASytZ09o3hwWLLB0JEIIY5MEXImzZ6FTJ0tHoWrQAKZMgb174cABS0cjhDAmScC30Ongjz/UGrC1eOIJdaxgqQULcWeRBHyLP/5Qu6BZUwJu1Ahmz4ZvvoHEREtHI4QwFosl4JKSEsLDw5k8eTIAKSkpREZGEhgYyIwZM9BdvwVMp9MxY8YMAgMDiYyM5OLFi/ptrF27lsDAQIKDgzlopKtUZ8+qz9bSBFFm8mS45x6IjpahKsXdLSsLdLqWVd7EVPaoDzcwWexGjI0bN9KhQweuXp+Fcvny5YwfP56QkBDmzZvH5s2befzxx4mJicHJyYk9e/aQkJDA8uXLWbVqFefOnSMhIYGEhAS0Wi0TJkxg165d2Nra1imuX39Vn62pBgxqLXjhQpgwAbZuVSfxFOJulJ0N69YV1dhLafx467+BySI14NTUVPbt26efUl5RFI4ePUpwcDAAo0ePJvH6d+29e/cyevRoAIKDgzly5AiKopCYmEhISAj29vZ4eXnRtm1bTp8+XefYzp5VR0Fzc6vzpowqOVkdrL1jR3j1VbWrXH37tBdClGeRGvDSpUuZNWsWeXl5AGRlZeHk5ISdnRqOp6cnWq0WAK1WS8uWLdVg7exwdHQkKysLrVaLj4+PfpseHh76dapTWlpKUlJSle+fOnUPbdrYkJSUXOE9na4l6elFNe5Dp3MmPT272jLFxcXodLoaywFkZjrz8cdquQ4d7Nm505nnnsvl/vsL9GUmTWpAamrtJpMrKCio9pyYi7XEARKLNceh07WkuLiY9PT0astlZzcgKcm6J1g0ewL+5ptvcHV1pWvXrhw7dszcu8fGxgZvb+8q3790CQYNotIyycmG3Zxhbw8taiiYnp6Ovb19jeVu3V7z5nDmDJw44Uj//o40aKCWcXaGdu1cag6uEklJSdWeE3OxljhAYrHmOJKTwc6uqMb/nbr8Txiqrh9IZk/AJ06cYO/evRw4cIDCwkKuXr3KkiVLyMnJobi4GDs7O1JTU/Hw8ADUmu3ly5fx9PSkuLiY3NxcmjVrhoeHB6mpqfrtarVa/Tq1lZ8PKSnWdwHuZhqNOlTlZ5/Bd99B//6WjkgIUVtmbwN++eWXOXDgAHv37mXFihX07duXd999lz59+rBr1y4AYmNjCQgIACAgIIDY2FgAdu3aRd++fdFoNAQEBJCQkIBOpyMlJYXk5GS6detWp9jKxoCwtgtwt2rXTh2n4tAhKCiosbgQwkpZTT/gWbNmsWHDBgIDA7ly5QqRkZEAREREcOXKFQIDA9mwYQOvvPIKAB07dmT48OGMGDGCZ599lnnz5tW5B0RZFzRrT8AAAQFqjf3bby0diRCitiw6HnCfPn3o06cPAF5eXmyuZHDchg0bsnr16krXnzp1KlOnTjVaPPUpAbdsCV27wtGj0Lu3paMRQtSG1dSArcGvv4KHBzg5WToSwwweDCUlMkaEEPWVJOCblI2CVl+4ukL37vD993DhgqWjEULcLknAN7GmUdAMNWgQ2NjAihWWjkQIcbskAV+Xk6NO/1OfasAAjo7q1EXx8XDqlKWjEULcDknA1/34o/rcpYtl46gNPz+10/mcOZaORAhxOyQBX1eWgLt2tWwctdGokTpo+44dckFOiPpEEvB1Z85AkybQtq2lI6mdp5+GVq1kuEoh6hNJwNedOaM2P9jU0zPSuLE6Y8a338L27ZaORghhiHqabozvxx/rZ/PDzSZMUHtxREer/YOFENZNEjDw119qD4j6noDt7OCNN9QPk//3/ywdjRCiJpKAqd89IG4VEQE9esC8eeoEo0II6yUJmPrdA+JWNjawdKk6Zuq6dZaORghRHUnAqBfgXFzUXgR3gqAg9Q65xYvh+qQjQggrJAmYGz0gNBpLR2IcGg28+SZotfB//2fpaIQQVbnrE7Ci3Bk9IG7Vrx+MGgVvvw2ZmZaORghRmbs+AaemqgnqTkvAAEuWqGNcvPWWpSMRQlTmrk/AZ86oz3diAu7aFZ54Alavhj//tHQ0Qohb3fUJ+E7qglaZhQvVmzIWLbJ0JEKIW931CbhXL3jxRcOmm6+P2rdXB+r56CMZrlIIa2PROeGsgZ8f3H+/2m+2JvX1xoaFC+Hf/4Zp09SZlOvreBdC3GnMnoAvX77Mq6++SkZGBhqNhocffpinn36aK1eu8NJLL3Hp0iVat27NqlWrcHZ2RlEUlixZwv79+2nUqBHLli2jy/X2gtjYWD744ANAnaBz9OjRtYopOxs+/bTmchERtdq8xTVrBu+8A+PHq8c5caLxtp2VpZ6/mjg7q3EIIW4wewK2tbVl9uzZdOnShatXrzJ27Fj8/PzYunUr/fr1Y9KkSaxbt45169Yxa9YsDhw4QHJyMrt37+Z///sfCxYsICYmhitXrrBmzRq2bNmCRqNhzJgxBAQE4OzsbO5DshrV1eIHDlSbW2bNgrAwcHMzzj4N/fAaP14SsBC3MnsCdnd3x93dHQAHBwfat2+PVqslMTGRf/7znwCEh4fz1FNPMWvWLBITEwkPD0ej0eDr60tOTg5paWkcP34cPz8/XFxcAPDz8+PgwYOMHDnS3IdkFa5ehc2bqy/z4INqO/Czz8LWrXfOjSdC1FcWbQ28ePEiSUlJ+Pj4kJGRoU/MLVq0ICMjAwCtVounp6d+HU9PT7RabYXlHh4eaLVa8x5APePhAa++CnFx8P77lo5GCOMpLYWfflIfFy5AQYGlIzKMxS7C5eXlERUVxeuvv46Dg0O59zQaDRoTVc9KS0tJSkoqt0yna0l6elGN6+p0zqSn19zgaUi54uJidDqd0bZnaGzPPdeAw4dtmTmzKS1bnuf++wsoKCiocE4MZei5y85uQFLS5WrL1CUOY5NYrDcOna4lxcXFpKenA5Cfr2HPHicuXrTXl2ncuJTu3fPIz79oqTANYpEEXFRURFRUFKGhoQQFBQHg5uZGWloa7u7upKWl4erqCqg129TUVP26qampeHh44OHhwfHjx/XLtVotvXv3rnHfNjY2eHt7l1uWnGxYNzR7e7V2boxy6enp2NvbG217hsbm4gIxMeDrCzNn3su+fQBJFc6JoQw9d87O0K6dS7VlkpJqH4exSSzWG0dyMtjZFdGiRQsuX4YtW9RBp0aOhDZt4MoV2LHDhkmTHNm925s+fUwXS10/kMzeBKEoCnPmzKF9+/ZMmDBBvzwgIIC4uDgA4uLiGDJkSLnliqJw6tQpHB0dcXd3x9/fn0OHDpGdnU12djaHDh3C39/f3IdTLzVvDtu2QW4uPPQQpKQ0sHRIQty2wkL4z3/Un595Rr3G4eEBnTurs8M0awZDh1p3/3ezJ+Dvv/+e+Ph4jh49SlhYGGFhYezfv59JkyZx+PBhgoKC+Pbbb5k0aRIAgwYNwsvLi8DAQObOncv8+fMBcHFxYdq0aURERBAREcH06dP1F+REzbp3h8RE9eLd00+35ehRS0ckxO3ZtUsd6yQiAlq2LP+ei4uanB0c1ORcXGyREGtk9iaInj178ssvv1T63meffVZhmUaj0SfdW5UlX1E73bvD3r0QHAz9+8Nzz6kD+DRvXnn5zEy1NnHy5I3n338HW1t1RulWrdTxJ+69V272EKZ1/rw9J0+qN1J5eVVexsNDHY71kUfgvffgpZfMG6Mh7vo74e52Pj6wffvvfPFFZ/7v/2D9evWrnL8/NGigfs1LTlaT7YULN9Zr3VptR+7TR30vL0+9An3qlFrrGDJE3bZ0dRPGlpsL33zjgLu72oRWnchI+OwzmDsXxo6Fe+4xS4gGkwQsaNq0lHffVe+Qi4lRmyb+8Q81edrbq1/v+veH6dPVpOvrC9d7DJKcfONGjOJiOHsWDh+G+Hg4cUK9MFJWVghjWL8erl2z5fHH1Yloq6PRqF0uu3SBqCi1C6Y1kQQs9Lp0UR8LFtRufTs78PaGv/0N/vc/2LNH/WcJDTVqmOIulpam/k116FBI69YNDVqnXTuYM0d9HD6sNltYC0nAd6Fbb1nW6VpWWFaXsRs0GrWW3KGDenfe1q3g5KSOyFZTjUWI6ixerDaL9emTBxiWgEEd8fC99yA6Gvbvt56mMfl3uMtUdstyenpRhb68xhi7wdERxo1Ta8KffqrWXv79b3W5ELfrjz/gww/h4YfBxaXkttZt2lRtB54+HXbuhOHDTRTkbZJr1cKkbG1h2DC1d8WuXeqgQJcuWToqUR/Nm6f+PUVF1W79Z59Ve+i8/rp667I1kAQszOLxx2H7djh3Tu05cfq0pSMS9cnp0/D552pTwk1DwNwWe3t44w21p86mTUYNr9YkAYsqJSfX/LidQeqHDVMHhAe1m9uuXUYNV9zBXn9dvS7x2mt1285jj8EDD6jNEUU1D2FictIGLCplyPCWcPuD1Pv4wNGjave0ESPgzTfVMYqt5aKIsD4HD0JCAixbpl6XMGQCgKrY2MDSpWrPnA0b4PoNtxYjNWBhdm3aqDXhsWPVGk1ERN3+qcSdS1Fg9mz1LssXXjDONkNC1H7tCxdCfr5xtllbkoCFRTg4qPfqv/uuetNG166wf39TS4clrMwXX8C336rJskkT42xTo1Fr03/+qXZNsyRJwMJsbm0/Pn8exoxRhxNs0gSmTr2HyEi1u5EQOTnwyivQs6c6upkxDRig1oQXL7ZsrxxpAxZmUVObcmQk7NqVx7ZtTYmPVwcGevllaN/efDEK67JwIaSmqt+QbG2Nv/3/+z/1m9dLL1muV4TUgIVVsLOD3r2vsW+fOnzgunXqnXRBQerXUGkjvrv8+KOaIJ99Vp1M1hQ6dIC//10d/+Srr0yzj5pIAhZWxdMTPvhAbaJYtAh++UXtQ9yiBQQGql8ZExPVEbHEnenaNbW7mIuL2mPBlF55RR27ZPp0dUQ/c5MELKxS69ZqX83ff1cHUJk5U/06OneuOsuBi4vapW3KFFi7Vu3advWqpaMWdaUoMG0anDkD//pX1WNTG0vDhurfT3Ky+s1LUUy7v1tJG7CwOrcODNSqlZpop0xRL8ycPKl+RT11Sm2eWLv2RtkOHdTE3K2b+vDxUUfDkgHi64f169Xxe+fPV2/cMYeBA9X+6LNnq4NIzZ5tnv2CJGBhZQy9AWTaNPVZUdSr2D/9BD//rD5OnIDY2Bu1maZN1a+ZZQ9vb3XesLLJuOsy8pswnn/+U/29BgWp33TM6dVX1Q/0119Xh2Q11xCqkoBFvVRZonZygt691YdOp95y+v/+H2i16khsmzerQxmWcXFRv+IOGKBe6OncGTp2rDi/mDAtRYF33lFvygkIUC+KmaLXQ3U0Gvj4Y3VCgTFj1OsQzz5r+v1KAhZ3JHt7NQE/+OCNZYqiNmFotWp7cloaZGSoQ2Ru2HCjnI0NuLndxz33qG3Rnp7qEJo3Pxo3Vv9pb36UlqqJ39BHXt6N+fQcHNTnpk3B1RXc3G48Gjc2/5QiWVkVe54Ye9xoUC+yvvoqfPklPPqoOmxpQ8OH+TWqJk3UC7yPPKJ2g/z5Z/Wib6NGptunJGBx19Bo1ITh7AydOt1YPm6cmpB//12d9y41FVJSSsnKUhPE4cPqlfm63LZqa6t+KNz8ADXR6XTVz9prZ+eKh4faE8TdXX1U9nOzZhXbuktK1ER/9eqNR06Out+sLLhy5cbPZa/LPiByc28M29iwIdjZNcXJSf3wadRI/cYREqJO7tqmjfph1bhxzefi2jV1MtitW2HjRjXxLVumjgli6bZ6Z2d11L4ZM9S7NDdtUmeIGTfONJMJ1PsEfODAAZYsWUJpaSmRkZH66eyFMNS1a7B7943Xrq5QUpKDj0/5UeqfekpNZnl5UFCg1qhvvWresKFai1UUdVJTe3v1H7eyr9Q6ndpEAmqiKypSt3vtmrqPsufWrQvRaBqRlqbW2s+eVZ/r2m3K1lZNOE5ON547dlRj1mjUOxLLaveFhZCdrXDtmvqtIT9fjTUxsfw23dzUZOzurjbxlLWzl5ZCerp6gfW339TtNW0KkyerF9ysad5AOztYswZGj1bbhJ95Rq2ljxihXhjs2lU9T4Z82NS4r7pvwnJKSkpYtGgRGzZswMPDg4iICAICArjvvvssHZq4A+Xnq7W2mkRE3P5IcjY2avJu2FBNhjd77LHKv5bn56vJMCNDrdX+9VfFMjY2N5o2mjRRPxS+/lqtwZYlWkOPIT09mxY3TZ2i06ldAgEuXoSUlBvPGRnqc26uGoONjfrB1qmTOhtFcLDa+8BSzQ2GGDJE7d6YkKDWhHfsUC8UgnreXFzUb0d1Ua8T8OnTp2nbti1eXl4AhISEkJiYKAlY3FHy8mz44ovqy1SX9LOyype7NcHXlr39jVvFa7plvGHD8hdAAS5fNqzcrW5nDOpb26wrU9M+u3ZVH0uXqgPD//67+sjMNDyOqmgUxdxdj41n586dHDx4kCVLlgAQFxfH6dOnmTdvXpXrnDp1iobW/LErhKg3CgsL8fX1rfX69boGXBt1OVlCCGFM9fr+IA8PD1JTU/WvtVotHh4eFoxICCEMV68T8AMPPEBycjIpKSnodDoSEhIICAiwdFhCCGGQet0EYWdnx7x583j22WcpKSlh7NixdOzY0dJhCSGEQer1RTghhKjP6nUThBBC1GeSgIUQwkLumgR84MABgoODCQwMZN26dWbff0BAAKGhoYSFhTFmzBgArly5woQJEwgKCmLChAlkm2jenejoaPr168fIkSP1y6rat6IoLF68mMDAQEJDQ/nxxx9NGsd7773HgAEDCAsLIywsjP379+vfW7t2LYGBgQQHB3Pw4EGjxXH58mWeeuopRowYQUhICJ999hlgmXNSVSzmPi+FhYVEREQwatQoQkJCWL16NQApKSlERkYSGBjIjBkz0F2/C0Kn0zFjxgwCAwOJjIzk4sWLRomjulhmz55NQECA/pwkJSUBpv39gHrHbXh4OJMnTwaMfE6Uu0BxcbEyZMgQ5cKFC0phYaESGhqqnD171qwxDB48WMnIyCi37K233lLWrl2rKIqirF27Vnn77bdNsu/jx48rZ86cUUJCQmrc9759+5RnnnlGKS0tVU6ePKlERESYNI7Vq1cr69evr1D27NmzSmhoqFJYWKhcuHBBGTJkiFJcXGyUOLRarXLmzBlFURQlNzdXCQoKUs6ePWuRc1JVLOY+L6WlpcrVq1cVRVEUnU6nREREKCdPnlSioqKU7du3K4qiKHPnzlU+//xzRVEU5V//+pcyd+5cRVEUZfv27cqLL75Y5xhqiuW1115TvvrqqwrlTfn7URRF+eSTT5SZM2cqkyZNUhRFMeo5uStqwDffsmxvb6+/ZdnSEhMTCQ8PByA8PJyvv/7aJPvp1asXzrfcf1rVvsuWazQafH19ycnJIS0tzWRxVCUxMZGQkBDs7e3x8vKibdu2nD592ihxuLu706VLFwAcHBxo3749Wq3WIuekqliqYqrzotFoaNq0KQDFxcUUFxej0Wg4evQowcHBAIwePVr/f7N3715Gjx4NQHBwMEeOHEEx0vX8qmKpiil/P6mpqezbt4+I6wN3KIpi1HNyVyRgrVaLp6en/rWHh0e1f+Sm8swzzzBmzBj+85//AJCRkYH79WGgWrRoQUZGhtliqWrft54rT09Pk5+rzz//nNDQUKKjo/Vf+831O7t48SJJSUn4+PhY/JzcHAuY/7yUlJQQFhZG//796d+/P15eXjg5OWF3fRzGm49bq9XS8vrI9XZ2djg6OpJ186ATRo6l7JysXLmS0NBQli5dqv/qb8rfz9KlS5k1axY218fJzMrKMuo5uSsSsDX44osviI2N5aOPPuLzzz/nu+++K/e+RqOp9lPelCy578cee4w9e/YQHx+Pu7s7y5YtM9u+8/LyiIqK4vXXX8ehbNzE68x9Tm6NxRLnxdbWlvj4ePbv38/p06f5/fffTb5PQ2P59ddfmTlzJjt37mTLli1kZ2eb/FrON998g6urK127djXZPu6KBGwNtyyX7c/NzY3AwEBOnz6Nm5ub/qtSWloarq6uZounqn3feq5SU1NNeq6aN2+Ora0tNjY2REZG8sMPP1Qah7F/Z0VFRURFRREaGkpQUBBguXNSWSyWOi8ATk5O9OnTh1OnTpGTk0Px9dHibz5uDw8PLl8fzqy4uJjc3FyamWBivbJYDh48iLu7OxqNBnt7e8aMGVPlOTHW7+fEiRPs3buXgIAAZs6cydGjR1myZIlRz8ldkYAtfcvytWvXuHp9zvRr165x+PBhOnbsSEBAAHFxcYA6ktuQIUPMFlNV+y5brigKp06dwtHRUf+13BRubqv7+uuv9XcyBgQEkJCQgE6nIyUlheTkZLp162aUfSqKwpw5c2jfvj0TJkzQL7fEOakqFnOfl8zMTHJycgAoKCjg22+/pUOHDvTp04ddu3YBEBsbq/+/CQgIIDY2FoBdu3bRt29fo31jqCyW9u3b68+JoigVzokpfj8vv/wyBw4cYO/evaxYsYK+ffvy7rvvGvWc3DV3wu3fv5+lS5fqb1meOnWq2fadkpLC9OnTAbVta+TIkUydOpWsrCxmzJjB5cuXadWqFatWrcLFxcXo+585cybHjx8nKysLNzc3XnjhBYYOHVrpvhVFYdGiRRw8eJDGjRuzdOlSHnjgAZPFcfz4cX7++WcAWrduzaJFi/T/PB988AFbtmzB1taW119/nUGDBhkljv/+97888cQTdOrUSd+2N3PmTLp162b2c1JVLNu3bzfrefn555+ZPXs2JSUlKIrCsGHDeP7550lJSeGll14iOzsbb29vli9fjr29PYWFhcyaNYukpCScnZ1ZuXKlflxuU8Uybtw4srKyUBSFv/3tbyxcuJCmTZua9PdT5tixY3zyySesXbvWqOfkrknAQghhbe6KJgghhLBGkoCFEMJCJAELIYSFSAIWQggLkQQshBAWIglYmJS3tzdhYWGEhIQwatQoPvnkE0pLS422/a1bt5a77XTOnDmcO3fOKNv++uuvWbNmzW2t89xzz+n7sN6upKSkcqOe3Q6dTscTTzyhv0FA1A+SgIVJNWrUiPj4eBISEtiwYQMHDhy47aRWUlJS5XuxsbHlblpYsmQJ9913X63jvdn69et5/PHHb2udjz76CCcnp1rtry4J2N7enn79+rFjx45arS8sQxKwMBs3NzfeeOMNPv/8cxRFYevWrSxatEj//uTJkzl27BgA3bt3Z9myZYwaNYqTJ0+yZs0axo4dy8iRI5k7dy6KorBz507OnDnDK6+8QlhYGAUFBTz11FP6W1S3b99OaGgoI0eO5J133tHvp3v37qxcuZJRo0bx8MMP89dff1WI9Y8//qBBgwb625Fnz57N/PnzefjhhxkyZAjHjh0jOjqa4cOHM3v2bP16AQEBZGZmcvHiRYYPH87f//53QkJCmDhxIgUFBQDlYszMzCQgIACdTsfq1avZsWMHYWFh7Nixg2vXrhEdHU1ERES50dnOnj1LREQEYWFhhIaGkpycDMDQoUPZtm2bsX5dwgwkAQuz8vLyoqSkpMaR365du0a3bt348ssv6dmzJ08++SRbtmxh+/btFBQU8M033zBs2DC6du3K8uXLiY+Pp1GjRvr1tVoty5cv57PPPiMuLo4ffvhBn8CuXbuGj4+PftubNm2qsP8TJ07oh4ksk5OTw3/+8x+io6OZOnUq48ePJyEhgV9//VU/OPjNzp8/zxNPPEFCQgKOjo7621crY29vT1RUFCNGjCA+Pp4RI0bw4Ycf0rdvXzZv3szGjRt55513uHbtGv/+978ZN24c8fHxbNmyRT8SWMeOHfWJXdQPkoCFVbK1tdWPuQrqraCRkZGEhoZy9OjRGtt5f/jhB3r37o2rqyt2dnaEhobqR6Br0KABgwcPBqBr165cunSpwvrp6ekVBkcaPHgwGo2Gzp0707x5czp37oyNjQ333Xdfpdto06YN3t7eAHTp0qXSMtU5dOgQH330EWFhYTz11FMUFhZy+fJlfH19Wbt2LevWrePPP//Uf/DY2trSoEED/bgjwvrV62npRf2TkpKCra0tbm5u2NralrsgV1hYqP+5YcOG2Nra6pcvXLiQLVu20LJlS957771yZW9XgwYN9IOk2NjYVNrG3KhRI3Jzc8sts7e3B9CPyFXGxsam0otfN5extbXVx2xra6sfqLtsTNuqrF69mvbt25db1qFDB3x8fNi3bx+TJk1i4cKF9OvXT7+9hg0bVrtNYT2kBizMJjMzk/nz5/PEE0+g0Who3bo1P//8M6WlpVy+fLnKmR3KElezZs3Iy8sr91W+adOm5OXlVVinW7dufPfdd2RmZlJSUkJCQgK9evUyONb27dtz/vz52zxCw7Ru3ZozZ84AsHPnTv3yW4/F39+ff/3rX/pk/dNPPwHqh5iXlxfjxo1jyJAh/PLLL4A6WHizZs1o0KCBSeIWxic1YGFSBQUFhIWFUVxcjK2tLWFhYfphFx988EFat27NiBEj6NChQ4U21zJOTk5ERkYycuRImjdvXm6kq9GjRzN//nwaNWqkn2kE1Kl+Xn75ZZ5++mkURWHQoEEMHTrU4Lh79erFW2+9haIoRh+YfeLEicyYMYNNmzaVG8msT58+rFu3jrCwMCZPnsy0adNYunQpo0aNorS0lDZt2rB27Vq++uor4uPjsbOzo3nz5vrJIo8dO8ZDDz1k1FiFacloaEJUYfHixQQEBNC/f39Lh2KQ559/npdffpl7773X0qEIA0kThBBVmDJlCvn5+ZYOwyA6nY6hQ4dK8q1npAYshBAWIjVgIYSwEEnAQghhIZKAhRDCQiQBCyGEhUgCFkIIC/n/Ba3YLuSj1JoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "\n",
    "# Define the quantiles\n",
    "quantiles = np.quantile(DX, [0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "# Set the style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.histplot(DX, bins=50, kde=True, color='blue')\n",
    "\n",
    "# Add quantile ticks and labels\n",
    "# for q in quantiles:\n",
    "#     plt.axvline(x=q, color='red', linestyle='--', alpha=0.8)\n",
    "#     plt.text(q, 0, f'{q:.0f}', rotation=45, color='red', verticalalignment='bottom')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Histogram of Traffic Accident Duration')\n",
    "plt.xlabel('Duration (minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0,400)\n",
    "# Save as PDF\n",
    "plt.tight_layout()\n",
    "plt.savefig('traffic_accident_histogram.pdf', format='pdf')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f8223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7dbd540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b747e10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(SX,bins=10)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e26278c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48714"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde3ee79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48714, 43)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c1522e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([360.,  34.,  22.,  30.,  60.,  30.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8333707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61af4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0a5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a306f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "803a23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PCA...\n",
      "PCA complete.\n",
      "Reduced data shape: (48714, 32)\n",
      "Cross-validating Linear Regression...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 9743 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e5adb6e483fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cross-validating {model_name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0maverage_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_mape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_mape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_name} cross-validation complete. Average RMSE: {average_rmse}, Average MAPE: {average_mape}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-e5adb6e483fb>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(model, X, y, kf)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Split the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9743 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet()\n",
    "# Define the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "#     \"SGD\": SGDRegressor(loss='huber',max_iter=1000),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(max_depth=8),\n",
    "#     \"LightGBM\": LGBMRegressor(),\n",
    "#     \"Random Forest\": RandomForestRegressor(),\n",
    "#     \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=50,n_jobs=1) \n",
    "}\n",
    "\n",
    "FX = load_features('roberta') #'bert','gpt2'\n",
    "VX = perform_pca(FX,32)\n",
    "\n",
    "# Prepare data (X, y)\n",
    "XES = np.concatenate([VX,TX],axis=1) #VX only, TX only, VX+TX\n",
    "YES = np.log1p(DX)  # Apply log1p transformation to the target\n",
    "\n",
    "# Function to compute the Mean Absolute Percentage Error (MAPE)\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.expm1(y_true), np.expm1(y_pred)  # Apply expm1 before calculating the metrics\n",
    "    return np.mean(np.where(y_true != 0, np.abs((y_true - y_pred) / y_true), 0)) * 100\n",
    "\n",
    "# For storing results\n",
    "results = []\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Function to perform cross-validation\n",
    "def cross_validate(model, X, y, kf):\n",
    "    rmse_scores = []\n",
    "    mape_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Create a clone of the model to ensure that the model's initial state is preserved\n",
    "        model_clone = clone(model)\n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit the model\n",
    "        print(f\"Fitting {model.__class__.__name__}...\")\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions...\")\n",
    "        y_pred = model_clone.predict(X_test)\n",
    "        \n",
    "        y_pred = np.clip(y_pred, 0, np.log1p(720))\n",
    "        # Calculate RMSE\n",
    "        mse_score = np.mean((np.expm1(y_test) - np.expm1(y_pred)) ** 2)\n",
    "        rmse_score = np.sqrt(mse_score)\n",
    "        rmse_scores.append(rmse_score)\n",
    "        \n",
    "        # Calculate MAPE\n",
    "        mape_score = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        mape_scores.append(mape_score)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    average_rmse = np.mean(rmse_scores)\n",
    "    average_mape = np.mean(mape_scores)\n",
    "    \n",
    "    return average_rmse, average_mape\n",
    "\n",
    "# Calculate cross-validation score for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Cross-validating {model_name}...\")\n",
    "    average_rmse, average_mape = cross_validate(model, XES, YES, kf)\n",
    "    results.append([model_name, average_rmse, average_mape])\n",
    "    print(f\"{model_name} cross-validation complete. Average RMSE: {average_rmse}, Average MAPE: {average_mape}\\n\")\n",
    "\n",
    "# Convert results to DataFrame for a nice table display\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Average RMSE', 'Average MAPE']).round(2)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "67ec5634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for language model: roberta...\n",
      "Starting PCA...\n",
      "PCA complete.\n",
      "Reduced data shape: (47915, 32)\n",
      "Cross-validating Linear Regression with VX features from roberta...\n",
      "Linear Regression cross-validation complete. Average RMSE: 104.8884906958792, Average MAPE: 70.70305116168046\n",
      "\n",
      "Cross-validating ElasticNet with VX features from roberta...\n",
      "ElasticNet cross-validation complete. Average RMSE: 118.48861009170032, Average MAPE: 79.36174039553539\n",
      "\n",
      "Cross-validating DecisionTree with VX features from roberta...\n",
      "DecisionTree cross-validation complete. Average RMSE: 130.30401799298437, Average MAPE: 111.17382413138598\n",
      "\n",
      "Cross-validating XGBoost with VX features from roberta...\n",
      "XGBoost cross-validation complete. Average RMSE: 98.44843735025587, Average MAPE: 68.22698880664606\n",
      "\n",
      "Cross-validating Linear Regression with VX+TX features from roberta...\n",
      "Linear Regression cross-validation complete. Average RMSE: 101.31657877888654, Average MAPE: 65.5828456851471\n",
      "\n",
      "Cross-validating ElasticNet with VX+TX features from roberta...\n",
      "ElasticNet cross-validation complete. Average RMSE: 105.53332099580284, Average MAPE: 69.01677358761617\n",
      "\n",
      "Cross-validating DecisionTree with VX+TX features from roberta...\n",
      "DecisionTree cross-validation complete. Average RMSE: 92.79982937908485, Average MAPE: 64.03719103805489\n",
      "\n",
      "Cross-validating XGBoost with VX+TX features from roberta...\n",
      "XGBoost cross-validation complete. Average RMSE: 71.07289532030035, Average MAPE: 42.58759830079221\n",
      "\n",
      "Loading features for language model: bert...\n",
      "Starting PCA...\n",
      "PCA complete.\n",
      "Reduced data shape: (47915, 32)\n",
      "Cross-validating Linear Regression with VX features from bert...\n",
      "Linear Regression cross-validation complete. Average RMSE: 104.44482066711639, Average MAPE: 68.8502969230688\n",
      "\n",
      "Cross-validating ElasticNet with VX features from bert...\n",
      "ElasticNet cross-validation complete. Average RMSE: 118.48861009170032, Average MAPE: 79.36174039553539\n",
      "\n",
      "Cross-validating DecisionTree with VX features from bert...\n",
      "DecisionTree cross-validation complete. Average RMSE: 125.92065010138715, Average MAPE: 104.48777288599976\n",
      "\n",
      "Cross-validating XGBoost with VX features from bert...\n",
      "XGBoost cross-validation complete. Average RMSE: 97.19911746507822, Average MAPE: 64.52936858528918\n",
      "\n",
      "Cross-validating Linear Regression with VX+TX features from bert...\n",
      "Linear Regression cross-validation complete. Average RMSE: 105.97600135540225, Average MAPE: 62.77145550277063\n",
      "\n",
      "Cross-validating ElasticNet with VX+TX features from bert...\n",
      "ElasticNet cross-validation complete. Average RMSE: 105.53332099580284, Average MAPE: 69.01677358761617\n",
      "\n",
      "Cross-validating DecisionTree with VX+TX features from bert...\n",
      "DecisionTree cross-validation complete. Average RMSE: 92.12433319732803, Average MAPE: 62.473398945237236\n",
      "\n",
      "Cross-validating XGBoost with VX+TX features from bert...\n",
      "XGBoost cross-validation complete. Average RMSE: 70.89031130996565, Average MAPE: 39.99057768212106\n",
      "\n",
      "Loading features for language model: gpt2...\n",
      "Starting PCA...\n",
      "PCA complete.\n",
      "Reduced data shape: (47915, 32)\n",
      "Cross-validating Linear Regression with VX features from gpt2...\n",
      "Linear Regression cross-validation complete. Average RMSE: inf, Average MAPE: inf\n",
      "\n",
      "Cross-validating ElasticNet with VX features from gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/ENV/lib64/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: overflow encountered in expm1\n",
      "/home/agrigore/ENV/lib64/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in expm1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet cross-validation complete. Average RMSE: 118.48861009170032, Average MAPE: 79.36174039553539\n",
      "\n",
      "Cross-validating DecisionTree with VX features from gpt2...\n",
      "DecisionTree cross-validation complete. Average RMSE: 118.49278711700863, Average MAPE: 79.37938348365506\n",
      "\n",
      "Cross-validating XGBoost with VX features from gpt2...\n",
      "XGBoost cross-validation complete. Average RMSE: 118.50034867412435, Average MAPE: 79.37377169022444\n",
      "\n",
      "Cross-validating Linear Regression with VX+TX features from gpt2...\n",
      "Linear Regression cross-validation complete. Average RMSE: 2.132279110661904e+63, Average MAPE: 3.889603042325823e+60\n",
      "\n",
      "Cross-validating ElasticNet with VX+TX features from gpt2...\n",
      "ElasticNet cross-validation complete. Average RMSE: 105.53332099580284, Average MAPE: 69.01677358761617\n",
      "\n",
      "Cross-validating DecisionTree with VX+TX features from gpt2...\n",
      "DecisionTree cross-validation complete. Average RMSE: 90.26999057830906, Average MAPE: 65.24157379629993\n",
      "\n",
      "Cross-validating XGBoost with VX+TX features from gpt2...\n",
      "XGBoost cross-validation complete. Average RMSE: 70.49401187089984, Average MAPE: 43.66189798396442\n",
      "\n",
      "Loading features for language model: mt5...\n",
      "Starting PCA...\n",
      "PCA complete.\n",
      "Reduced data shape: (47915, 32)\n",
      "Cross-validating Linear Regression with VX features from mt5...\n",
      "Linear Regression cross-validation complete. Average RMSE: 111.86089267334822, Average MAPE: 71.91867266726682\n",
      "\n",
      "Cross-validating ElasticNet with VX features from mt5...\n",
      "ElasticNet cross-validation complete. Average RMSE: 118.48861009170032, Average MAPE: 79.36174039553539\n",
      "\n",
      "Cross-validating DecisionTree with VX features from mt5...\n",
      "DecisionTree cross-validation complete. Average RMSE: 146.35234188391465, Average MAPE: 131.01160410476731\n",
      "\n",
      "Cross-validating XGBoost with VX features from mt5...\n",
      "XGBoost cross-validation complete. Average RMSE: 109.20006456385985, Average MAPE: 72.30994828223257\n",
      "\n",
      "Cross-validating Linear Regression with VX+TX features from mt5...\n",
      "Linear Regression cross-validation complete. Average RMSE: 103.13619835072618, Average MAPE: 66.1629770831502\n",
      "\n",
      "Cross-validating ElasticNet with VX+TX features from mt5...\n",
      "ElasticNet cross-validation complete. Average RMSE: 105.53332099580284, Average MAPE: 69.01677358761617\n",
      "\n",
      "Cross-validating DecisionTree with VX+TX features from mt5...\n",
      "DecisionTree cross-validation complete. Average RMSE: 94.20218215117453, Average MAPE: 66.11293400310335\n",
      "\n",
      "Cross-validating XGBoost with VX+TX features from mt5...\n",
      "XGBoost cross-validation complete. Average RMSE: 72.6392850738571, Average MAPE: 43.67476413294895\n",
      "\n",
      "Loading features for language model: xlnet...\n",
      "Starting PCA...\n",
      "PCA complete.\n",
      "Reduced data shape: (47915, 32)\n",
      "Cross-validating Linear Regression with VX features from xlnet...\n",
      "Linear Regression cross-validation complete. Average RMSE: 104.99637980224472, Average MAPE: 68.42963543668827\n",
      "\n",
      "Cross-validating ElasticNet with VX features from xlnet...\n",
      "ElasticNet cross-validation complete. Average RMSE: 114.80425332155144, Average MAPE: 72.78965315507064\n",
      "\n",
      "Cross-validating DecisionTree with VX features from xlnet...\n",
      "DecisionTree cross-validation complete. Average RMSE: 126.96366253631145, Average MAPE: 103.77765402695141\n",
      "\n",
      "Cross-validating XGBoost with VX features from xlnet...\n",
      "XGBoost cross-validation complete. Average RMSE: 96.62644522509427, Average MAPE: 62.4081637965168\n",
      "\n",
      "Cross-validating Linear Regression with VX+TX features from xlnet...\n",
      "Linear Regression cross-validation complete. Average RMSE: 108.82789254828575, Average MAPE: 60.29612842871277\n",
      "\n",
      "Cross-validating ElasticNet with VX+TX features from xlnet...\n",
      "ElasticNet cross-validation complete. Average RMSE: 105.5333201700472, Average MAPE: 69.01677403288865\n",
      "\n",
      "Cross-validating DecisionTree with VX+TX features from xlnet...\n",
      "DecisionTree cross-validation complete. Average RMSE: 92.64781910790298, Average MAPE: 60.73861970479646\n",
      "\n",
      "Cross-validating XGBoost with VX+TX features from xlnet...\n",
      "XGBoost cross-validation complete. Average RMSE: 70.71120804272655, Average MAPE: 39.2007339286497\n",
      "\n",
      "   Language Model Features              Model  Average RMSE  Average MAPE\n",
      "0         roberta       VX  Linear Regression  1.048900e+02  7.070000e+01\n",
      "1         roberta       VX         ElasticNet  1.184900e+02  7.936000e+01\n",
      "2         roberta       VX       DecisionTree  1.303000e+02  1.111700e+02\n",
      "3         roberta       VX            XGBoost  9.845000e+01  6.823000e+01\n",
      "4         roberta    VX+TX  Linear Regression  1.013200e+02  6.558000e+01\n",
      "5         roberta    VX+TX         ElasticNet  1.055300e+02  6.902000e+01\n",
      "6         roberta    VX+TX       DecisionTree  9.280000e+01  6.404000e+01\n",
      "7         roberta    VX+TX            XGBoost  7.107000e+01  4.259000e+01\n",
      "8            bert       VX  Linear Regression  1.044400e+02  6.885000e+01\n",
      "9            bert       VX         ElasticNet  1.184900e+02  7.936000e+01\n",
      "10           bert       VX       DecisionTree  1.259200e+02  1.044900e+02\n",
      "11           bert       VX            XGBoost  9.720000e+01  6.453000e+01\n",
      "12           bert    VX+TX  Linear Regression  1.059800e+02  6.277000e+01\n",
      "13           bert    VX+TX         ElasticNet  1.055300e+02  6.902000e+01\n",
      "14           bert    VX+TX       DecisionTree  9.212000e+01  6.247000e+01\n",
      "15           bert    VX+TX            XGBoost  7.089000e+01  3.999000e+01\n",
      "16           gpt2       VX  Linear Regression           inf           inf\n",
      "17           gpt2       VX         ElasticNet  1.184900e+02  7.936000e+01\n",
      "18           gpt2       VX       DecisionTree  1.184900e+02  7.938000e+01\n",
      "19           gpt2       VX            XGBoost  1.185000e+02  7.937000e+01\n",
      "20           gpt2    VX+TX  Linear Regression  2.132279e+63  3.889603e+60\n",
      "21           gpt2    VX+TX         ElasticNet  1.055300e+02  6.902000e+01\n",
      "22           gpt2    VX+TX       DecisionTree  9.027000e+01  6.524000e+01\n",
      "23           gpt2    VX+TX            XGBoost  7.049000e+01  4.366000e+01\n",
      "24            mt5       VX  Linear Regression  1.118600e+02  7.192000e+01\n",
      "25            mt5       VX         ElasticNet  1.184900e+02  7.936000e+01\n",
      "26            mt5       VX       DecisionTree  1.463500e+02  1.310100e+02\n",
      "27            mt5       VX            XGBoost  1.092000e+02  7.231000e+01\n",
      "28            mt5    VX+TX  Linear Regression  1.031400e+02  6.616000e+01\n",
      "29            mt5    VX+TX         ElasticNet  1.055300e+02  6.902000e+01\n",
      "30            mt5    VX+TX       DecisionTree  9.420000e+01  6.611000e+01\n",
      "31            mt5    VX+TX            XGBoost  7.264000e+01  4.367000e+01\n",
      "32          xlnet       VX  Linear Regression  1.050000e+02  6.843000e+01\n",
      "33          xlnet       VX         ElasticNet  1.148000e+02  7.279000e+01\n",
      "34          xlnet       VX       DecisionTree  1.269600e+02  1.037800e+02\n",
      "35          xlnet       VX            XGBoost  9.663000e+01  6.241000e+01\n",
      "36          xlnet    VX+TX  Linear Regression  1.088300e+02  6.030000e+01\n",
      "37          xlnet    VX+TX         ElasticNet  1.055300e+02  6.902000e+01\n",
      "38          xlnet    VX+TX       DecisionTree  9.265000e+01  6.074000e+01\n",
      "39          xlnet    VX+TX            XGBoost  7.071000e+01  3.920000e+01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=50, n_jobs=1) \n",
    "}\n",
    "\n",
    "# List of language models\n",
    "language_models = ['roberta', 'bert', 'gpt2', 'mt5','xlnet']\n",
    "\n",
    "# language_models = ['xlnet']\n",
    "\n",
    "# Feature sets\n",
    "feature_sets = ['VX', 'TX', 'VX+TX']\n",
    "\n",
    "feature_sets = ['VX', 'VX+TX']\n",
    "\n",
    "# Function to compute the Mean Absolute Percentage Error (MAPE)\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.expm1(y_true), np.expm1(y_pred)  # Apply expm1 before calculating the metrics\n",
    "    return np.mean(np.where(y_true != 0, np.abs((y_true - y_pred) / y_true), 0)) * 100\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Function to perform cross-validation\n",
    "def cross_validate(model, X, y, kf):\n",
    "    rmse_scores = []\n",
    "    mape_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Create a clone of the model to ensure that the model's initial state is preserved\n",
    "        model_clone = clone(model)\n",
    "        # Split the data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Fit the model\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        # Make predictions\n",
    "        y_pred = model_clone.predict(X_test)\n",
    "        # Calculate RMSE\n",
    "        mse_score = np.mean((np.expm1(y_test) - np.expm1(y_pred)) ** 2)\n",
    "        rmse_score = np.sqrt(mse_score)\n",
    "        rmse_scores.append(rmse_score)\n",
    "        # Calculate MAPE\n",
    "        mape_score = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        mape_scores.append(mape_score)\n",
    "    # Calculate average scores\n",
    "    average_rmse = np.mean(rmse_scores)\n",
    "    average_mape = np.mean(mape_scores)\n",
    "    return average_rmse, average_mape\n",
    "\n",
    "\n",
    "results = []\n",
    "y = np.log1p(DX)\n",
    "# Calculate cross-validation score for each model\n",
    "for language_model in language_models:\n",
    "    print(f\"Loading features for language model: {language_model}...\")\n",
    "    FX = load_features(language_model)[filtered]\n",
    "    VX = perform_pca(FX, 32)\n",
    "    for feature_set in feature_sets:\n",
    "        if feature_set == 'VX':\n",
    "            X = VX\n",
    "        elif feature_set == 'TX':\n",
    "            X = TX\n",
    "        elif feature_set == 'VX+TX':\n",
    "            X = np.concatenate([VX,TX],axis=1)\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Cross-validating {model_name} with {feature_set} features from {language_model}...\")\n",
    "            average_rmse, average_mape = cross_validate(model, XES, YES, kf)\n",
    "            results.append([language_model, feature_set, model_name, average_rmse, average_mape])\n",
    "            print(f\"{model_name} cross-validation complete. Average RMSE: {average_rmse}, Average MAPE: {average_mape}\\n\")\n",
    "\n",
    "# Convert results to DataFrame for a nice table display\n",
    "results_df = pd.DataFrame(results, columns=['Language Model', 'Features', 'Model', 'Average RMSE', 'Average MAPE']).round(2)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "208a86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df.Features=='TX']['Language Model']='-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3920cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_feature_values(df):\n",
    "    df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df['Features'] = df['Features'].replace({'VX': 'LLM-features', 'TX': 'Report-features', 'VX+TX': 'LLM-features + Report-features'})\n",
    "    \n",
    "    # Replace 'Language Model' with '-' when 'Features' is 'Report-features'\n",
    "    df.loc[df['Features'] == 'Report-features', 'Language Model'] = '-'\n",
    "    \n",
    "    return df\n",
    "results_df = replace_feature_values(results_df.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0294730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language Model</th>\n",
       "      <th>Features</th>\n",
       "      <th>Model</th>\n",
       "      <th>Average RMSE</th>\n",
       "      <th>Average MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.049000e+02</td>\n",
       "      <td>7.070000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.185000e+02</td>\n",
       "      <td>7.940000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>1.303000e+02</td>\n",
       "      <td>1.112000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>9.840000e+01</td>\n",
       "      <td>6.820000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.013000e+02</td>\n",
       "      <td>6.560000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.055000e+02</td>\n",
       "      <td>6.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>9.280000e+01</td>\n",
       "      <td>6.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>roberta</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>7.110000e+01</td>\n",
       "      <td>4.260000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.044000e+02</td>\n",
       "      <td>6.880000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.185000e+02</td>\n",
       "      <td>7.940000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>1.259000e+02</td>\n",
       "      <td>1.045000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>9.720000e+01</td>\n",
       "      <td>6.450000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.060000e+02</td>\n",
       "      <td>6.280000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.055000e+02</td>\n",
       "      <td>6.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>9.210000e+01</td>\n",
       "      <td>6.250000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>7.090000e+01</td>\n",
       "      <td>4.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.185000e+02</td>\n",
       "      <td>7.940000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>1.185000e+02</td>\n",
       "      <td>7.940000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.185000e+02</td>\n",
       "      <td>7.940000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>2.132279e+63</td>\n",
       "      <td>3.889603e+60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.055000e+02</td>\n",
       "      <td>6.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>9.030000e+01</td>\n",
       "      <td>6.520000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>7.050000e+01</td>\n",
       "      <td>4.370000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.119000e+02</td>\n",
       "      <td>7.190000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.185000e+02</td>\n",
       "      <td>7.940000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>1.464000e+02</td>\n",
       "      <td>1.310000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.092000e+02</td>\n",
       "      <td>7.230000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.031000e+02</td>\n",
       "      <td>6.620000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.055000e+02</td>\n",
       "      <td>6.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>9.420000e+01</td>\n",
       "      <td>6.610000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>mt5</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>7.260000e+01</td>\n",
       "      <td>4.370000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>6.840000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.148000e+02</td>\n",
       "      <td>7.280000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>1.270000e+02</td>\n",
       "      <td>1.038000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>9.660000e+01</td>\n",
       "      <td>6.240000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.088000e+02</td>\n",
       "      <td>6.030000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.055000e+02</td>\n",
       "      <td>6.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>9.260000e+01</td>\n",
       "      <td>6.070000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>LLM-features + Report-features</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>7.070000e+01</td>\n",
       "      <td>3.920000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Language Model                        Features              Model  \\\n",
       "0         roberta                    LLM-features  Linear Regression   \n",
       "1         roberta                    LLM-features         ElasticNet   \n",
       "2         roberta                    LLM-features       DecisionTree   \n",
       "3         roberta                    LLM-features            XGBoost   \n",
       "4         roberta  LLM-features + Report-features  Linear Regression   \n",
       "5         roberta  LLM-features + Report-features         ElasticNet   \n",
       "6         roberta  LLM-features + Report-features       DecisionTree   \n",
       "7         roberta  LLM-features + Report-features            XGBoost   \n",
       "8            bert                    LLM-features  Linear Regression   \n",
       "9            bert                    LLM-features         ElasticNet   \n",
       "10           bert                    LLM-features       DecisionTree   \n",
       "11           bert                    LLM-features            XGBoost   \n",
       "12           bert  LLM-features + Report-features  Linear Regression   \n",
       "13           bert  LLM-features + Report-features         ElasticNet   \n",
       "14           bert  LLM-features + Report-features       DecisionTree   \n",
       "15           bert  LLM-features + Report-features            XGBoost   \n",
       "16           gpt2                    LLM-features  Linear Regression   \n",
       "17           gpt2                    LLM-features         ElasticNet   \n",
       "18           gpt2                    LLM-features       DecisionTree   \n",
       "19           gpt2                    LLM-features            XGBoost   \n",
       "20           gpt2  LLM-features + Report-features  Linear Regression   \n",
       "21           gpt2  LLM-features + Report-features         ElasticNet   \n",
       "22           gpt2  LLM-features + Report-features       DecisionTree   \n",
       "23           gpt2  LLM-features + Report-features            XGBoost   \n",
       "24            mt5                    LLM-features  Linear Regression   \n",
       "25            mt5                    LLM-features         ElasticNet   \n",
       "26            mt5                    LLM-features       DecisionTree   \n",
       "27            mt5                    LLM-features            XGBoost   \n",
       "28            mt5  LLM-features + Report-features  Linear Regression   \n",
       "29            mt5  LLM-features + Report-features         ElasticNet   \n",
       "30            mt5  LLM-features + Report-features       DecisionTree   \n",
       "31            mt5  LLM-features + Report-features            XGBoost   \n",
       "32          xlnet                    LLM-features  Linear Regression   \n",
       "33          xlnet                    LLM-features         ElasticNet   \n",
       "34          xlnet                    LLM-features       DecisionTree   \n",
       "35          xlnet                    LLM-features            XGBoost   \n",
       "36          xlnet  LLM-features + Report-features  Linear Regression   \n",
       "37          xlnet  LLM-features + Report-features         ElasticNet   \n",
       "38          xlnet  LLM-features + Report-features       DecisionTree   \n",
       "39          xlnet  LLM-features + Report-features            XGBoost   \n",
       "\n",
       "    Average RMSE  Average MAPE  \n",
       "0   1.049000e+02  7.070000e+01  \n",
       "1   1.185000e+02  7.940000e+01  \n",
       "2   1.303000e+02  1.112000e+02  \n",
       "3   9.840000e+01  6.820000e+01  \n",
       "4   1.013000e+02  6.560000e+01  \n",
       "5   1.055000e+02  6.900000e+01  \n",
       "6   9.280000e+01  6.400000e+01  \n",
       "7   7.110000e+01  4.260000e+01  \n",
       "8   1.044000e+02  6.880000e+01  \n",
       "9   1.185000e+02  7.940000e+01  \n",
       "10  1.259000e+02  1.045000e+02  \n",
       "11  9.720000e+01  6.450000e+01  \n",
       "12  1.060000e+02  6.280000e+01  \n",
       "13  1.055000e+02  6.900000e+01  \n",
       "14  9.210000e+01  6.250000e+01  \n",
       "15  7.090000e+01  4.000000e+01  \n",
       "16           inf           inf  \n",
       "17  1.185000e+02  7.940000e+01  \n",
       "18  1.185000e+02  7.940000e+01  \n",
       "19  1.185000e+02  7.940000e+01  \n",
       "20  2.132279e+63  3.889603e+60  \n",
       "21  1.055000e+02  6.900000e+01  \n",
       "22  9.030000e+01  6.520000e+01  \n",
       "23  7.050000e+01  4.370000e+01  \n",
       "24  1.119000e+02  7.190000e+01  \n",
       "25  1.185000e+02  7.940000e+01  \n",
       "26  1.464000e+02  1.310000e+02  \n",
       "27  1.092000e+02  7.230000e+01  \n",
       "28  1.031000e+02  6.620000e+01  \n",
       "29  1.055000e+02  6.900000e+01  \n",
       "30  9.420000e+01  6.610000e+01  \n",
       "31  7.260000e+01  4.370000e+01  \n",
       "32  1.050000e+02  6.840000e+01  \n",
       "33  1.148000e+02  7.280000e+01  \n",
       "34  1.270000e+02  1.038000e+02  \n",
       "35  9.660000e+01  6.240000e+01  \n",
       "36  1.088000e+02  6.030000e+01  \n",
       "37  1.055000e+02  6.900000e+01  \n",
       "38  9.260000e+01  6.070000e+01  \n",
       "39  7.070000e+01  3.920000e+01  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5473b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(df):\n",
    "    df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "    # Group by 'Features' and 'Model', then calculate mean for 'Average RMSE' and 'Average MAPE'\n",
    "    df_avg = df.groupby(['Language Model','Features', 'Model'], as_index=False).mean()\n",
    "    \n",
    "    return df_avg\n",
    "results_df = calculate_average(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "667f5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(df):\n",
    "    df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "    # Group by 'Features' and 'Model', then calculate mean for 'Average RMSE' and 'Average MAPE'\n",
    "    df_avg = df.groupby(['Language Model','Features', 'Model'], as_index=False).mean()\n",
    "    \n",
    "    return df_avg\n",
    "results_df = calculate_average(results_df)\n",
    "results_df.to_csv('results-LLM-32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99fb3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hpelm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8d94ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.2-py3-none-manylinux2014_x86_64.whl (173.6 MB)\n",
      "     || 173.6 MB 17.9 MB/s                                        | 1.3 MB 5.0 MB/s eta 0:00:355.0 MB/s eta 0:00:34              | 10.6 MB 5.0 MB/s eta 0:00:33                        | 25.0 MB 5.0 MB/s eta 0:00:303 MB 5.0 MB/s eta 0:00:29                         | 36.6 MB 5.0 MB/s eta 0:00:28          | 40.8 MB 5.0 MB/s eta 0:00:27:16                 | 51.6 MB 7.8 MB/s eta 0:00:16                 | 53.9 MB 7.8 MB/s eta 0:00:161615                | 60.5 MB 7.8 MB/s eta 0:00:15  |                    | 63.6 MB 7.8 MB/s eta 0:00:15                  | 75.9 MB 20.1 MB/s eta 0:00:05               | 91.3 MB 20.1 MB/s eta 0:00:05              | 97.3 MB 20.1 MB/s eta 0:00:04             | 98.9 MB 20.1 MB/s eta 0:00:04  |            | 105.5 MB 20.1 MB/s eta 0:00:04    |            | 107.1 MB 64.2 MB/s eta 0:00:02           | 111.2 MB 64.2 MB/s eta 0:00:01B 64.2 MB/s eta 0:00:01B 64.2 MB/s eta 0:00:01       | 130.9 MB 64.2 MB/s eta 0:00:01       | 132.6 MB 64.2 MB/s eta 0:00:01       | 135.6 MB 64.2 MB/s eta 0:00:01      | 137.3 MB 64.2 MB/s eta 0:00:01 7.9 MB/s eta 0:00:05 | 148.5 MB 7.9 MB/s eta 0:00:04     | 152.3 MB 7.9 MB/s eta 0:00:03    | 153.5 MB 7.9 MB/s eta 0:00:03    | 157.8 MB 7.9 MB/s eta 0:00:02   | 158.4 MB 7.9 MB/s eta 0:00:02   | 159.9 MB 7.9 MB/s eta 0:00:02   | 162.0 MB 7.9 MB/s eta 0:00:02  | | 166.1 MB 7.9 MB/s eta 0:00:01  | 167.3 MB 7.9 MB/s eta 0:00:01 \n",
      "\u001b[?25hRequirement already satisfied: scipy in ./ENV/lib/python3.6/site-packages (from xgboost) (1.5.4)\n",
      "Requirement already satisfied: numpy in ./ENV/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98aded6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
